{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to GAICo","text":"GAICo Quickstart Demonstration <p>GenAI Results Comparator (GAICo) is a Python library for comparing, analyzing, and visualizing outputs from Large Language Models (LLMs). It offers an extensible range of metrics, including standard text similarity scores and specialized metrics for structured data like planning sequences and time-series.</p> <p>Check out our latest release updates!</p>"},{"location":"#description","title":"Description","text":"<p>At its core, the library provides a set of metrics for evaluating various types of outputs\u2014from plain text strings to structured data like planning sequences and time-series. These metrics produce normalized scores (typically 0 to 1), where 1 indicates a perfect match, enabling robust analysis and visualization of LLM performance.</p> <p>Class Structure: All metrics are implemented as extensible classes inheriting from <code>BaseMetric</code>. Each metric requires just one method: <code>calculate()</code>.</p> <p>The <code>calculate()</code> method takes two main parameters:</p> <ul> <li><code>generated_texts</code>: A single generated output or an iterable (list, numpy array, etc.) of outputs.</li> <li><code>reference_texts</code>: A single reference output or an iterable of outputs.</li> </ul> <p>Important</p> <p>Handling Missing References: If <code>reference_texts</code> is <code>None</code> or empty, GAICo will automatically use the first item from <code>generated_texts</code> as the reference for comparison. A warning will be printed to the console.</p> <p>Note</p> <p>Batch Processing: When you provide iterables as input, <code>calculate()</code> assumes a one-to-one mapping between generated and reference items. If a single reference is provided for multiple generated items, it will be broadcasted for comparison against each one.</p> <p>Note</p> <p>Optional Dependencies: The standard <code>pip install gaico</code> is lightweight. Some metrics with heavy dependencies (like <code>BERTScore</code> or <code>JSDivergence</code>) require optional installation.</p> <p>Inspiration: The design and evaluation metrics are inspired by Microsoft's article on evaluating LLM-generated content. GAICo currently focuses on reference-based metrics.</p> Overview of the workflow supported by the GAICo library"},{"location":"#features","title":"Features","text":"<ul> <li>Comprehensive Metric Library:</li> <li>Textual Similarity: Jaccard, Cosine, Levenshtein, Sequence Matcher.</li> <li>N-gram Based: BLEU, ROUGE, JS Divergence.</li> <li>Semantic Similarity: BERTScore.</li> <li>Structured Data: Specialized metrics for planning sequences (<code>PlanningLCS</code>, <code>PlanningJaccard</code>) and time-series data (<code>TimeSeriesElementDiff</code>, <code>TimeSeriesDTW</code>).</li> <li>Streamlined Evaluation Workflow:</li> <li>A high-level <code>Experiment</code> class to easily compare multiple models, apply thresholds, generate plots, and create CSV reports.</li> <li>Powerful Visualization:</li> <li>Generate bar charts and radar plots to compare model performance using Matplotlib and Seaborn.</li> <li>Efficient &amp; Flexible:</li> <li>Supports batch processing for efficient computation on datasets.</li> <li>Optimized for various input types (lists, NumPy arrays, Pandas Series).</li> <li>Easily extensible architecture for adding new custom metrics.</li> <li>Robust and Reliable:</li> <li>Includes a comprehensive test suite using Pytest.     </li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Important</p> <p>We strongly recommend using a Python virtual environment to manage dependencies and avoid conflicts with other packages.</p> <p>GAICo can be installed using pip.</p> <ul> <li>Create and activate a virtual environment (e.g., named <code>gaico-env</code>):</li> </ul> <pre><code>  # For Python 3.10+\n  python3 -m venv gaico-env\n  source gaico-env/bin/activate  # On macOS/Linux\n  # gaico-env\\Scripts\\activate   # On Windows\n</code></pre> <ul> <li>Install GAICo:   Once your virtual environment is active, install GAICo using pip:</li> </ul> <pre><code>  pip install gaico\n</code></pre> <p>This installs the core GAICo library.</p>"},{"location":"#using-gaico-with-jupyter-notebookslab","title":"Using GAICo with Jupyter Notebooks/Lab","text":"<p>If you plan to use GAICo within Jupyter Notebooks or JupyterLab (recommended for exploring examples and interactive analysis), install them into the same activated virtual environment:</p> <pre><code># (Ensure your 'gaico-env' is active)\npip install notebook  # For Jupyter Notebook\n# OR\n# pip install jupyterlab # For JupyterLab\n</code></pre> <p>Then, launch Jupyter from the same terminal where your virtual environment is active:</p> <pre><code># (Ensure your 'gaico-env' is active)\njupyter notebook\n# OR\n# jupyter lab\n</code></pre> <p>New notebooks created in this session should automatically use the <code>gaico-env</code> Python environment. For troubleshooting kernel issues, please see our FAQ document.</p>"},{"location":"#optional-installations","title":"Optional Installations","text":"<p>The default <code>pip install gaico</code> is lightweight. Some metrics require extra dependencies, which you can install as needed.</p> <ul> <li>To include the JSDivergence metric (requires SciPy and NLTK):   <pre><code>pip install 'gaico[jsd]'\n</code></pre></li> <li>To include the CosineSimilarity metric (requires scikit-learn):   <pre><code>pip install 'gaico[cosine]'\n</code></pre></li> <li>To include the BERTScore metric (which has larger dependencies like PyTorch):   <pre><code>pip install 'gaico[bertscore]'\n</code></pre></li> <li>To install with all optional features:   <pre><code>pip install 'gaico[jsd,cosine,bertscore]'\n</code></pre></li> </ul> <p>Tip</p> <p>The <code>dev</code> extra, used for development installs, also includes all optional features.</p>"},{"location":"#installation-size-comparison","title":"Installation Size Comparison","text":"<p>The following table provides an estimated overview of the relative disk space impact of different installation options. Actual sizes may vary depending on your operating system, Python version, and existing packages. These are primarily to illustrate the relative impact of optional dependencies.</p> <p>Note: Core dependencies include: <code>levenshtein</code>, <code>matplotlib</code>, <code>numpy</code>, <code>pandas</code>, <code>rouge-score</code>, and <code>seaborn</code>.</p> Installation Command Dependencies Estimated Total Size Impact <code>pip install gaico</code> Core 215 MB <code>pip install 'gaico[jsd]'</code> Core + <code>scipy</code>, <code>nltk</code> 310 MB <code>pip install 'gaico[cosine]'</code> Core + <code>scikit-learn</code> 360 MB <code>pip install 'gaico[bertscore]'</code> Core + <code>bert-score</code> (includes <code>torch</code>, <code>transformers</code>, etc.) 800 MB <code>pip install 'gaico[jsd,cosine,bertscore]'</code> Core + all dependencies from above 960 MB"},{"location":"#for-developers-installing-from-source","title":"For Developers (Installing from source)","text":"<p>If you want to contribute to GAICo or install it from source for development:</p> <ol> <li> <p>Clone the repository:</p> <pre><code>git clone https://github.com/ai4society/GenAIResultsComparator.git\ncd GenAIResultsComparator\n</code></pre> </li> <li> <p>Set up a virtual environment and install dependencies:</p> <p>We recommend using UV for fast environment and dependency management.</p> <pre><code># Create a virtual environment (Python 3.10-3.12 recommended)\nuv venv\n# Activate the environment\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n# Install in editable mode with all development dependencies\nuv pip install -e \".[dev]\"\n</code></pre> <p>If you prefer not to use <code>uv</code>, you can use <code>pip</code>:</p> <pre><code># Create a virtual environment (Python 3.10-3.12 recommended)\npython3 -m venv .venv\n# Activate the environment\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n# Install the package in editable mode with development extras\npip install -e \".[dev]\"\n</code></pre> <p>The <code>dev</code> extra installs GAICo with all optional features, plus dependencies for testing, linting, and documentation.</p> </li> <li> <p>Set up pre-commit hooks (recommended for contributors):</p> <p>Pre-commit hooks help maintain code quality by running checks automatically before you commit.</p> <pre><code>pre-commit install\n</code></pre> </li> </ol>"},{"location":"#citation","title":"Citation","text":"<p>If you find this project useful, please consider citing it in your work:</p> <pre><code>@software{AI4Society_GAICo_GenAI_Results,\n  author = {{Nitin Gupta, Pallav Koppisetti, Biplav Srivastava}},\n  license = {MIT},\n  title = {{GAICo: GenAI Results Comparator}},\n  year = {2025},\n  url = {https://github.com/ai4society/GenAIResultsComparator}\n}\n</code></pre>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<ul> <li>The library is developed by Nitin Gupta, Pallav Koppisetti, and Biplav Srivastava. Members of AI4Society contributed to this tool as part of ongoing discussions. Major contributors are credited.</li> <li>This library uses several open-source packages including NLTK, scikit-learn, and others. Special thanks to the creators and maintainers of the implemented metrics.</li> </ul>"},{"location":"#contact","title":"Contact","text":"<p>If you have any questions, feel free to reach out to us at ai4societyteam@gmail.com.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome! Please feel free to submit a Pull Request.</p> <ol> <li>Fork the repository</li> <li>Create your feature branch (<code>git checkout -b feature/FeatureName</code>)</li> <li>Commit your changes (<code>git commit -m 'Add some FeatureName'</code>)</li> <li>Push to the branch (<code>git push origin feature/FeatureName</code>)</li> <li>Open a Pull Request</li> </ol> <p>Please ensure that your code passes all tests and adheres to our code style guidelines (enforced by pre-commit hooks) before submitting a pull request.</p>"},{"location":"examples/","title":"Examples","text":"<p>Detailed, runnable examples are available as Jupyter Notebooks in the <code>examples</code> folder:</p> <ul> <li> <p><code>quickstart.ipynb</code>:</p> <p>Using GAICo's <code>Experiment</code> module to provide a simple, quickstart workflow.</p> </li> <li> <p><code>example-1.ipynb</code>:</p> <p>Evaluating multiple models (LLMs, Google, and Custom) using a single metric.</p> </li> <li> <p><code>example-2.ipynb</code>:</p> <p>Evaluating a single model on multiple metrics.</p> </li> <li> <p><code>DeepSeek-example.ipynb</code></p> <p>The aim for this notebook was to aid with evaluating DeepSeek R1 for AI4Society's Point of View (POV).</p> </li> </ul>"},{"location":"examples/#advanced-examples","title":"Advanced Examples","text":"<p>The <code>advanced-examples</code> directory contains advances notebooks showcasing more complex use cases and metrics. These examples are intended for users who are already familiar with the basics of GAICo. Please refer to the README.md file in that directory for details. A quick description:</p> <ul> <li> <p><code>llm_faq-example.ipynb</code></p> <p>Comparison of various LLM responses (Phi, Mixtral, etc.) on FAQ dataset from USC.</p> </li> <li> <p><code>threshold-example.ipynb</code></p> <p>Exploration of default and custom thresholding techniques for LLM responses.</p> </li> <li> <p><code>viz-example.ipynb</code></p> <p>Hands-on visualizations for LLM results.</p> </li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>Here are answers to some common questions about using GAICo.</p>"},{"location":"faq/#q-my-jupyter-notebook-isnt-working-correctly-with-gaico-eg-import-errors-version-conflicts-what-should-i-do","title":"Q: My Jupyter Notebook isn't working correctly with GAICo (e.g., import errors, version conflicts). What should I do?","text":"<p>This usually happens if Jupyter Notebook/Lab is not using the Python environment where GAICo was installed. Here's how to troubleshoot:</p> <ol> <li> <p>Ensure Virtual Environment Usage:</p> <ul> <li>Always create and activate a Python virtual environment before installing GAICo and Jupyter.   <pre><code># Example:\npython3 -m venv my-gaico-env\nsource my-gaico-env/bin/activate  # macOS/Linux\n# my-gaico-env\\Scripts\\activate    # Windows\n</code></pre></li> <li>Install GAICo and Jupyter (Notebook/Lab) into this activated environment:   <pre><code>pip install gaico notebook  # Or jupyterlab\n</code></pre></li> <li>Launch Jupyter from the same terminal where the virtual environment is active:   <pre><code>jupyter notebook # Or jupyter lab\n</code></pre></li> </ul> </li> <li> <p>Select the Correct Kernel in Jupyter:</p> <ul> <li>When you open or create a notebook, Jupyter needs to use the kernel associated with your virtual environment.</li> <li>In the Jupyter Notebook menu, go to Kernel &gt; Change kernel.</li> <li>You should see an option corresponding to your virtual environment. It might be named after the environment folder (e.g., \"my-gaico-env\"), simply \"Python 3\", or a display name you set up. Select it.</li> <li>If the kernel restarts, allow it. Then try running your GAICo code again.</li> </ul> </li> <li> <p>Explicitly Registering the Kernel (if the above doesn't resolve it):     If Jupyter consistently fails to find or use your virtual environment's kernel, you can make it explicitly available:</p> <ul> <li>Make sure your virtual environment is active.</li> <li>Install <code>ipykernel</code> if it's not already there (it's usually a dependency of <code>notebook</code> or <code>jupyterlab</code>):   <pre><code>pip install ipykernel\n</code></pre></li> <li>Register the environment as a Jupyter kernel:   <pre><code>python -m ipykernel install --user --name=my-gaico-env --display-name=\"Python (my-gaico-env)\"\n</code></pre>   (Replace <code>my-gaico-env</code> with your actual environment name and choose a descriptive <code>display-name</code>.)</li> <li>Restart your Jupyter server (close the terminal window where it's running and relaunch it from the activated venv).</li> <li>Now, in your notebook, go to Kernel &gt; Change kernel and select the \"Python (my-gaico-env)\" (or your chosen display name).</li> </ul> </li> <li> <p>Check <code>sys.path</code> and <code>sys.executable</code>:     If problems persist, add a cell to your notebook with the following to verify which Python environment is being used:</p> <pre><code>import sys\nimport numpy # A core dependency\nimport gaico\n\nprint(\"Python Executable:\", sys.executable)\nprint(\"\\nSystem Path:\")\nfor p in sys.path:\n    print(p)\nprint(\"\\nNumPy version:\", numpy.__version__)\nprint(\"NumPy location:\", numpy.__file__)\nprint(\"\\nGAICo location:\", gaico.__file__)\n</code></pre> <p>The <code>sys.executable</code> and the paths for <code>numpy</code> and <code>gaico</code> should all point to directories within your active virtual environment (e.g., <code>.../my-gaico-env/lib/python3.x/site-packages/...</code>). If they point to a global Python installation or a different environment, the kernel is not set up correctly.</p> </li> </ol>"},{"location":"faq/#q-how-do-i-add-a-new-custom-metric","title":"Q: How do I add a new custom metric?","text":"<p>Adding a new metric to GAICo is designed to be straightforward. All metrics inherit from the <code>BaseMetric</code> class.</p> <ol> <li> <p>Create a New Metric Class:</p> <ul> <li>Your new metric class should inherit from <code>gaico.metrics.base.BaseMetric</code>.</li> <li>You must implement two methods:</li> <li><code>_single_calculate(self, generated_text: str, reference_text: str, **kwargs: Any) -&gt; Union[float, dict]</code>: This method calculates the metric for a single pair of texts.</li> <li><code>_batch_calculate(self, generated_texts: Union[Iterable, np.ndarray, pd.Series], reference_texts: Union[Iterable, np.ndarray, pd.Series], **kwargs: Any) -&gt; Union[List[float], List[dict], np.ndarray, pd.Series, float, dict]</code>: This method calculates the metric for a batch of texts. Often, this can be implemented by iterating and calling <code>_single_calculate</code>, but you can optimize it for batch operations if possible.</li> <li>The public <code>calculate()</code> method is inherited from <code>BaseMetric</code> and handles input type checking and dispatching to either <code>_single_calculate</code> or <code>_batch_calculate</code>.</li> </ul> </li> <li> <p>Example Structure:</p> <pre><code>from typing import Any, Iterable, List, Union\nimport numpy as np\nimport pandas as pd\nfrom gaico.metrics.base import BaseMetric\n\nclass MyCustomMetric(BaseMetric):\n    def __init__(self, custom_param: str = \"default\"):\n        self.custom_param = custom_param\n        # Add any other initialization logic\n\n    def _single_calculate(\n        self, generated_text: str, reference_text: str, **kwargs: Any\n    ) -&gt; float:\n        # Your logic to compare generated_text and reference_text\n        # Example:\n        score = 0.0\n        if self.custom_param in generated_text and self.custom_param in reference_text:\n            score = 1.0\n        # Ensure score is normalized between 0 and 1\n        return score\n\n    def _batch_calculate(\n        self,\n        generated_texts: Union[Iterable, np.ndarray, pd.Series],\n        reference_texts: Union[Iterable, np.ndarray, pd.Series],\n        **kwargs: Any,\n    ) -&gt; Union[List[float], np.ndarray, pd.Series]:\n        # Default batch implementation (can be optimized)\n        results = [\n            self._single_calculate(gen, ref, **kwargs)\n            for gen, ref in zip(generated_texts, reference_texts)\n        ]\n        if isinstance(generated_texts, np.ndarray):\n            return np.array(results)\n        elif isinstance(generated_texts, pd.Series):\n            return pd.Series(results, index=generated_texts.index)\n        return results\n</code></pre> </li> <li> <p>Using Your Custom Metric:     You can directly instantiate and use your custom metric:</p> <pre><code>from gaico.metrics.base import BaseMetric # Or your custom metric file\n# from your_module import MyCustomMetric # If defined in a separate file\n\n# Assuming MyCustomMetric is defined as above\ncustom_metric = MyCustomMetric(custom_param=\"example\")\nscore = custom_metric.calculate(\"This is an example generated text.\", \"This is an example reference text.\")\nprint(f\"Custom metric score: {score}\")\n</code></pre> </li> <li> <p>(Optional) Registering with the <code>Experiment</code> Class:     If you want your custom metric to be usable by its string name within the <code>Experiment</code> class (e.g., <code>exp.compare(metrics=['MyCustomMetric'])</code>), you'll need to add it to the <code>REGISTERED_METRICS</code> dictionary in <code>gaico/experiment.py</code>:</p> <pre><code># In gaico/experiment.py\n# ... other imports ...\n# from ..metrics.my_custom_metric_module import MyCustomMetric # Adjust import path\n\nREGISTERED_METRICS: Dict[str, type[BaseMetric]] = {\n    \"Jaccard\": JaccardSimilarity,\n    \"Cosine\": CosineSimilarity,\n    # ... other default metrics ...\n    \"MyCustomMetric\": MyCustomMetric, # Add your metric here\n}\n</code></pre> <p>You would also typically add it to <code>DEFAULT_METRICS_TO_RUN</code> in the same file if you want it to run by default when <code>metrics=None</code> in <code>Experiment.compare()</code>.</p> <p>Refer to existing metric implementations in <code>gaico/metrics/</code> for more detailed examples (e.g., <code>text_similarity_metrics.py</code>).</p> </li> </ol>"},{"location":"faq/#q-how-do-i-get-the-list-of-all-supported-metrics-or-use-them-directly","title":"Q: How do I get the list of all supported metrics or use them directly?","text":"<p>There are a couple of ways to understand and access \"supported metrics\":</p> <ol> <li> <p>All metric classes available for direct instantiation from <code>gaico.metrics</code>:     These are all the metric classes defined in the <code>gaico.metrics</code> sub-package that are intended for public use. They are typically listed in <code>gaico.metrics.__init__.__all__</code>.</p> <ul> <li>To import and use specific metric classes by name:   This is the recommended way for clarity.</li> </ul> <pre><code>from gaico.metrics import JaccardSimilarity, BLEU, BERTScore\n\njaccard_scorer = JaccardSimilarity()\nbleu_scorer = BLEU()\nbert_scorer = BERTScore(model_type=\"distilbert-base-uncased\") # Example of passing params\n\nscore1 = jaccard_scorer.calculate(\"text a\", \"text b\")\nscore2 = bleu_scorer.calculate(\"text a\", \"text b\")\n</code></pre> <ul> <li>To import all available metric classes using <code>*</code>:   This makes all metric classes from <code>gaico.metrics.__init__.__all__</code> available in your current namespace.</li> </ul> <pre><code>from gaico.metrics import *\n\n# Now you can use the class names directly\njaccard_scorer = JaccardSimilarity()\nrouge_scorer = ROUGE()\n# ... and so on for all metrics in gaico.metrics.__all__\n\nscore = jaccard_scorer.calculate(\"text a\", \"text b\")\n</code></pre> <p>While convenient, using <code>import *</code> can sometimes make it less clear where names are coming from in larger projects. For <code>gaico.metrics</code>, which is focused, it's generally acceptable.</p> <ul> <li>To see what's available (programmatically):</li> </ul> <pre><code>import gaico.metrics as gaico_metrics\n\n# Metrics explicitly listed in gaico.metrics.__init__.__all__\navailable_metric_class_names = gaico_metrics.__all__\nprint(\"Available metric classes for direct use (class names):\", available_metric_class_names)\n# Example Output:\n# Available metric classes for direct use (class names): ['BLEU', 'ROUGE', 'JSDivergence', 'BERTScore', 'CosineSimilarity', 'JaccardSimilarity', 'LevenshteinDistance', 'SequenceMatcherSimilarity']\n</code></pre> <p>Note that the class names (e.g., <code>JaccardSimilarity</code>) might be different from the shorter keys used in <code>REGISTERED_METRICS</code> (e.g., <code>Jaccard</code>).</p> </li> <li> <p>Metrics available by string name in the <code>Experiment</code> class:     These are the metrics registered in <code>gaico.experiment.REGISTERED_METRICS</code>. You can access this dictionary programmatically to see which short names are recognized by the <code>Experiment</code> class:</p> <pre><code>from gaico.experiment import REGISTERED_METRICS\n\nregistered_metric_names = list(REGISTERED_METRICS.keys())\nprint(\"Metrics usable by name in Experiment class:\", registered_metric_names)\n# Example Output:\n# ['Jaccard', 'Cosine', 'Levenshtein', 'SequenceMatcher', 'BLEU', 'ROUGE', 'JSD', 'BERTScore']\n</code></pre> <p>The <code>gaico.experiment.DEFAULT_METRICS_TO_RUN</code> list also shows which of these are run by default if no specific metrics are requested in <code>Experiment.compare()</code>:</p> <pre><code>from gaico.experiment import DEFAULT_METRICS_TO_RUN\n\nprint(\"Default metrics for Experiment class:\", DEFAULT_METRICS_TO_RUN)\n</code></pre> </li> </ol> <p>If you have other questions, please open an issue on our GitHub repository!</p>"},{"location":"license/","title":"License","text":"<p>This project is licensed under the MIT License:</p> <pre><code>MIT License\n\nCopyright (c) 2024 AI for Society Research Group\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"news/","title":"GAICo Release News","text":"<p>This page details the major releases of the GAICo library, highlighting key features and providing quick start examples.</p>"},{"location":"news/#020-july-2025","title":"0.2.0 - July 2025","text":"<p>This release significantly expands GAICo's capabilities by introducing specialized metrics for structured data: automated planning and time series.</p>"},{"location":"news/#key-features","title":"Key Features:","text":"<ul> <li>Structured Data Metrics:</li> <li>Automated Planning: Added <code>PlanningLCS</code> and <code>PlanningJaccard</code> for analyzing planning sequences.</li> <li>Time-Series: Introduced metrics like <code>TimeSeriesElementDiff</code> and <code>TimeSeriesDTW</code> for evaluating time-series data.</li> </ul>"},{"location":"news/#quick-start-example-020","title":"Quick Start Example (0.2.0):","text":"<p>This example demonstrates using the <code>Experiment</code> class with a mix of general and specialized metrics.</p> <pre><code>from gaico import Experiment\n\n# Initialize Experiment with references for different data types\nexp = Experiment(\n    llm_responses={\n        \"Model A\": \"t1:1, t2:2, t3:3, t4:4, t5:3, t6:2\",\n        \"Model B\": \"t1:1, t2:2.1, t3:3.4, t4:8, t5:5\",\n    },\n    reference_answer=\"t1:1, t2:2.2, t3:3.1, t4:4, t5:3.5\",\n)\n\n# Compare using general text metrics and specialized metrics\nresults_df = exp.compare(\n    metrics=['BLEU', 'JSD', 'Levenshtein', 'TimeSeriesDTW', 'TimeSeriesElementDiff'],\n    plot=True,\n    output_csv_path=\"experiment_release_020.csv\"\n)\n</code></pre> GAICo Quick Start Example Output"},{"location":"news/#015-june-2025","title":"0.1.5 - June 2025","text":"<p>This initial release of GAICo focused on providing a solid foundation for comparing general text outputs from LLMs, including core similarity metrics, the <code>Experiment</code> class, and basic visualization tools.</p>"},{"location":"news/#key-features_1","title":"Key Features:","text":"<ul> <li>Core Text Similarity Metrics: Included fundamental metrics such as Jaccard, Levenshtein, Cosine Similarity, and ROUGE.</li> <li><code>Experiment</code> Class: Introduced a high-level abstraction for simplifying evaluation workflows, including multi-model comparison and report generation.</li> <li>Basic Visualizations: Enabled the creation of bar charts and radar plots for visualizing metric scores.</li> <li>Extensible Architecture: Designed for easy addition of new metrics.</li> </ul>"},{"location":"news/#quick-start-example-015","title":"Quick Start Example (0.1.5):","text":"<p>This example showcases the basic usage of the <code>Experiment</code> class for comparing general text responses.</p> <pre><code>from gaico import Experiment\n\n# Sample data from https://arxiv.org/abs/2504.07995\nllm_responses = {\n    \"Google\": \"Title: Jimmy Kimmel Reacts to Donald Trump Winning the Presidential ... Snippet: Nov 6, 2024 ...\",\n    \"Mixtral 8x7b\": \"I'm an Al and I don't have the ability to predict the outcome of elections.\",\n    \"SafeChat\": \"Sorry, I am designed not to answer such a question.\",\n}\nreference_answer = \"Sorry, I am unable to answer such a question as it is not appropriate.\"\n\n# 1. Initialize Experiment\nexp = Experiment(\n    llm_responses=llm_responses,\n    reference_answer=reference_answer\n)\n\n# 2. Compare models using specific metrics\nresults_df = exp.compare(\n    metrics=['Jaccard', 'ROUGE'],  # Specify metrics, or None for all defaults\n    plot=True,\n    output_csv_path=\"experiment_report_015.csv\"\n)\n</code></pre> GAICo Quick Start Example Output"},{"location":"quickstart/","title":"Quick Start","text":"<p>GAICo makes it easy to evaluate and compare LLM outputs. For detailed, runnable examples, please refer to our Jupyter Notebooks in the <code>examples/</code> folder:</p> <ul> <li><code>quickstart.ipynb</code>: Rapid hands-on with the Experiment sub-module.</li> <li><code>example-1.ipynb</code>: For fine-grained usage, this notebook focuses on comparing multiple model outputs using a single metric.</li> <li><code>example-2.ipynb</code>: For fine-grained usage, this notebook demonstrates evaluating a single model output across all available metrics.</li> </ul>"},{"location":"quickstart/#streamlined-workflow-with-experiment","title":"Streamlined Workflow with <code>Experiment</code>","text":"<p>For a more integrated approach to comparing multiple models, applying thresholds, generating plots, and creating CSV reports, the <code>Experiment</code> class offers a convenient abstraction.</p>"},{"location":"quickstart/#quick-example","title":"Quick Example","text":"<p>This example demonstrates comparing multiple LLM responses against a reference answer using specified metrics, generating a plot, and outputting a CSV report.</p> <pre><code>from gaico import Experiment\n\n# Sample data from https://arxiv.org/abs/2504.07995\nllm_responses = {\n    \"Google\": \"Title: Jimmy Kimmel Reacts to Donald Trump Winning the Presidential ... Snippet: Nov 6, 2024 ...\",\n    \"Mixtral 8x7b\": \"I'm an Al and I don't have the ability to predict the outcome of elections.\",\n    \"SafeChat\": \"Sorry, I am designed not to answer such a question.\",\n}\nreference_answer = \"Sorry, I am unable to answer such a question as it is not appropriate.\"\n\n# 1. Initialize Experiment\nexp = Experiment(\n    llm_responses=llm_responses,\n    reference_answer=reference_answer\n)\n\n# 2. Compare models using specific metrics\n#   This will calculate scores for 'Jaccard' and 'ROUGE',\n#   generate a plot (e.g., radar plot for multiple metrics/models),\n#   and save a CSV report.\nresults_df = exp.compare(\n    metrics=['Jaccard', 'ROUGE'],  # Specify metrics, or None for all defaults\n    plot=True,\n    output_csv_path=\"experiment_report.csv\",\n    custom_thresholds={\"Jaccard\": 0.6, \"ROUGE_rouge1\": 0.35} # Optional: override default thresholds\n)\n\n# The returned DataFrame contains the calculated scores\nprint(\"Scores DataFrame from compare():\")\nprint(results_df)\n</code></pre> <p>This abstraction streamlines common evaluation tasks, while still allowing access to the underlying metric classes and dataframes for more advanced or customized use cases. More details in <code>examples/quickstart.ipynb</code>.</p> <p>However, you might prefer to use the individual metric classes directly for more granular control or if you want to implement custom metrics. See the remaining notebooks in the <code>examples</code> subdirectory.</p> Example Radar Chart generated by the <code>examples/example-2.ipynb</code> notebook."},{"location":"api/experiment/","title":"Experiment Class","text":""},{"location":"api/experiment/#gaico.Experiment","title":"gaico.Experiment","text":"<p>An abstraction to simplify plotting, applying thresholds, and generating CSVs for comparing LLM responses against reference answers using various metrics.</p>"},{"location":"api/experiment/#gaico.Experiment.__init__","title":"__init__","text":"<pre><code>__init__(llm_responses, reference_answer)\n</code></pre> <p>Initializes the Experiment.</p> <p>Parameters:</p> Name Type Description Default <code>llm_responses</code> <code>Dict[str, Any]</code> <p>A dictionary mapping model names (str) to their generated outputs (Any).</p> required <code>reference_answer</code> <code>Optional[Any]</code> <p>A single reference output (Any) to compare against. If None, the output from the first model in <code>llm_responses</code> will be used as the reference.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If llm_responses is not a dictionary.</p> <code>ValueError</code> <p>If llm_responses does not contain string keys, or if it's empty when reference_answer is None.</p>"},{"location":"api/experiment/#gaico.Experiment.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe(metrics=None)\n</code></pre> <p>Returns a DataFrame of scores for the specified metrics. If metrics is None, scores for all default metrics are returned.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Optional[List[str]]</code> <p>A list of base metric names (e.g., \"Jaccard\", \"ROUGE\"). Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A pandas DataFrame with columns \"model_name\", \"metric_name\", \"score\". \"metric_name\" will contain flat metric names (e.g., \"ROUGE_rouge1\").</p>"},{"location":"api/experiment/#gaico.Experiment.compare","title":"compare","text":"<pre><code>compare(metrics=None, plot=False, custom_thresholds=None, output_csv_path=None, aggregate_func=None, plot_title_suffix='Comparison', radar_metrics_limit=12)\n</code></pre> <p>Compares models based on specified metrics, optionally plotting and generating a CSV.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Optional[List[str]]</code> <p>List of base metric names. If None, uses all default registered metrics.</p> <code>None</code> <code>plot</code> <code>bool</code> <p>If True, generates and shows plots. Defaults to False.</p> <code>False</code> <code>custom_thresholds</code> <code>Optional[Dict[str, float]]</code> <p>Dictionary of metric names (base or flat) to threshold values. Overrides default thresholds.</p> <code>None</code> <code>output_csv_path</code> <code>Optional[str]</code> <p>If provided, path to save a CSV report of thresholded results.</p> <code>None</code> <code>aggregate_func</code> <code>Optional[Callable]</code> <p>Aggregation function (e.g., np.mean, np.median) for plotting when multiple scores exist per model/metric (not typical for default setup).</p> <code>None</code> <code>plot_title_suffix</code> <code>str</code> <p>Suffix for plot titles.</p> <code>'Comparison'</code> <code>radar_metrics_limit</code> <code>int</code> <p>Maximum number of metrics for a radar plot to maintain readability.</p> <code>12</code> <p>Returns:</p> Type Description <code>Optional[pd.DataFrame]</code> <p>A pandas DataFrame containing the scores for the compared metrics, or None if no valid metrics.</p>"},{"location":"api/thresholds_module/","title":"Thresholds Module","text":""},{"location":"api/thresholds_module/#gaico.thresholds.apply_thresholds","title":"gaico.thresholds.apply_thresholds","text":"<pre><code>apply_thresholds(results, thresholds=None)\n</code></pre> <p>Apply thresholds to scores for single pair or batch of generated and reference texts. Type ThresholdedResults is a dictionary where keys are metric names and values are either scores or dictionaries with scores. Specifically, it is of type Dict[str, float | Any].</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>ThresholdedResults | List[ThresholdedResults]</code> <p>Either a single dictionary of scores or a list of score dictionaries Single: {\"BLEU\": 0.6, \"JSD\": 0.1} Batch: [{\"BLEU\": 0.6, \"JSD\": 0.1}, {\"BLEU\": 0.4, \"JSD\": 0.2}]</p> required <code>thresholds</code> <code>Optional[Dict[str, float]]</code> <p>Dictionary of metric names to threshold values. Defaults to get_default_thresholds() if not provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>ThresholdedResults | List[ThresholdedResults]</code> <p>For single input, returns a dictionary. For batch input, returns a list. Single: {\"BLEU\": {\"score\": 0.6, \"threshold_applied\": 0.5, \"passed_threshold\": True}, ...} Batch: [{\"BLEU\": {\"score\": 0.6, ...}, ...}, {\"BLEU\": {\"score\": 0.4, ...}, ...}]</p>"},{"location":"api/thresholds_module/#gaico.thresholds.get_default_thresholds","title":"gaico.thresholds.get_default_thresholds","text":"<pre><code>get_default_thresholds()\n</code></pre> <p>Returns the default thresholds for each metric. This is useful for testing and can be overridden by user-defined thresholds.</p> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>A dictionary of default thresholds for each metric. e.g., {\"BLEU\": 0.5, \"JSD\": 0.5}</p>"},{"location":"api/thresholds_module/#gaico.thresholds.calculate_pass_fail_percent","title":"gaico.thresholds.calculate_pass_fail_percent","text":"<pre><code>calculate_pass_fail_percent(results, thresholds=None)\n</code></pre> <p>Calculate pass/fail percentages for each metric across results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>Dict[str, List[float]]</code> <p>Dictionary where keys are metric names and values are lists of scores</p> required <code>thresholds</code> <code>Optional[Dict[str, float]]</code> <p>Dictionary of thresholds for each metric</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, float | int]]</code> <p>Dictionary with metric names as keys and pass/fail statistics as values</p>"},{"location":"api/thresholds_module/#gaico.thresholds.DEFAULT_THRESHOLD","title":"gaico.thresholds.DEFAULT_THRESHOLD  <code>module-attribute</code>","text":"<pre><code>DEFAULT_THRESHOLD = {'BLEU': 0.5, 'ROUGE': 0.5, 'JSD': 0.5, 'BERTScore': 0.5, 'Jaccard': 0.5, 'Cosine': 0.5, 'Levenshtein': 0.5, 'SequenceMatcher': 0.5, 'ActionSequenceDiff': 0.5, 'TimeSeriesElementDiff': 0.5, 'TimeSeriesDTW': 0.5, 'SSIM': 0.5, 'PSNR': 0.5, 'AudioSNR': 0.5, 'SpectrogramDistance': 0.5}\n</code></pre>"},{"location":"api/utils/","title":"Utility Functions","text":""},{"location":"api/utils/#gaico.utils.prepare_results_dataframe","title":"gaico.utils.prepare_results_dataframe","text":"<pre><code>prepare_results_dataframe(results_dict, model_col='model_name', metric_col='metric_name', score_col='score')\n</code></pre> <p>Converts a nested dictionary of results into a long-format DataFrame suitable for plotting.</p> <p>Example Input <code>results_dict</code>: {     'ModelA': {'BLEU': 0.8, 'ROUGE': {'f1': 0.75}},     'ModelB': {'BLEU': 0.7, 'ROUGE': {'f1': 0.65}} } Example Output DataFrame:    model_name  metric_name  score 0     ModelA    BLEU       0.80 1     ModelA    ROUGE_f1   0.75 2     ModelB    BLEU       0.70 3     ModelB    ROUGE_f1   0.65</p> <p>Parameters:</p> Name Type Description Default <code>results_dict</code> <code>Dict[str, Dict[str, Any]]</code> <p>Nested dictionary where keys are model names and values are dictionaries of metric names to scores (or nested score dicts).</p> required <code>model_col</code> <code>str</code> <p>Name for the column containing model names in the output DataFrame.</p> <code>'model_name'</code> <code>metric_col</code> <code>str</code> <p>Name for the column containing metric names in the output DataFrame.</p> <code>'metric_name'</code> <code>score_col</code> <code>str</code> <p>Name for the column containing scores in the output DataFrame.</p> <code>'score'</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A pandas DataFrame in long format.</p>"},{"location":"api/utils/#gaico.utils.generate_deltas_frame","title":"gaico.utils.generate_deltas_frame","text":"<pre><code>generate_deltas_frame(threshold_results, generated_texts=None, reference_texts=None, output_csv_path=None)\n</code></pre> <p>Generate a Pandas DataFrame from threshold function outputs with optional text strings. If <code>output_csv_path</code> is provided, it saves the DataFrame to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>threshold_results</code> <code>Dict[str, Dict[str, Any]] | List[Dict[str, Dict[str, Any]]]</code> <p>Output from apply_thresholds (handles both single and batch) Single: {\"BLEU\": {\"score\": 0.6, \"threshold_applied\": 0.5, \"passed_threshold\": True}, ...} Batch: [{\"BLEU\": {\"score\": 0.6, ...}, ...}, {\"BLEU\": {\"score\": 0.4, ...}, ...}]</p> required <code>generated_texts</code> <code>Optional[str | List[str]]</code> <p>Optional generated text string(s)</p> <code>None</code> <code>reference_texts</code> <code>Optional[str | List[str]]</code> <p>Optional reference text string(s)</p> <code>None</code> <code>output_csv_path</code> <code>Optional[str]</code> <p>Optional path to save the CSV file</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A Pandas DataFrame containing the results</p>"},{"location":"api/visualize_module/","title":"Visualization Module","text":""},{"location":"api/visualize_module/#gaico.visualize.plot_metric_comparison","title":"gaico.visualize.plot_metric_comparison","text":"<pre><code>plot_metric_comparison(df, aggregate_func=None, **kwargs)\n</code></pre> <p>Generates a bar plot comparing different models based on a single metric, after aggregating scores using the provided aggregate_func.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the scores, typically from prepare_results_dataframe. Expected columns are defined by model_col, metric_col, and score_col in kwargs.</p> required <code>aggregate_func</code> <code>Optional[Callable]</code> <p>A function to aggregate scores (e.g., numpy.mean, numpy.median). Defaults to numpy.mean if None.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments: - metric_name (str, required): The name of the metric to plot. - model_col (str, optional): Name of the column identifying models. Defaults to \"model_name\". - score_col (str, optional): Name of the column containing scores. Defaults to \"score\". - metric_col (str, optional): Name of the column containing metric names. Defaults to \"metric_name\". - title (Optional[str], optional): Title for the plot. - xlabel (Optional[str], optional): Label for the x-axis. Defaults to \"Model\". - ylabel (Optional[str], optional): Label for the y-axis. Defaults to the plotted metric's name. - figsize (tuple, optional): Figure size. Defaults to (10, 6). - axis (Optional[matplotlib.axes.Axes], optional): Matplotlib Axes to plot on. - Other kwargs are passed to seaborn.barplot.</p> <code>{}</code> <p>Returns:</p> Type Description <code>matplotlib.axes.Axes</code> <p>The matplotlib Axes object containing the plot.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If required libraries (matplotlib, seaborn, pandas, numpy) are not installed.</p> <code>ValueError</code> <p>If 'metric_name' is not provided in kwargs.</p>"},{"location":"api/visualize_module/#gaico.visualize.plot_radar_comparison","title":"gaico.visualize.plot_radar_comparison","text":"<pre><code>plot_radar_comparison(df, aggregate_func=None, **kwargs)\n</code></pre> <p>Generates a radar plot comparing multiple models across several metrics, after aggregating scores using the provided aggregate_func.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the scores in long format, typically from prepare_results_dataframe. Expected columns are defined by model_col, metric_col, and score_col in kwargs.</p> required <code>aggregate_func</code> <code>Optional[Callable]</code> <p>A function to aggregate scores (e.g., numpy.mean, numpy.median). Defaults to numpy.mean if None.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments: - metrics (List[str], optional): List of metric names to include. If None, all metrics in df are used. - model_col (str, optional): Name of the column identifying models. Defaults to \"model_name\". - score_col (str, optional): Name of the column containing scores. Defaults to \"score\". - metric_col (str, optional): Name of the column containing metric names. Defaults to \"metric_name\". - title (Optional[str], optional): Title for the plot. Defaults to \"Model Comparison Radar Plot\". - figsize (tuple, optional): Figure size. Defaults to (8, 8). - fill_alpha (float, optional): Alpha for filled area. Defaults to 0.1. - line_width (float, optional): Width of plot lines. Defaults to 1.0. - y_ticks (Optional[List[float]], optional): Custom y-axis ticks. - axis (Optional[matplotlib.axes.Axes], optional): Matplotlib polar Axes to plot on.</p> <code>{}</code> <p>Returns:</p> Type Description <code>matplotlib.axes.Axes</code> <p>The matplotlib Axes object containing the plot.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If required libraries (matplotlib, numpy, pandas) are not installed.</p> <code>ValueError</code> <p>If aggregation results in no data or metrics.</p>"},{"location":"api/metrics/","title":"Metrics","text":"<p>GAICo provides several metric classes for text evaluation. All metrics inherit from <code>BaseMetric</code>.</p> <p>To make a custom metric, you can create a new class that inherits from <code>BaseMetric</code> and implements the <code>calculate()</code> method.</p> <p>To view the documentation for a specific metric, you can click on the metric name in the table of contents on the left.</p>"},{"location":"api/metrics/base/","title":"BaseMetric","text":""},{"location":"api/metrics/base/#gaico.metrics.base.BaseMetric","title":"gaico.metrics.base.BaseMetric","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all language model metrics. This class defines the interface that all metric classes should implement. The public method to be accessed is <code>calculate</code>.</p>"},{"location":"api/metrics/base/#gaico.metrics.base.BaseMetric.calculate","title":"calculate","text":"<pre><code>calculate(generated, reference, **kwargs)\n</code></pre> <p>Calculates the metric for a single or batch of generated and reference items. This method handles both single and batch inputs.</p> <p>If the reference is None and <code>generated</code> is an iterable, the function will assume the first element of the iterable as the reference. A warning will be printed.</p> <p>Parameters:</p> Name Type Description Default <code>generated</code> <code>Any</code> <p>A single generated item or an iterable of generated items. Must not be None.</p> required <code>reference</code> <code>Optional[Any]</code> <p>A single reference item, an iterable of reference items, or None.</p> required <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments for specific metrics.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The calculated metric score(s).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>generated</code> is None, or if batch inputs have mismatched lengths.</p> <code>TypeError</code> <p>If inputs cannot be converted to suitable iterables.</p>"},{"location":"api/metrics/ngram_metrics/","title":"N-gram Metrics","text":"<p>GAICo provides several n-gram-based metrics for evaluating text similarity and quality.</p> <p>These metrics are useful for tasks such as machine translation evaluation, text summarization, and general text comparison.</p>"},{"location":"api/metrics/ngram_metrics/#gaico.metrics.ngram_metrics.BLEU","title":"gaico.metrics.ngram_metrics.BLEU","text":"<p>               Bases: <code>TextualMetric</code></p> <p>BLEU (Bilingual Evaluation Understudy) score implementation. This class provides methods to calculate BLEU scores for individual sentence pairs and for batches of sentences. It uses the NLTK library to calculate BLEU scores.</p>"},{"location":"api/metrics/ngram_metrics/#gaico.metrics.ngram_metrics.BLEU.__init__","title":"__init__","text":"<pre><code>__init__(n=4, smoothing_function=None)\n</code></pre> <p>Initialize the BLEU scorer with the specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The max n-gram order to use for BLEU calculation, defaults to 4</p> <code>4</code> <code>smoothing_function</code> <code>Optional[Callable]</code> <p>The smoothing function to use for BLEU, defaults to SmoothingFunction.method1 from NLTK</p> <code>None</code>"},{"location":"api/metrics/ngram_metrics/#gaico.metrics.ngram_metrics.ROUGE","title":"gaico.metrics.ngram_metrics.ROUGE","text":"<p>               Bases: <code>TextualMetric</code></p> <p>ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score implementation using the <code>rouge_score</code> library.</p>"},{"location":"api/metrics/ngram_metrics/#gaico.metrics.ngram_metrics.ROUGE.__init__","title":"__init__","text":"<pre><code>__init__(rouge_types=None, use_stemmer=True, **kwargs)\n</code></pre> <p>Initialize the ROUGE scorer with the specified ROUGE types and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>rouge_types</code> <code>Optional[List[str]]</code> <p>The ROUGE types to calculate, defaults to None Should be one of \"rouge1\", \"rouge2\", or \"rougeL\" in a list to return a single F1 score of that type. If multiple types are provided in a list, the output will be a dictionary of F1 scores for each type. Defaults is None which returns a dictionary of all scores. Equivalent of passing [\"rouge1\", \"rouge2\", \"rougeL\"]</p> <code>None</code> <code>use_stemmer</code> <code>bool</code> <p>Whether to use stemming for ROUGE calculation, defaults to True</p> <code>True</code> <code>kwargs</code> <code>Any</code> <p>Additional parameters to pass to the ROUGE calculation, defaults to None Default only passes the <code>use_stemmer</code> parameter</p> <code>{}</code>"},{"location":"api/metrics/ngram_metrics/#gaico.metrics.ngram_metrics.JSDivergence","title":"gaico.metrics.ngram_metrics.JSDivergence","text":"<p>               Bases: <code>TextualMetric</code></p> <p>Jensen-Shannon Divergence metric implementation using the <code>scipy</code> library.</p>"},{"location":"api/metrics/ngram_metrics/#gaico.metrics.ngram_metrics.JSDivergence.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the Jensen-Shannon Divergence metric.</p>"},{"location":"api/metrics/planning/","title":"Planning Metrics","text":"<p>This section details metrics specialized for evaluating outputs in automated planning, typically sequences of actions.</p> <p>The <code>PlanningLCS</code> metric evaluates the similarity between two action sequences by respecting the order of actions. It is designed for outputs common in automated planning where an LLM might generate a sequence of actions to achieve a goal.</p>"},{"location":"api/metrics/planning/#gaico.metrics.structured.PlanningLCS","title":"gaico.metrics.structured.PlanningLCS","text":"<p>               Bases: <code>PlanningSequenceMetric</code></p> <p>Calculates the difference between two planning action sequences based on the Longest Common Subsequence (LCS). The score is normalized to [0, 1], where 1 indicates a perfect match. This metric respects the order of actions.</p> <p>Input strings are expected to be comma-separated actions. Concurrent actions can be represented in curly braces, e.g., \"a1, {a2, a3}, a4\".</p>"},{"location":"api/metrics/planning/#gaico.metrics.structured.PlanningLCS.__init__","title":"__init__","text":"<pre><code>__init__(**kwargs)\n</code></pre> <p>Initialize the PlanningLCS metric.</p>"},{"location":"api/metrics/planning/#input-format","title":"Input Format","text":"<p>The metric expects input sequences as strings, where actions are comma-separated. Concurrent actions (actions that can happen in parallel or are part of a single step) can be grouped using curly braces <code>{}</code>.</p> <ul> <li>Example Generated Sequence: <code>\"take(objA), move(loc1, loc2), {action_set_1(param), action_set_2}, drop(objA)\"</code></li> <li>Example Reference Sequence: <code>\"take(objA), move(loc1, loc2), drop(objA)\"</code></li> </ul> <p>Each action or action set is treated as a single element in the sequence during comparison.</p>"},{"location":"api/metrics/planning/#calculation","title":"Calculation","text":"<ol> <li>Parsing: Both the generated and reference strings are parsed into lists of elements. Each element is either a string (for a single action) or a <code>frozenset</code> of strings (for a set of concurrent actions).<ul> <li><code>\"a1, {a2, a3}, a4\"</code> becomes <code>['a1', frozenset({'a2', 'a3'}), 'a4']</code>.</li> </ul> </li> <li>Comparison: The metric calculates the length of the Longest Common Subsequence (LCS) between the two parsed sequences.</li> <li>Normalization: The score is normalized by dividing the LCS length by the length of the longer of the two sequences.<ul> <li><code>Score = LCS_Length / max(Length_Generated_Sequence, Length_Reference_Sequence)</code></li> <li>If both sequences are empty after parsing, the score is <code>1.0</code>.</li> </ul> </li> </ol> <p>The final score is a float between <code>0.0</code> and <code>1.0</code>, where <code>1.0</code> indicates identical sequences.</p>"},{"location":"api/metrics/planning/#usage","title":"Usage","text":"<pre><code>from gaico.metrics.structured import PlanningLCS\n\nmetric = PlanningLCS()\n\ngenerated_plan = \"pickup(A), stack(A,B), {noop1, noop2}, pickup(C)\"\nreference_plan = \"pickup(A), stack(A,B), pickup(C)\"\n\n# Generated (parsed): ['pickup(A)', 'stack(A,B)', frozenset({'noop1', 'noop2'}), 'pickup(C)'] (len 4)\n# Reference (parsed): ['pickup(A)', 'stack(A,B)', 'pickup(C)'] (len 3)\n# LCS: ['pickup(A)', 'stack(A,B)', 'pickup(C)'] (len 3)\n# Score = 3 / max(4, 3) = 3 / 4 = 0.75\n\nscore = metric.calculate(generated_plan, reference_plan)\nprint(f\"PlanningLCS Score: {score}\")\n# Expected output: PlanningLCS Score: 0.75\n</code></pre> <p>The <code>PlanningJaccard</code> metric calculates the similarity between two action sequences by comparing their sets of unique actions, ignoring order and frequency. This is useful for checking if the same overall actions were taken, regardless of the sequence.</p>"},{"location":"api/metrics/planning/#gaico.metrics.structured.PlanningJaccard","title":"gaico.metrics.structured.PlanningJaccard","text":"<p>               Bases: <code>PlanningSequenceMetric</code></p> <p>Calculates the Jaccard similarity between the sets of actions from two planning sequences. The score is normalized to [0, 1], where 1 indicates that both sequences contain the exact same set of actions, ignoring order and frequency.</p> <p>Concurrent actions are flattened into the set.</p>"},{"location":"api/metrics/planning/#gaico.metrics.structured.PlanningJaccard.__init__","title":"__init__","text":"<pre><code>__init__(**kwargs)\n</code></pre> <p>Initialize the PlanningJaccard metric.</p>"},{"location":"api/metrics/planning/#input-format_1","title":"Input Format","text":"<p>The input format is the same as for <code>PlanningLCS</code>. Concurrent actions in curly braces are flattened and treated as individual actions within the set.</p>"},{"location":"api/metrics/planning/#calculation_1","title":"Calculation","text":"<ol> <li>Parsing: Both sequences are parsed into lists of elements, just like in <code>PlanningLCS</code>.</li> <li>Flattening: The parsed lists are converted into flat sets of unique action strings.<ul> <li><code>['a1', frozenset({'a2', 'a3'}), 'a4']</code> becomes the set <code>{'a1', 'a2', 'a3', 'a4'}</code>.</li> </ul> </li> <li>Comparison: The metric calculates the Jaccard similarity index between the two sets of actions.<ul> <li><code>Jaccard_Index = |Actions_Generated \u2229 Actions_Reference| / |Actions_Generated \u222a Actions_Reference|</code></li> </ul> </li> <li>Normalization: The Jaccard index is naturally a score between <code>0.0</code> and <code>1.0</code>. If both sets are empty, the score is <code>1.0</code>.</li> </ol>"},{"location":"api/metrics/planning/#usage_1","title":"Usage","text":"<pre><code>from gaico.metrics.structured import PlanningJaccard\n\nmetric = PlanningJaccard()\n\ngenerated_plan = \"pickup(A), stack(A,B), pickup(C)\"\nreference_plan = \"pickup(C), pickup(A), stack(A,B)\" # Same actions, different order\n\n# Generated Set: {'pickup(A)', 'stack(A,B)', 'pickup(C)'}\n# Reference Set: {'pickup(C)', 'pickup(A)', 'stack(A,B)'}\n# Intersection size: 3, Union size: 3\n# Score = 3 / 3 = 1.0\n\nscore = metric.calculate(generated_plan, reference_plan)\nprint(f\"PlanningJaccard Score (same actions, different order): {score}\")\n# Expected output: PlanningJaccard Score (same actions, different order): 1.0\n\ngenerated_plan_2 = \"pickup(A), {stack(A,B), noop}\"\nreference_plan_2 = \"pickup(A), stack(A,B), drop(B)\"\n\n# Generated Set: {'pickup(A)', 'stack(A,B)', 'noop'}\n# Reference Set: {'pickup(A)', 'stack(A,B)', 'drop(B)'}\n# Intersection: {'pickup(A)', 'stack(A,B)'} (size 2)\n# Union: {'pickup(A)', 'stack(A,B)', 'noop', 'drop(B)'} (size 4)\n# Score = 2 / 4 = 0.5\n\nscore_2 = metric.calculate(generated_plan_2, reference_plan_2)\nprint(f\"PlanningJaccard Score (different actions): {score_2}\")\n# Expected output: PlanningJaccard Score (different actions): 0.5\n</code></pre>"},{"location":"api/metrics/planning/#theoretical-background-and-further-reading","title":"Theoretical Background and Further Reading","text":"<p>The metrics used for comparing planning sequences in this library are inspired by foundational work on measuring diversity and similarity between plans. The core idea is that plans can be compared based on their constituent actions, the states they traverse, or their underlying causal structures.</p> <p>The <code>PlanningJaccard</code> and <code>ActionSequenceDiff</code> metrics are direct implementations of action-based, set-difference measures discussed in the following papers. These measures are computationally efficient and provide a domain-independent way to quantify how different two plans are based on the actions they contain.</p> <ul> <li> <p>Srivastava, Biplav, et al. \"Finding inter-related plans.\" ICAPS 2006 (2006): 18.</p> </li> <li> <p>Srivastava, Biplav, et al. \"Domain Independent Approaches for Finding Diverse Plans.\" IJCAI. 2007.</p> </li> </ul> <p>These papers formalize various distance functions (e.g., <code>\u03b41</code>, <code>\u03b4a</code>) that serve as the basis for our order-agnostic planning metrics.</p>"},{"location":"api/metrics/semantic_similarity_metrics/","title":"Semantic Similarity Metrics","text":"<p>GAICo provides a semantic similarity metric to evaluate the semantic similarity between text inputs.</p> <p>This metric is useful for tasks such as semantic text matching, paraphrase detection, and understanding the meaning of text in a more nuanced way.</p>"},{"location":"api/metrics/semantic_similarity_metrics/#gaico.metrics.semantic_similarity_metrics.BERTScore","title":"gaico.metrics.semantic_similarity_metrics.BERTScore","text":"<p>               Bases: <code>TextualMetric</code></p> <p>This class provides methods to calculate BERTScore for individual sentence pairs and for batches of sentences. It uses the BERTScore library to calculate precision, recall, and F1 scores.</p>"},{"location":"api/metrics/semantic_similarity_metrics/#gaico.metrics.semantic_similarity_metrics.BERTScore.__init__","title":"__init__","text":"<pre><code>__init__(model_type='bert-base-uncased', output_val=None, num_layers=8, batch_size=64, additional_params=None)\n</code></pre> <p>Initialize the BERTScore metric.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The BERT model to use, defaults to \"bert-base-uncased\"</p> <code>'bert-base-uncased'</code> <code>output_val</code> <code>Optional[List[str]]</code> <p>The output value to return, defaults to None Should be one of \"precision\", \"recall\", or \"f1\" to return a single score of type float Wrap in a list to return multiple scores of type dict Default returns a dictionary of all scores. Equivalent to passing [\"precision\", \"recall\", \"f1\"]</p> <code>None</code> <code>num_layers</code> <code>int</code> <p>Number of layers to use from BERT, defaults to 8</p> <code>8</code> <code>batch_size</code> <code>int</code> <p>Batch size for processing, defaults to 64</p> <code>64</code> <code>additional_params</code> <code>Optional[Dict[str, Any]]</code> <p>Additional parameters to pass to the BERTScorer class from the bert_score library, defaults to None Default only passes the model_type, num_layers, and batch_size</p> <code>None</code>"},{"location":"api/metrics/text_similarity_metrics/","title":"Text Similarity Metrics","text":"<p>GAICo provides several text similarity metrics to evaluate the similarity between text inputs.</p> <p>These metrics are useful for tasks such as text comparison, duplicate detection, and semantic similarity analysis.</p>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.JaccardSimilarity","title":"gaico.metrics.text_similarity_metrics.JaccardSimilarity","text":"<p>               Bases: <code>TextualMetric</code></p> <p>Jaccard Similarity implementation for text similarity using the formula: J(A, B) = |A \u2229 B| / |A \u222a B|</p> <p>Supports calculation for individual sentence pairs and for batches of sentences.</p>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.JaccardSimilarity.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the Jaccard Similarity metric.</p>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.CosineSimilarity","title":"gaico.metrics.text_similarity_metrics.CosineSimilarity","text":"<p>               Bases: <code>TextualMetric</code></p> <p>Cosine Similarity implementation for text similarity using <code>cosine_similarity</code> from scikit-learn. The class also uses the <code>CountVectorizer</code> from scikit-learn to convert text to vectors.</p> <p>Supports calculation for individual sentence pairs and for batches of sentences.</p>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.CosineSimilarity.__init__","title":"__init__","text":"<pre><code>__init__(**kwargs)\n</code></pre> <p>Initialize the Cosine Similarity metric.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Any</code> <p>Parameters for the CountVectorizer</p> <code>{}</code>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.LevenshteinDistance","title":"gaico.metrics.text_similarity_metrics.LevenshteinDistance","text":"<p>               Bases: <code>TextualMetric</code></p> <p>This class provides methods to calculate Levenshtein Distance for individual sentence pairs and for batches of sentences. It uses the <code>distance</code> and <code>ratio</code> functions from the <code>Levenshtein</code> package.</p>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.LevenshteinDistance.__init__","title":"__init__","text":"<pre><code>__init__(calculate_ratio=True)\n</code></pre> <p>Initialize the Levenshtein Distance metric.</p> <p>Parameters:</p> Name Type Description Default <code>calculate_ratio</code> <code>bool</code> <p>Whether to calculate the ratio of the distance to the length of the longer string, defaults to True.</p> <code>True</code>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.SequenceMatcherSimilarity","title":"gaico.metrics.text_similarity_metrics.SequenceMatcherSimilarity","text":"<p>               Bases: <code>TextualMetric</code></p> <p>This class calculates similarity ratio between texts using the ratio() method from difflib.SequenceMatcher, which returns a float in the range [0, 1] indicating how similar the sequences are.</p> <p>Supports calculation for individual sentence pairs and for batches of sentences.</p>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.SequenceMatcherSimilarity.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the SequenceMatcher Similarity metric</p>"},{"location":"api/metrics/timeseries/","title":"Time Series Metrics","text":"<p>This section covers metrics for evaluating time-series data, particularly when represented as textual or structured sequences.</p> <p>The <code>TimeSeriesElementDiff</code> metric provides a weighted comparison between two time series. It evaluates similarity based on both the presence of common time points (keys) and the closeness of their corresponding values. The metric allows you to weigh the importance of a matching key more heavily than a matching value.</p>"},{"location":"api/metrics/timeseries/#gaico.metrics.structured.TimeSeriesElementDiff","title":"gaico.metrics.structured.TimeSeriesElementDiff","text":"<p>               Bases: <code>TimeSeriesDataMetric</code></p> <p>Calculates a weighted difference between two time series. This metric considers both the presence of time points (keys) and the similarity of their corresponding values. It assigns a higher weight to matching keys than to matching values.</p> <p>The final score is normalized to [0, 1], where 1 indicates a perfect match.</p> <p>Input strings are expected to be comma-separated \"key:value\" pairs, e.g., \"t1:70, t2:72, t3:75\".</p>"},{"location":"api/metrics/timeseries/#gaico.metrics.structured.TimeSeriesElementDiff.__init__","title":"__init__","text":"<pre><code>__init__(key_to_value_weight_ratio=2.0, **kwargs)\n</code></pre> <p>Initialize the TimeSeriesElementDiff metric.</p> <p>Parameters:</p> Name Type Description Default <code>key_to_value_weight_ratio</code> <code>float</code> <p>The weight of a key match relative to a perfect value match. For example, a ratio of 2 means a key match is worth twice as much as a value match. Defaults to 2.0.</p> <code>2.0</code>"},{"location":"api/metrics/timeseries/#input-format","title":"Input Format","text":"<p>The metric expects time-series data as strings, where each time point and its value are represented as a \"key:value\" pair, with pairs separated by commas.</p> <ul> <li>Example Generated Time Series: <code>\"t1:100, t2:105, t4:110\"</code></li> <li>Example Reference Time Series: <code>\"t1:102, t3:120, t4:110\"</code></li> </ul> <p>Values are expected to be numeric (float). Malformed pairs (e.g., missing colon, non-numeric value) or duplicate keys are handled during parsing, with warnings issued.</p>"},{"location":"api/metrics/timeseries/#calculation","title":"Calculation","text":"<ol> <li>Initialization: The metric can be initialized with a <code>key_to_value_weight_ratio</code> (default is <code>2.0</code>), which sets the weight of a key match relative to a perfect value match. For a ratio of 2, the key weight is <code>2.0</code> and the value weight is <code>1.0</code>.</li> <li>Parsing: Both the generated and reference strings are parsed into dictionaries mapping keys (e.g., <code>'t1'</code>) to their float values (e.g., <code>100.0</code>).</li> <li>Comparison: The metric iterates over the union of all keys from both time series. For each key, a score is calculated and compared against the maximum possible score for that key (<code>key_weight + value_weight</code>).<ul> <li>If a key exists in both series:</li> <li>The score gets <code>key_weight</code> for the key match.</li> <li>A value similarity score is calculated as <code>max(0, 1 - |v_gen - v_ref| / |v_ref|)</code>. This score is <code>1.0</code> for a perfect match and decreases towards <code>0.0</code> as the relative difference grows.</li> <li>The score gets <code>value_weight * value_similarity</code>.</li> <li>If a key exists in only one of the series, it contributes <code>0</code> to the total score.</li> </ul> </li> <li>Normalization: The final score is the sum of all accumulated scores divided by the sum of all maximum possible scores.<ul> <li><code>Score = Total_Accumulated_Score / Total_Max_Possible_Score</code></li> <li>If both series are empty, the score is <code>1.0</code>.</li> </ul> </li> </ol> <p>The final score is a float between <code>0.0</code> and <code>1.0</code>, where <code>1.0</code> indicates a perfect match in both keys and values.</p>"},{"location":"api/metrics/timeseries/#usage","title":"Usage","text":"<pre><code>from gaico.metrics.structured import TimeSeriesElementDiff\n\n# Initialize with default key_weight=2.0, value_weight=1.0\nmetric = TimeSeriesElementDiff()\n\ngenerated_ts = \"t1:100, t2:105, t4:110\"\nreference_ts = \"t1:102, t3:120, t4:110\"\n\n# --- Manual Calculation ---\n# Keys union: {t1, t2, t3, t4}. Max score per key = 2+1=3. Total max score = 4*3=12.\n# t1: In both. Key score=2. Value sim = 1 - |100-102|/102 \u2248 0.98. Value score = 1*0.98. Total=2.98\n# t2: Only in generated. Total=0\n# t3: Only in reference. Total=0\n# t4: In both. Key score=2. Value sim = 1 - |110-110|/110 = 1.0. Value score = 1*1.0. Total=3.0\n# Total accumulated score = 2.98 + 0 + 0 + 3.0 = 5.98\n# Final score = 5.98 / 12 \u2248 0.498\n\nscore = metric.calculate(generated_ts, reference_ts)\nprint(f\"TimeSeriesElementDiff Score: {score:.3f}\")\n# Expected output: TimeSeriesElementDiff Score: 0.498\n</code></pre> <p>The <code>TimeSeriesDTW</code> metric calculates the similarity between two time series using Dynamic Time Warping (DTW). DTW is particularly effective for measuring the similarity between two temporal sequences that may vary in speed or have phase differences. It finds the optimal alignment between the sequences, and the DTW distance is the sum of distances between the aligned points.</p>"},{"location":"api/metrics/timeseries/#gaico.metrics.structured.TimeSeriesDTW","title":"gaico.metrics.structured.TimeSeriesDTW","text":"<p>               Bases: <code>TimeSeriesDataMetric</code></p> <p>Calculates the similarity between two time series using Dynamic Time Warping (DTW). The DTW distance measures the optimal alignment between two sequences of values, which is useful when the series are out of phase. The distance is then converted to a similarity score between 0 and 1.</p> <p>This metric only considers the sequence of values, ignoring the keys. The order of values is preserved from the input string.</p> <p>A score of 1 indicates identical value sequences.</p> <p>Input strings are expected to be comma-separated \"key:value\" pairs or just values, e.g., \"t1:70, 72, t3:75\". Non-numeric parts will be ignored with a warning.</p>"},{"location":"api/metrics/timeseries/#gaico.metrics.structured.TimeSeriesDTW.__init__","title":"__init__","text":"<pre><code>__init__(similarity_method='reciprocal', **kwargs)\n</code></pre> <p>Initialize the TimeSeriesDTW metric.</p> <p>Parameters:</p> Name Type Description Default <code>similarity_method</code> <code>str</code> <p>The method to convert DTW distance to similarity. Options: 'reciprocal' (default, 1/(1+d)), 'exponential', 'gaussian'. The 'exponential' and 'gaussian' methods use <code>dtaidistance</code> and are most effective in batch mode. In single calculation mode, they will fall back to 'reciprocal' with a warning.</p> <code>'reciprocal'</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the <code>dtaidistance.dtw.distance</code> function.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the <code>dtaidistance</code> package is not installed.</p> <code>ValueError</code> <p>If an unsupported similarity_method is provided.</p>"},{"location":"api/metrics/timeseries/#input-format_1","title":"Input Format","text":"<p>The metric accepts the same input format as <code>TimeSeriesElementDiff</code>: a string of comma-separated \"key:value\" pairs or simple values. However, <code>TimeSeriesDTW</code> only uses the values for its calculation, preserving their original order and ignoring the keys. This allows for direct comparison of the value sequences.</p> <ul> <li>Example Generated Time Series: <code>\"t1:1, t2:2, t3:3, t4:4, t5:3, t6:2\"</code> (parsed as <code>[1, 2, 3, 4, 3, 2]</code>)</li> <li>Example Reference Time Series: <code>\"a:1, b:2.2, c:3.1, d:4, e:3.5\"</code> (parsed as <code>[1, 2.2, 3.1, 4, 3.5]</code>)</li> <li>Mixed Format: <code>\"t1:1, 2.2, 3.1, t4:4\"</code> (parsed as <code>[1, 2.2, 3.1, 4]</code>)</li> </ul> <p>Any non-numeric parts in the value will be ignored, and a warning will be issued.</p>"},{"location":"api/metrics/timeseries/#calculation_1","title":"Calculation","text":"<ol> <li>Dependency Check: This metric requires the <code>dtaidistance</code> library which is pre-installed with <code>gaico</code>. If it's not installed, an <code>ImportError</code> will be raised.</li> <li>Parsing: Both the generated and reference strings are parsed into NumPy arrays of floating-point numbers, extracting only the values in their original order.</li> <li>DTW Distance: The <code>dtaidistance.dtw.distance</code> function is used to compute the raw DTW distance between the two value sequences.</li> <li>Normalization: The raw distance is not bounded and depends on the scale of the values and length of the sequences. To convert it into a normalized similarity score between 0 and 1, the following formula is used:<ul> <li><code>Score = 1.0 / (1.0 + DTW_Distance)</code></li> <li>If the DTW distance is <code>0</code> (indicating identical sequences), the score is <code>1.0</code>.</li> <li>As the distance increases, the score approaches <code>0.0</code>.</li> <li>If both sequences are empty, the score is <code>1.0</code>. If only one is empty, the score is <code>0.0</code>.</li> </ul> </li> </ol>"},{"location":"api/metrics/timeseries/#usage_1","title":"Usage","text":"<pre><code>from gaico.metrics.structured import TimeSeriesDTW\n\n# Initialize the metric\nmetric = TimeSeriesDTW()\n\n# Two similar time series, slightly out of phase. Keys are ignored.\ngenerated_ts = \"t1:1, t2:2, t3:3, t4:4, t5:3, t6:2\"\nreference_ts = \"a:1, b:2.2, c:3.1, d:4, e:3.5\"\n\n# The metric will extract the value sequences:\n# Generated values: [1.0, 2.0, 3.0, 4.0, 3.0, 2.0]\n# Reference values: [1.0, 2.2, 3.1, 4.0, 3.5]\n# The dtaidistance library will calculate the optimal alignment distance.\n\nscore = metric.calculate(generated_ts, reference_ts)\nprint(f\"TimeSeriesDTW Score: {score:.3f}\")\n# Expected output might be around: TimeSeriesDTW Score: 0.385\n\n# Identical value sequences, different keys\ngenerated_ts_2 = \"k1:5, k2:6, k3:7\"\nreference_ts_2 = \"z1:5, z2:6, z3:7\"\n\n# DTW distance will be 0 because value sequences are identical.\n# Score = 1 / (1 + 0) = 1.0\nscore_2 = metric.calculate(generated_ts_2, reference_ts_2)\nprint(f\"TimeSeriesDTW Score (identical values): {score_2:.3f}\")\n# Expected output: TimeSeriesDTW Score (identical values): 1.000\n</code></pre>"},{"location":"user_guide/direct_metrics/","title":"Using Metrics Directly","text":"<p>While the Experiment Class provides a high-level interface for common evaluation scenarios, GAICo also allows you to use its individual metric classes directly. This approach offers more granular control and flexibility, especially when:</p> <ul> <li>You need to evaluate a single model's output against a reference.</li> <li>You are comparing lists of generated texts against corresponding lists of reference texts (i.e., pair-wise comparisons for multiple examples).</li> <li>You want to integrate a specific metric into a custom evaluation pipeline.</li> <li>You need to configure metric-specific parameters not exposed by the <code>Experiment</code> class's default initialization.</li> <li>You are developing or testing a new custom metric.</li> </ul>"},{"location":"user_guide/direct_metrics/#the-basemetric-class","title":"The <code>BaseMetric</code> Class","text":"<p>All metric classes in GAICo (e.g., <code>JaccardSimilarity</code>, <code>ROUGE</code>, <code>BERTScore</code>) inherit from the <code>gaico.metrics.base.BaseMetric</code> abstract class. This base class defines the common interface for all metrics, primarily through the <code>calculate()</code> method.</p>"},{"location":"user_guide/direct_metrics/#core-method-calculate","title":"Core Method: <code>calculate()</code>","text":"<p>The <code>calculate()</code> method is the primary way to compute a metric's score. It's designed to be flexible and can handle:</p> <ul> <li>Single Pair of Texts: Comparing one generated text string to one reference text string.</li> <li>Batch of Texts: Comparing an iterable (list, NumPy array, Pandas Series) of generated texts to a corresponding iterable of reference texts.</li> <li>Broadcasting: Comparing a single generated text to multiple reference texts, or multiple generated texts to a single reference text.</li> </ul>"},{"location":"user_guide/direct_metrics/#parameters-of-calculate","title":"Parameters of <code>calculate()</code>:","text":"<ul> <li><code>generated_texts</code> (<code>str | Iterable | np.ndarray | pd.Series</code>):     A single generated text string or an iterable of generated texts.</li> <li><code>reference_texts</code> (<code>str | Iterable | np.ndarray | pd.Series</code>):     A single reference text string or an iterable of reference texts.</li> <li><code>**kwargs</code>: Additional keyword arguments specific to the metric being used (e.g., <code>use_corpus_bleu=False</code> for <code>BLEU</code>, or <code>output_val=['f1']</code> for <code>BERTScore</code>).</li> </ul>"},{"location":"user_guide/direct_metrics/#return-value-of-calculate","title":"Return Value of <code>calculate()</code>:","text":"<p>The return type depends on the metric and the input: *   For most metrics, it returns a <code>float</code> for single inputs or a <code>List[float]</code> (or <code>np.ndarray</code>/<code>pd.Series</code> if inputs were such) for batch inputs. *   Metrics like <code>ROUGE</code> or <code>BERTScore</code> can return a <code>dict</code> of scores (e.g., <code>{'rouge1': 0.8, 'rougeL': 0.75}</code>) for single inputs or a <code>List[dict]</code> for batch inputs, depending on their configuration.</p>"},{"location":"user_guide/direct_metrics/#examples","title":"Examples","text":"<p>Let's look at how to use some of the individual metric classes.</p>"},{"location":"user_guide/direct_metrics/#1-jaccard-similarity","title":"1. Jaccard Similarity","text":"<pre><code>from gaico.metrics import JaccardSimilarity\n\n# Initialize the metric\njaccard_metric = JaccardSimilarity()\n\n#  Single Pair\ngenerated_text_single = \"The quick brown fox\"\nreference_text_single = \"A quick brown dog\"\nscore_single = jaccard_metric.calculate(generated_text_single, reference_text_single)\nprint(f\"Jaccard Score (Single): {score_single}\") # Output: Jaccard Score (Single): 0.3333333333333333\n\n#  Batch of Texts\ngenerated_texts_batch = [\"Hello world\", \"GAICo is great\"]\nreference_texts_batch = [\"Hello there world\", \"GAICo is an awesome library\"]\nscores_batch = jaccard_metric.calculate(generated_texts_batch, reference_texts_batch)\nprint(f\"Jaccard Scores (Batch): {scores_batch}\")\n# Jaccard Scores (Batch): [0.6666666666666666, 0.3333333333333333]\n\n#  Broadcasting: Single generated text to multiple references\ngenerated_text_broadcast = \"Common evaluation text\"\nreference_texts_list = [\"Evaluation text for comparison\", \"Another reference text\"]\nscores_broadcast_gen = jaccard_metric.calculate(generated_text_broadcast, reference_texts_list)\nprint(f\"Jaccard Scores (Broadcast Gen): {scores_broadcast_gen}\")\n# Jaccard Scores (Broadcast Gen): [0.4, 0.2]\n</code></pre>"},{"location":"user_guide/direct_metrics/#2-rouge-score-with-specific-configuration","title":"2. ROUGE Score (with specific configuration)","text":"<p>The <code>ROUGE</code> metric, by default, calculates 'rouge1', 'rouge2', and 'rougeL' F1-scores. You can customize this.</p> <pre><code>from gaico.metrics import ROUGE\n\n# Initialize ROUGE to calculate only 'rouge1' and 'rougeL' F1-scores\nrouge_metric = ROUGE(rouge_types=['rouge1', 'rougeL'], use_stemmer=True)\n\ngenerated = \"The cat sat on the mat.\"\nreference = \"A cat was sitting on a mat.\"\n\n# Calculate ROUGE scores\nrouge_scores = rouge_metric.calculate(generated, reference)\nprint(f\"ROUGE Scores: {rouge_scores}\")\n# Example Output: ROUGE Scores: {'rouge1': 0.4615384615384615, 'rougeL': 0.4615384615384615}\n\n# If you configure for a single ROUGE type, it returns a float\nrouge_metric_single_type = ROUGE(rouge_types=['rougeL'])\nrouge_l_score = rouge_metric_single_type.calculate(generated, reference)\nprint(f\"ROUGE-L Score: {rouge_l_score}\") # Example Output: ROUGE-L Score: 0.4615384615384615\n</code></pre>"},{"location":"user_guide/direct_metrics/#3-bertscore-with-specific-output","title":"3. BERTScore (with specific output)","text":"<p><code>BERTScore</code> can also be configured to return specific components (precision, recall, or F1) or a dictionary of them.</p> <pre><code>from gaico.metrics import BERTScore\n\n# Initialize BERTScore to return only the F1 score\n# Note: BERTScore can be slow to initialize the first time as it downloads models.\n# For faster tests/examples, you might use a smaller model or mock it.\n\n# To get a dictionary with only F1 scores:\nbertscore_metric_f1_dict = BERTScore()\ngenerated_bert = \"This is a test sentence for BERTScore.\"\nreference_bert = \"This is a reference sentence for BERTScore evaluation.\"\nbert_f1_dict = bertscore_metric_f1_dict.calculate(generated_bert, reference_bert)\nprint(f\"BERTScore (F1 dict): {bert_f1_dict}\")\n# Example Output: BERTScore (F1 dict): {'precision': 0.9229249954223633, 'recall': 0.8905344009399414, 'f1': 0.9064403772354126}\n\nbertscore_metric_f1 = BERTScore(output_val=['f1']) # Returns a dict: {'f1': value}\nbert_f1_score_float = bertscore_metric_f1.calculate(generated_bert, reference_bert) # Using the same instance\nprint(f\"BERTScore (F1 float): {bert_f1_score_float}\") # This will be the float value of F1\n# Example Output: BERTScore (F1 float): 0.9064403772354126...\n</code></pre>"},{"location":"user_guide/direct_metrics/#available-metrics","title":"Available Metrics","text":"<p>GAICo includes the following built-in metrics, all usable directly:</p> <ul> <li>N-gram-based Metrics:<ul> <li><code>gaico.metrics.BLEU</code></li> <li><code>gaico.metrics.ROUGE</code></li> <li><code>gaico.metrics.JSDivergence</code> (Jensen-Shannon Divergence)</li> </ul> </li> <li>Text Similarity Metrics:<ul> <li><code>gaico.metrics.JaccardSimilarity</code></li> <li><code>gaico.metrics.CosineSimilarity</code></li> <li><code>gaico.metrics.LevenshteinDistance</code></li> <li><code>gaico.metrics.SequenceMatcherSimilarity</code></li> </ul> </li> <li>Semantic Similarity Metrics:<ul> <li><code>gaico.metrics.BERTScore</code></li> </ul> </li> </ul> <p>Refer to the API Reference for detailed constructor parameters and any specific <code>**kwargs</code> for each metric's <code>calculate()</code> method.</p> <p>Using metrics directly provides the foundational building blocks for more complex evaluation setups or for when you need precise control over individual metric calculations.</p>"},{"location":"user_guide/experiment_class/","title":"The Experiment Class","text":"<p>The <code>Experiment</code> class in GAICo provides a streamlined and integrated workflow for evaluating and comparing multiple Language Model (LLM) responses against a single reference answer. It simplifies common tasks such as:</p> <ul> <li>Calculating scores for multiple metrics across different LLM outputs.</li> <li>Applying custom or default thresholds to these scores.</li> <li>Generating comparative plots (bar charts for single metrics, radar charts for multiple metrics).</li> <li>Creating CSV reports summarizing the evaluation.</li> </ul>"},{"location":"user_guide/experiment_class/#why-use-the-experiment-class","title":"Why Use the <code>Experiment</code> Class?","text":"<p>While you can use GAICo's individual metric classes directly for fine-grained control (see Using Metrics Directly), the <code>Experiment</code> class is ideal when:</p> <ul> <li>You have responses from several LLMs (or different versions of the same LLM) that you want to compare against a common reference.</li> <li>You want a quick way to get an overview of performance across multiple metrics.</li> <li>You need to generate reports and visualizations with minimal boilerplate code.</li> </ul> <p>It acts as a high-level orchestrator, making the end-to-end evaluation process more convenient.</p>"},{"location":"user_guide/experiment_class/#initializing-an-experiment","title":"Initializing an Experiment","text":"<p>To start, you need to instantiate the <code>Experiment</code> class. It requires two main pieces of information:</p> <ol> <li><code>llm_responses</code>: A Python dictionary where keys are model names (strings) and values are the text responses generated by those models (strings).</li> <li><code>reference_answer</code>: A single string representing the ground truth or reference text against which all LLM responses will be compared.</li> </ol> <pre><code>from gaico import Experiment\n\n# Example LLM responses from different models\nllm_responses = {\n    \"Google\": \"Title: Jimmy Kimmel Reacts to Donald Trump Winning the Presidential ... Snippet: Nov 6, 2024 ...\",\n    \"Mixtral 8x7b\": \"I'm an Al and I don't have the ability to predict the outcome of elections.\",\n    \"SafeChat\": \"Sorry, I am designed not to answer such a question.\",\n}\n\n# The reference answer\nreference_answer = \"Sorry, I am unable to answer such a question as it is not appropriate.\"\n\n# Initialize the Experiment\nexp = Experiment(\n    llm_responses=llm_responses,\n    reference_answer=reference_answer\n)\n</code></pre>"},{"location":"user_guide/experiment_class/#comparing-models-with-compare","title":"Comparing Models with <code>compare()</code>","text":"<p>The primary method for conducting the evaluation is <code>compare()</code>. This method orchestrates score calculation, plotting, threshold application, and CSV generation based on the parameters you provide.</p> <pre><code># Basic comparison using default metrics, without plotting or CSV output\nresults_df = exp.compare()\nprint(\"Scores DataFrame (default metrics):\")\nprint(results_df)\n\n# A more comprehensive comparison:\n# - Specify a subset of metrics\n# - Enable plotting\n# - Define custom thresholds for some metrics\n# - Output results to a CSV file\nresults_df_custom = exp.compare(\n    metrics=['Jaccard', 'ROUGE', 'Levenshtein'], # Specify metrics, or None for all defaults\n    plot=True,                                  # Generate and show plots\n    output_csv_path=\"my_experiment_report.csv\", # Save a CSV report\n    custom_thresholds={\"Jaccard\": 0.6, \"ROUGE_rougeL\": 0.35} # Optional: override default thresholds\n)\n\nprint(\"\\nScores DataFrame (custom run):\")\nprint(results_df_custom)\n</code></pre>"},{"location":"user_guide/experiment_class/#key-parameters-of-compare","title":"Key Parameters of <code>compare()</code>:","text":"<ul> <li><code>metrics</code> (Optional <code>List[str]</code>):<ul> <li>A list of base metric names (e.g., <code>\"Jaccard\"</code>, <code>\"ROUGE\"</code>, <code>\"BERTScore\"</code>) to calculate.</li> <li>If <code>None</code> (default), all registered default metrics are used (currently: Jaccard, Cosine, Levenshtein, SequenceMatcher, BLEU, ROUGE, JSD, BERTScore).</li> <li>Note: For metrics like ROUGE or BERTScore that produce multiple sub-scores (e.g., <code>ROUGE_rouge1</code>, <code>ROUGE_rougeL</code>, <code>BERTScore_f1</code>), specifying the base name (e.g., <code>\"ROUGE\"</code>) will include all its sub-scores in the results.</li> </ul> </li> <li><code>plot</code> (Optional <code>bool</code>, default <code>False</code>):<ul> <li>If <code>True</code>, generates and displays plots using Matplotlib/Seaborn.</li> <li>If one metric is evaluated, a bar chart comparing models for that metric is shown.</li> <li>If multiple metrics are evaluated (and at least 3, up to <code>radar_metrics_limit</code>), a radar chart is shown, providing a multi-dimensional comparison of models. If fewer than 3 (but more than 1) or more than <code>radar_metrics_limit</code> metrics are present, individual bar charts are shown for each.</li> </ul> </li> <li><code>custom_thresholds</code> (Optional <code>Dict[str, float]</code>):<ul> <li>A dictionary to specify custom pass/fail thresholds for metrics.</li> <li>Keys can be base metric names (e.g., <code>\"Jaccard\"</code>) or specific \"flattened\" metric names as they appear in the output DataFrame (e.g., <code>\"ROUGE_rouge1\"</code>, <code>\"BERTScore_f1\"</code>).</li> <li>These thresholds override the library's default thresholds for the specified metrics.</li> <li>The thresholds are used to determine the \"pass/fail\" status in the CSV report.</li> </ul> </li> <li><code>output_csv_path</code> (Optional <code>str</code>):<ul> <li>If a file path is provided, a CSV file is generated at this location.</li> <li>The CSV report includes:<ul> <li><code>generated_text</code>: The response from each LLM.</li> <li><code>reference_text</code>: The reference answer (repeated for each model).</li> <li>Columns for each metric's score (e.g., <code>Jaccard_score</code>, <code>ROUGE_rouge1_score</code>).</li> <li>Columns for each metric's pass/fail status based on the applied threshold (e.g., <code>Jaccard_passed</code>, <code>ROUGE_rouge1_passed</code>).</li> </ul> </li> </ul> </li> <li><code>aggregate_func</code> (Optional <code>Callable</code>):<ul> <li>An aggregation function (e.g., <code>numpy.mean</code>, <code>numpy.median</code>) used for plotting when multiple scores might exist per model/metric. For the standard <code>Experiment</code> use case (one response per model), this typically doesn't change the outcome of scores but is available for plot customization. Defaults to <code>numpy.mean</code>.</li> </ul> </li> <li><code>plot_title_suffix</code> (Optional <code>str</code>, default <code>\"Comparison\"</code>):<ul> <li>A string suffix added to the titles of generated plots.</li> </ul> </li> <li><code>radar_metrics_limit</code> (Optional <code>int</code>, default <code>12</code>):<ul> <li>The maximum number of metrics to display on a single radar plot to maintain readability. If more metrics are present and a radar plot is applicable, only the first <code>radar_metrics_limit</code> are plotted on that radar chart.</li> </ul> </li> </ul>"},{"location":"user_guide/experiment_class/#what-compare-returns","title":"What <code>compare()</code> Returns","text":"<p>The <code>compare()</code> method returns a pandas DataFrame containing the calculated scores. The DataFrame typically has the following columns:</p> <ul> <li><code>model_name</code>: The name of the LLM (from the keys of your <code>llm_responses</code> dictionary).</li> <li><code>metric_name</code>: The specific metric calculated. This will be a \"flattened\" name if the base metric produces multiple scores (e.g., <code>\"Jaccard\"</code>, <code>\"ROUGE_rouge1\"</code>, <code>\"ROUGE_rougeL\"</code>, <code>\"BERTScore_f1\"</code>).</li> <li><code>score</code>: The numerical score for that model and metric, typically normalized between 0 and 1.</li> </ul> <p>This DataFrame is useful for any further custom analysis, filtering, or reporting you might want to perform.</p>"},{"location":"user_guide/experiment_class/#accessing-scores-separately-with-to_dataframe","title":"Accessing Scores Separately with <code>to_dataframe()</code>","text":"<p>If you only need the scores in a pandas DataFrame without triggering plots or CSV generation, you can use the <code>to_dataframe()</code> method. This can be useful for programmatic access to the scores.</p> <p><pre><code># Get scores for Jaccard and Levenshtein only\nscores_subset_df = exp.to_dataframe(metrics=['Jaccard', 'Levenshtein'])\nprint(\"\\nSubset of scores:\")\nprint(scores_subset_df)\n\n# Get scores for all default metrics (if not already computed, they will be)\nall_scores_df = exp.to_dataframe() # Equivalent to exp.to_dataframe(metrics=None)\nprint(\"\\nAll default scores:\")\nprint(all_scores_df)\n</code></pre> This method is efficient as it uses cached scores if they have already been computed (e.g., by a previous call to <code>compare()</code> or an earlier call to <code>to_dataframe()</code>). If scores for requested metrics haven't been computed yet, <code>to_dataframe()</code> will calculate them.</p>"},{"location":"user_guide/experiment_class/#how-experiment-uses-metrics","title":"How <code>Experiment</code> Uses Metrics","text":"<p>Internally, the <code>Experiment</code> class instantiates and utilizes the individual metric classes (like <code>JaccardSimilarity</code>, <code>ROUGE</code>, <code>BERTScore</code>, etc.) found in the <code>gaico.metrics</code> module. It handles the iteration over your LLM responses, applies each specified metric to compare each response against the single <code>reference_answer</code>, and then aggregates these results into the output DataFrame.</p> <p>For most common comparison tasks involving multiple models against a single reference, the <code>Experiment</code> class provides the most convenient and comprehensive interface. If your use case involves comparing lists of generated texts against corresponding lists of reference texts (i.e., pair-wise comparisons for multiple examples), or if you need to implement highly custom evaluation logic or integrate new, un-registered metrics on the fly, using the individual metric classes directly might be more appropriate.</p>"},{"location":"user_guide/thresholds/","title":"Working with Thresholds","text":"<p>GAICo provides utilities to apply thresholds to metric scores and analyze pass/fail statistics. This is useful for determining if generated text meets a certain quality bar for specific metrics. While the Experiment Class can apply thresholds automatically, you can also use these functions directly.</p>"},{"location":"user_guide/thresholds/#key-threshold-functions","title":"Key Threshold Functions","text":"<p>The primary functions for working with thresholds are located in the <code>gaico.thresholds</code> module:</p> <ul> <li><code>get_default_thresholds()</code>: Returns a dictionary of the library's default threshold values for each base metric.</li> <li><code>apply_thresholds()</code>: Applies specified or default thresholds to a dictionary (or list of dictionaries) of metric scores.</li> <li><code>calculate_pass_fail_percent()</code>: Calculates pass/fail percentages for a collection of scores for each metric.</li> </ul>"},{"location":"user_guide/thresholds/#1-getting-default-thresholds","title":"1. Getting Default Thresholds","text":"<p>You can inspect the default thresholds used by the library:</p> <p><pre><code>from gaico.thresholds import get_default_thresholds\n\ndefault_thresholds = get_default_thresholds()\nprint(\"Default Thresholds:\")\nfor metric, threshold_val in default_thresholds.items():\n    print(f\"- {metric}: {threshold_val}\")\n\n# Example Output:\n# Default Thresholds:\n# - BLEU: 0.5\n# - ROUGE: 0.5\n# - JSD: 0.5\n# - BERTScore: 0.5\n# - Jaccard: 0.5\n# - Cosine: 0.5\n# - Levenshtein: 0.5\n# - SequenceMatcher: 0.5\n</code></pre> The <code>DEFAULT_THRESHOLD</code> constant in <code>gaico.thresholds</code> also holds these values.</p>"},{"location":"user_guide/thresholds/#2-applying-thresholds-with-apply_thresholds","title":"2. Applying Thresholds with <code>apply_thresholds()</code>","text":"<p>This function takes your calculated metric scores and a dictionary of thresholds, then returns a detailed structure indicating the score, the threshold applied, and whether the score passed the threshold.</p>"},{"location":"user_guide/thresholds/#input","title":"Input:","text":"<ul> <li><code>results</code> (<code>Dict[str, Union[float, Any]] | List[Dict[str, Union[float, Any]]]</code>):<ul> <li>For a single evaluation: A dictionary where keys are metric names (e.g., \"Jaccard\", \"ROUGE_rouge1\") and values are their scores.</li> <li>For batch evaluations: A list of such dictionaries.</li> </ul> </li> <li><code>thresholds</code> (Optional <code>Dict[str, float]</code>):<ul> <li>A dictionary mapping metric names to their desired threshold values.</li> <li>If <code>None</code>, <code>get_default_thresholds()</code> is used.</li> <li>Note: For JSDivergence, \"passing\" means <code>(1 - score) &gt;= threshold_value</code> because lower JSD is better. For other metrics, \"passing\" means <code>score &gt;= threshold_value</code>.</li> </ul> </li> </ul>"},{"location":"user_guide/thresholds/#output","title":"Output:","text":"<p>A dictionary (or list of dictionaries) where each metric entry contains: *   <code>\"score\"</code>: The original score. *   <code>\"threshold_applied\"</code>: The threshold value used for this metric. *   <code>\"passed_threshold\"</code>: A boolean indicating if the score passed.</p>"},{"location":"user_guide/thresholds/#example","title":"Example:","text":"<pre><code>from gaico.thresholds import apply_thresholds, get_default_thresholds\n\n# Example scores for a single evaluation\nsingle_eval_scores = {\n    \"Jaccard\": 0.75,\n    \"ROUGE_rouge1\": 0.45,\n    \"Levenshtein\": 0.80,\n    \"JSD\": 0.2 # Lower JSD is better\n}\n\n# Using default thresholds\nthresholded_results_default = apply_thresholds(single_eval_scores)\nprint(\"Thresholded Results (Default):\")\nfor metric, details in thresholded_results_default.items():\n    print(f\"  {metric}: Score={details['score']}, Threshold={details['threshold_applied']}, Passed={details['passed_threshold']}\")\n\n# Thresholded Results (Default):\n#   Jaccard: Score=0.75, Threshold=0.5, Passed=True\n#   Levenshtein: Score=0.8, Threshold=0.5, Passed=True\n#   JSD: Score=0.2, Threshold=0.5, Passed=True\n\n# Using custom thresholds\ncustom_thresholds = {\n    \"Jaccard\": 0.7,\n    \"ROUGE_rouge1\": 0.5,\n    \"Levenshtein\": 0.75,\n    \"JSD\": 0.6 # This means (1 - JSD_score) should be &gt;= 0.6, so JSD_score &lt;= 0.4\n}\nthresholded_results_custom = apply_thresholds(single_eval_scores, thresholds=custom_thresholds)\nprint(\"\\nThresholded Results (Custom):\")\nfor metric, details in thresholded_results_custom.items():\n    print(f\"  {metric}: Score={details['score']}, Threshold={details['threshold_applied']}, Passed={details['passed_threshold']}\")\n\n# Thresholded Results (Custom):\n#   Jaccard: Score=0.75, Threshold=0.7, Passed=True\n#   ROUGE_rouge1: Score=0.45, Threshold=0.5, Passed=False\n#   Levenshtein: Score=0.8, Threshold=0.75, Passed=True\n#   JSD: Score=0.2, Threshold=0.6, Passed=True\n\n# Example for batch results\nbatch_eval_scores = [\n    {\"Jaccard\": 0.8, \"ROUGE_rouge1\": 0.6},\n    {\"Jaccard\": 0.4, \"ROUGE_rouge1\": 0.3}\n]\nthresholded_batch = apply_thresholds(batch_eval_scores, thresholds={\"Jaccard\": 0.5, \"ROUGE_rouge1\": 0.55})\nprint(\"\\nThresholded Batch Results (Custom):\")\nfor i, item_results in enumerate(thresholded_batch):\n    print(f\"  Item {i+1}:\")\n    for metric, details in item_results.items():\n        print(f\"    {metric}: Score={details['score']}, Threshold={details['threshold_applied']}, Passed={details['passed_threshold']}\")\n\n# Thresholded Batch Results (Custom):\n#   Item 1:\n#     Jaccard: Score=0.8, Threshold=0.5, Passed=True\n#     ROUGE_rouge1: Score=0.6, Threshold=0.55, Passed=True\n#   Item 2:\n#     Jaccard: Score=0.4, Threshold=0.5, Passed=False\n#     ROUGE_rouge1: Score=0.3, Threshold=0.55, Passed=False\n</code></pre> <p>The output from <code>apply_thresholds</code> is also what <code>gaico.utils.generate_deltas_frame</code> (used by <code>Experiment.compare()</code> for CSV output) expects.</p>"},{"location":"user_guide/thresholds/#3-calculating-passfail-percentages-with-calculate_pass_fail_percent","title":"3. Calculating Pass/Fail Percentages with <code>calculate_pass_fail_percent()</code>","text":"<p>If you have a collection of scores for multiple items (e.g., from evaluating many generated texts against their references) and want to see overall pass/fail rates for each metric, this function is useful.</p>"},{"location":"user_guide/thresholds/#input_1","title":"Input:","text":"<ul> <li><code>results</code> (<code>Dict[str, List[float]]</code>):     A dictionary where keys are metric names and values are lists of scores obtained for that metric across multiple evaluations.</li> <li><code>thresholds</code> (Optional <code>Dict[str, float]</code>):     Custom thresholds to use. Defaults to <code>get_default_thresholds()</code>.</li> </ul>"},{"location":"user_guide/thresholds/#output_1","title":"Output:","text":"<p>A dictionary where keys are metric names. Each value is another dictionary containing: *   <code>\"total_passed\"</code>: Count of items that passed the threshold. *   <code>\"total_failed\"</code>: Count of items that failed. *   <code>\"pass_percentage\"</code>: Percentage of items that passed. *   <code>\"fail_percentage\"</code>: Percentage of items that failed.</p>"},{"location":"user_guide/thresholds/#example_1","title":"Example:","text":"<pre><code>from gaico.thresholds import calculate_pass_fail_percent\n\n# Example: Scores from multiple evaluations for different metrics\nbatch_scores_for_stats = {\n    \"Jaccard\": [0.8, 0.4, 0.9, 0.6, 0.7],\n    \"Levenshtein\": [0.9, 0.85, 0.6, 0.77, 0.92],\n    \"JSD\": [0.1, 0.5, 0.05, 0.6, 0.2] # Lower is better\n}\n\ncustom_thresholds_for_stats = {\n    \"Jaccard\": 0.7,\n    \"Levenshtein\": 0.8,\n    \"JSD\": 0.6 # (1 - JSD_score) &gt;= 0.6  =&gt; JSD_score &lt;= 0.4\n}\n\npass_fail_stats = calculate_pass_fail_percent(batch_scores_for_stats, thresholds=custom_thresholds_for_stats)\n\nprint(\"\\nPass/Fail Statistics:\")\nfor metric, stats in pass_fail_stats.items():\n    print(f\"  Metric: {metric}\")\n    print(f\"    Total Passed: {stats['total_passed']}\")\n    print(f\"    Total Failed: {stats['total_failed']}\")\n    print(f\"    Pass Percentage: {stats['pass_percentage']:.2f}%\")\n    print(f\"    Fail Percentage: {stats['fail_percentage']:.2f}%\")\n\n# Pass/Fail Statistics:\n#   Metric: Jaccard\n#     Total Passed: 3\n#     Total Failed: 2\n#     Pass Percentage: 60.00%\n#     Fail Percentage: 40.00%\n#   Metric: Levenshtein\n#     Total Passed: 3\n#     Total Failed: 2\n#     Pass Percentage: 60.00%\n#     Fail Percentage: 40.00%\n#   Metric: JSD\n#     Total Passed: 3\n#     Total Failed: 2\n#     Pass Percentage: 60.00%\n#     Fail Percentage: 40.00%\n</code></pre> <p>These thresholding utilities provide flexible ways to interpret your metric scores beyond just their raw values, helping you make more informed decisions about model performance.</p>"},{"location":"user_guide/visualization/","title":"Visualization","text":"<p>GAICo includes functions to help you visualize comparison results, making it easier to understand model performance across different metrics. These functions are used internally by the Experiment Class when <code>plot=True</code> is set in the <code>compare()</code> method, but you can also use them directly for custom plotting needs.</p> <p>The primary visualization functions are found in the <code>gaico.visualize</code> module:</p> <ul> <li><code>plot_metric_comparison()</code>: Generates a bar plot comparing different models based on a single metric.</li> <li><code>plot_radar_comparison()</code>: Generates a radar (spider) plot comparing multiple models across several metrics.</li> </ul> <p>Prerequisites: These functions require <code>matplotlib</code>, <code>seaborn</code>, <code>numpy</code>, and <code>pandas</code> to be installed. GAICo attempts to import them, and will raise an <code>ImportError</code> if they are not available.</p>"},{"location":"user_guide/visualization/#preparing-data-for-plotting","title":"Preparing Data for Plotting","text":"<p>Both plotting functions expect input data as a pandas DataFrame in a \"long\" format. This DataFrame typically has columns for model names, metric names, and scores. The <code>gaico.utils.prepare_results_dataframe()</code> function is designed to convert a nested dictionary of scores into this format.</p>"},{"location":"user_guide/visualization/#prepare_results_dataframe","title":"<code>prepare_results_dataframe()</code>","text":"<p>This utility takes a dictionary where keys are model names and values are dictionaries of metric names to scores (or nested score dictionaries, like those from ROUGE or BERTScore).</p> <p><pre><code>from gaico.utils import prepare_results_dataframe\nimport pandas as pd\n\n# Example raw scores (perhaps from direct metric calculations)\nraw_scores_data = {\n    'Model_A': {'Jaccard': 0.8, 'ROUGE': {'rouge1': 0.75, 'rougeL': 0.70}, 'Levenshtein': 0.85},\n    'Model_B': {'Jaccard': 0.6, 'ROUGE': {'rouge1': 0.65, 'rougeL': 0.60}, 'Levenshtein': 0.70},\n    'Model_C': {'Jaccard': 0.9, 'ROUGE': {'rouge1': 0.80, 'rougeL': 0.78}, 'Levenshtein': 0.92}\n}\n\n# Convert to a long-format DataFrame\nplot_df = prepare_results_dataframe(raw_scores_data)\nprint(\"Prepared DataFrame for Plotting:\")\nprint(plot_df)\n\n# Expected Output:\n# Prepared DataFrame for Plotting:\n#    model_name   metric_name  score\n# 0     Model_A       Jaccard   0.80\n# 1     Model_A  ROUGE_rouge1   0.75\n# 2     Model_A  ROUGE_rougeL   0.70\n# 3     Model_A   Levenshtein   0.85\n# 4     Model_B       Jaccard   0.60\n# 5     Model_B  ROUGE_rouge1   0.65\n# 6     Model_B  ROUGE_rougeL   0.60\n# 7     Model_B   Levenshtein   0.70\n# 8     Model_C       Jaccard   0.90\n# 9     Model_C  ROUGE_rouge1   0.80\n# 10    Model_C  ROUGE_rougeL   0.78\n# 11    Model_C   Levenshtein   0.92\n</code></pre> This <code>plot_df</code> is now ready to be used with the visualization functions.</p>"},{"location":"user_guide/visualization/#1-bar-plot-plot_metric_comparison","title":"1. Bar Plot: <code>plot_metric_comparison()</code>","text":"<p>Use this function to compare models on a single, specific metric.</p>"},{"location":"user_guide/visualization/#key-parameters","title":"Key Parameters:","text":"<ul> <li><code>df</code> (<code>pd.DataFrame</code>): The long-format DataFrame (from <code>prepare_results_dataframe</code>).</li> <li><code>metric_name</code> (<code>str</code>): Required. The name of the metric to plot (e.g., \"Jaccard\", \"ROUGE_rouge1\").</li> <li><code>aggregate_func</code> (Optional <code>Callable</code>): Aggregation function if multiple scores exist per model for the chosen metric (e.g., <code>numpy.mean</code>). Defaults to <code>numpy.mean</code>.</li> <li><code>model_col</code>, <code>score_col</code>, <code>metric_col</code> (Optional <code>str</code>): Names of columns for model, score, and metric. Defaults to \"model_name\", \"score\", \"metric_name\".</li> <li><code>title</code>, <code>xlabel</code>, <code>ylabel</code> (Optional <code>str</code>): Plot customization.</li> <li><code>figsize</code> (Optional <code>tuple</code>): Figure size.</li> <li><code>axis</code> (Optional <code>matplotlib.axes.Axes</code>): Existing Matplotlib Axes to plot on.</li> </ul>"},{"location":"user_guide/visualization/#example","title":"Example:","text":"<p><pre><code>from gaico.visualize import plot_metric_comparison\nimport matplotlib.pyplot as plt # For plt.show()\nimport numpy as np # For np.mean (default aggregate_func)\n\n# Assuming plot_df is available from the previous example\n\n# Plot Jaccard scores\nplot_metric_comparison(plot_df, metric_name=\"Jaccard\", title=\"Jaccard Similarity Comparison\")\nplt.show()\n\n# Plot ROUGE_rouge1 scores\nplot_metric_comparison(plot_df, metric_name=\"ROUGE_rouge1\", title=\"ROUGE-1 F1 Score Comparison\")\nplt.show()\n</code></pre> This will generate and display bar charts, each showing the specified metric's scores for Model_A, Model_B, and Model_C.</p>"},{"location":"user_guide/visualization/#2-radar-plot-plot_radar_comparison","title":"2. Radar Plot: <code>plot_radar_comparison()</code>","text":"<p>Use this function to get a multi-dimensional view of how models perform across several metrics simultaneously. Radar plots are most effective with 3 to 10-12 metrics.</p>"},{"location":"user_guide/visualization/#key-parameters_1","title":"Key Parameters:","text":"<ul> <li><code>df</code> (<code>pd.DataFrame</code>): The long-format DataFrame.</li> <li><code>metrics</code> (Optional <code>List[str]</code>): A list of metric names (e.g., [\"Jaccard\", \"ROUGE_rouge1\", \"Levenshtein\"]) to include in the radar plot. If <code>None</code>, all metrics present in the <code>df</code> for the models are used.</li> <li><code>aggregate_func</code> (Optional <code>Callable</code>): Aggregation function. Defaults to <code>numpy.mean</code>.</li> <li><code>model_col</code>, <code>score_col</code>, <code>metric_col</code> (Optional <code>str</code>): Column names.</li> <li><code>title</code> (Optional <code>str</code>): Plot title.</li> <li><code>figsize</code> (Optional <code>tuple</code>): Figure size.</li> <li><code>axis</code> (Optional <code>matplotlib.axes.Axes</code>): Existing Matplotlib polar Axes to plot on.</li> </ul>"},{"location":"user_guide/visualization/#example_1","title":"Example:","text":"<p><pre><code>from gaico.visualize import plot_radar_comparison\nimport matplotlib.pyplot as plt # For plt.show()\nimport numpy as np # For np.mean\n\n# Assuming plot_df is available\n\n# Define which metrics to include in the radar plot\nmetrics_for_radar = [\"Jaccard\", \"ROUGE_rougeL\", \"Levenshtein\"]\n\nplot_radar_comparison(plot_df, metrics=metrics_for_radar, title=\"Overall Model Performance Radar\")\nplt.show()\n\n# If you want to plot all available metrics (that have scores for the models)\n# plot_radar_comparison(plot_df, title=\"Overall Model Performance Radar (All Metrics)\")\n# plt.show()\n</code></pre> This will generate a radar plot where each axis represents one of the <code>metrics_for_radar</code>, and each model (Model_A, Model_B, Model_C) is represented by a colored shape connecting its scores on these axes.</p> <p>By using these visualization tools directly, you can create custom plots tailored to specific analyses or integrate them into larger reporting dashboards. Remember to have the necessary plotting libraries installed in your environment.</p>"}]}