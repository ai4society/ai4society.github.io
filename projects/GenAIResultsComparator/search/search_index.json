{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to GAICo","text":"GAICo Quickstart Demonstration <p>GenAI Results Comparator (GAICo) is a Python library for comparing, analyzing, and visualizing outputs from Large Language Models (LLMs). It offers an extensible range of metrics, including standard text similarity scores, specialized metrics for structured data like planning sequences and time-series, and multimedia metrics for image and audio.</p> <p>\ud83e\udd73 Papers accepted at IAAI/AAAI 2026 and AAAI Demonstrations 2026!</p> <p>We're pleased to announce that our paper has been accepted at the 38th Annual Conference on Innovative Applications of Artificial Intelligence (IAAI/AAAI-26) and at the Demonstration Program for the 40th Annual AAAI Conference on Artificial Intelligence (AAAI-26). Read the preprint: </p>"},{"location":"#video-demo","title":"Video Demo","text":"<p>Latest release updates</p> <p>Stay up to date with the latest GAICo releases, changelogs, and project news: Release notes and news</p>"},{"location":"#description","title":"Description","text":"<p>At its core, the library provides a set of metrics for evaluating various types of outputs\u2014from plain text strings to structured data like planning sequences and time-series, and multimedia content such as images and audio. While the <code>Experiment</code> class streamlines evaluation for text-based and structured string outputs, individual metric classes offer direct control for all data types, including binary or array-based multimedia. These metrics produce normalized scores (typically 0 to 1), where 1 indicates a perfect match, enabling robust analysis and visualization of LLM performance.</p> <p>Class Structure: All metrics are implemented as extensible classes inheriting from <code>BaseMetric</code>. Each metric requires just one method: <code>calculate()</code>.</p> <p>The <code>calculate()</code> method takes two main parameters:</p> <ul> <li><code>generated_texts</code>: A single generated output or an iterable (list, numpy array, etc.) of outputs.</li> <li><code>reference_texts</code>: A single reference output or an iterable of outputs.</li> </ul> <p>Important</p> <p>Handling Missing References: If <code>reference_texts</code> is <code>None</code> or empty, GAICo will automatically use the first item from <code>generated_texts</code> as the reference for comparison. A warning will be printed to the console.</p> <p>Note</p> <p>Batch Processing: When you provide iterables as input, <code>calculate()</code> assumes a one-to-one mapping between generated and reference items. If a single reference is provided for multiple generated items, it will be broadcasted for comparison against each one.</p> <p>Note</p> <p>Optional Dependencies: The standard <code>pip install gaico</code> is lightweight. Some metrics with heavy dependencies (like <code>BERTScore</code> or <code>JSDivergence</code>) require optional installation.</p> <p>Inspiration: The design and evaluation metrics are inspired by Microsoft's article on evaluating LLM-generated content. GAICo currently focuses on reference-based metrics.</p> Overview of the workflow supported by the GAICo library"},{"location":"#features","title":"Features","text":"<ul> <li>Comprehensive Metric Library:</li> <li>Textual Similarity: Jaccard, Cosine, Levenshtein, Sequence Matcher.</li> <li>N-gram Based: BLEU, ROUGE, JS Divergence.</li> <li>Semantic Similarity: BERTScore.</li> <li>Structured Data: Specialized metrics for planning sequences (<code>PlanningLCS</code>, <code>PlanningJaccard</code>) and time-series data (<code>TimeSeriesElementDiff</code>, <code>TimeSeriesDTW</code>).</li> <li>Multimedia: Metrics for image similarity (<code>ImageSSIM</code>, <code>ImageAverageHash</code>, <code>ImageHistogramMatch</code>) and audio quality (<code>AudioSNRNormalized</code>, <code>AudioSpectrogramDistance</code>).</li> <li>Streamlined Evaluation Workflow:</li> <li>A high-level <code>Experiment</code> class to easily compare multiple models, apply thresholds, generate plots, and create CSV reports.</li> <li>Enhanced Reporting:</li> <li>A <code>summarize()</code> method for quick, aggregated overviews of model performance, including mean scores and pass rates.</li> <li>Dynamic Metric Registration:</li> <li>Easily extend the <code>Experiment</code> class by registering your own custom <code>BaseMetric</code> implementations at runtime.</li> <li>Powerful Visualization:</li> <li>Generate bar charts and radar plots to compare model performance using Matplotlib and Seaborn.</li> <li>Efficient &amp; Flexible:</li> <li>Supports batch processing for efficient computation on datasets.</li> <li>Optimized for various input types (lists, NumPy arrays, Pandas Series).</li> <li>Easily extensible architecture for adding new custom metrics.</li> <li>Robust and Reliable:</li> <li>Includes a comprehensive test suite using Pytest.     </li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Important</p> <p>We strongly recommend using a Python virtual environment to manage dependencies and avoid conflicts with other packages.</p> <p>GAICo can be installed using pip.</p> <ul> <li>Create and activate a virtual environment (e.g., named <code>gaico-env</code>):</li> </ul> <pre><code>  # For Python 3.10+\n  python3 -m venv gaico-env\n  source gaico-env/bin/activate  # On macOS/Linux\n  # gaico-env\\Scripts\\activate   # On Windows\n</code></pre> <ul> <li>Install GAICo:   Once your virtual environment is active, install GAICo using pip:</li> </ul> <pre><code>  pip install gaico\n</code></pre> <p>This installs the core GAICo library.</p>"},{"location":"#using-gaico-with-jupyter-notebookslab","title":"Using GAICo with Jupyter Notebooks/Lab","text":"<p>If you plan to use GAICo within Jupyter Notebooks or JupyterLab (recommended for exploring examples and interactive analysis), install them into the same activated virtual environment:</p> <pre><code># (Ensure your 'gaico-env' is active)\npip install notebook  # For Jupyter Notebook\n# OR\n# pip install jupyterlab # For JupyterLab\n</code></pre> <p>Then, launch Jupyter from the same terminal where your virtual environment is active:</p> <pre><code># (Ensure your 'gaico-env' is active)\njupyter notebook\n# OR\n# jupyter lab\n</code></pre> <p>New notebooks created in this session should automatically use the <code>gaico-env</code> Python environment. For troubleshooting kernel issues, please see our FAQ document.</p>"},{"location":"#optional-installations","title":"Optional Installations","text":"<p>The default <code>pip install gaico</code> is lightweight. Some metrics require extra dependencies, which you can install as needed.</p> <ul> <li>To include Audio metrics (requires SciPy and SoundFile):   <pre><code>pip install 'gaico[audio]'\n</code></pre></li> <li>To include the BERTScore metric (which has larger dependencies like PyTorch):   <pre><code>pip install 'gaico[bertscore]'\n</code></pre></li> <li>To include the CosineSimilarity metric (requires scikit-learn):   <pre><code>pip install 'gaico[cosine]'\n</code></pre></li> <li>To include the JSDivergence metric (requires SciPy and NLTK):   <pre><code>pip install 'gaico[jsd]'\n</code></pre></li> <li>To install with all optional features:   <pre><code>pip install 'gaico[audio,bertscore,cosine,jsd]'\n</code></pre></li> </ul> <p>Tip</p> <p>The <code>dev</code> extra, used for development installs, also includes all optional features.</p>"},{"location":"#installation-size-comparison","title":"Installation Size Comparison","text":"<p>The following table provides an estimated overview of the relative disk space impact of different installation options. Actual sizes may vary depending on your operating system, Python version, and existing packages. These are primarily to illustrate the relative impact of optional dependencies.</p> <p>Note: Core dependencies include: <code>levenshtein</code>, <code>matplotlib</code>, <code>numpy</code>, <code>pandas</code>, <code>rouge-score</code>, and <code>seaborn</code>.</p> Installation Command Dependencies Estimated Total Size Impact <code>pip install gaico</code> Core 215 MB <code>pip install 'gaico[audio]'</code> Core + <code>scipy</code>, <code>soundfile</code> 330 MB <code>pip install 'gaico[bertscore]'</code> Core + <code>bert-score</code> (includes <code>torch</code>, <code>transformers</code>, etc.) 800 MB <code>pip install 'gaico[cosine]'</code> Core + <code>scikit-learn</code> 360 MB <code>pip install 'gaico[jsd]'</code> Core + <code>scipy</code>, <code>nltk</code> 310 MB <code>pip install 'gaico[audio,jsd,cosine,bertscore]'</code> Core + all dependencies from above 1.0 GB"},{"location":"#for-developers-installing-from-source","title":"For Developers (Installing from source)","text":"<p>If you want to contribute to GAICo or install it from source for development:</p> <ol> <li> <p>Clone the repository:</p> <pre><code>git clone https://github.com/ai4society/GenAIResultsComparator.git\ncd GenAIResultsComparator\n</code></pre> </li> <li> <p>Set up a virtual environment and install dependencies:</p> <p>We recommend using UV for fast environment and dependency management.</p> <pre><code># Create a virtual environment (Python 3.10-3.12 recommended)\nuv venv\n# Activate the environment\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n# Install in editable mode with all development dependencies\nuv pip install -e \".[dev]\"\n</code></pre> <p>If you prefer not to use <code>uv</code>, you can use <code>pip</code>:</p> <pre><code># Create a virtual environment (Python 3.10-3.12 recommended)\npython3 -m venv .venv\n# Activate the environment\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n# Install the package in editable mode with development extras\npip install -e \".[dev]\"\n</code></pre> <p>The <code>dev</code> extra installs GAICo with all optional features, plus dependencies for testing, linting, and documentation.</p> </li> <li> <p>Set up pre-commit hooks (recommended for contributors):</p> <p>Pre-commit hooks help maintain code quality by running checks automatically before you commit.</p> <pre><code>pre-commit install\n</code></pre> </li> </ol>"},{"location":"#citation","title":"Citation","text":"<p>If you find this project useful, please consider citing it in your work. You may use our ArXiv version for now:</p> <pre><code>@article{gupta2025gaico,\n  title={GAICo: A Deployed and Extensible Framework for Evaluating Diverse and Multimodal Generative AI Outputs},\n  author={Gupta, Nitin and Koppisetti, Pallav and Lakkaraju, Kausik and Srivastava, Biplav},\n  journal={arXiv preprint arXiv:2508.16753},\n  year={2025}\n}\n</code></pre>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<ul> <li>The library is developed by Nitin Gupta, Pallav Koppisetti, Kausik Lakkaraju, and Biplav Srivastava. Members of AI4Society contributed to this tool as part of ongoing discussions. Major contributors are credited.</li> <li>This library uses several open-source packages including NLTK, scikit-learn, and others. Special thanks to the creators and maintainers of the implemented metrics.</li> </ul>"},{"location":"#contact","title":"Contact","text":"<p>If you have any questions, feel free to reach out to us at ai4societyteam@gmail.com.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome! Please feel free to submit a Pull Request.</p> <ol> <li>Fork the repository</li> <li>Create your feature branch (<code>git checkout -b feature/FeatureName</code>)</li> <li>Commit your changes (<code>git commit -m 'Add some FeatureName'</code>)</li> <li>Push to the branch (<code>git push origin feature/FeatureName</code>)</li> <li>Open a Pull Request</li> </ol> <p>Please ensure that your code passes all tests and adheres to our code style guidelines (enforced by pre-commit hooks) before submitting a pull request.</p>"},{"location":"examples/","title":"Examples","text":"<p>Detailed, runnable examples are available as Jupyter Notebooks in the <code>examples</code> folder:</p> <ul> <li> <p><code>quickstart.ipynb</code>:</p> <p>Using GAICo's <code>Experiment</code> module to provide a simple, quickstart workflow.</p> </li> <li> <p><code>example-1.ipynb</code>:</p> <p>Evaluating multiple models (LLMs, Google, and Custom) using a single metric.</p> </li> <li> <p><code>example-2.ipynb</code>:</p> <p>Evaluating a single model on multiple metrics.</p> </li> <li> <p><code>DeepSeek-example.ipynb</code></p> <p>The aim for this notebook was to aid with evaluating DeepSeek R1 for AI4Society's Point of View (POV).</p> </li> </ul>"},{"location":"examples/#advanced-examples","title":"Advanced Examples","text":"<p>The <code>advanced-examples</code> directory contains advances notebooks showcasing more complex use cases and metrics. These examples are intended for users who are already familiar with the basics of GAICo. Please refer to the README.md file in that directory for details. A quick description:</p> <ul> <li> <p><code>llm_faq-example.ipynb</code></p> <p>Comparison of various LLM responses (Phi, Mixtral, etc.) on FAQ dataset from USC.</p> </li> <li> <p><code>threshold-example.ipynb</code></p> <p>Exploration of default and custom thresholding techniques for LLM responses.</p> </li> <li> <p><code>viz-example.ipynb</code></p> <p>Hands-on visualizations for LLM results.</p> </li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>Here are answers to some common questions about using GAICo.</p>"},{"location":"faq/#q-my-jupyter-notebook-isnt-working-correctly-with-gaico-eg-import-errors-version-conflicts-what-should-i-do","title":"Q: My Jupyter Notebook isn't working correctly with GAICo (e.g., import errors, version conflicts). What should I do?","text":"<p>This usually happens if Jupyter Notebook/Lab is not using the Python environment where GAICo was installed. Here's how to troubleshoot:</p> <ol> <li> <p>Ensure Virtual Environment Usage:</p> <ul> <li>Always create and activate a Python virtual environment before installing GAICo and Jupyter.   <pre><code># Example:\npython3 -m venv my-gaico-env\nsource my-gaico-env/bin/activate  # macOS/Linux\n# my-gaico-env\\Scripts\\activate    # Windows\n</code></pre></li> <li>Install GAICo and Jupyter (Notebook/Lab) into this activated environment:   <pre><code>pip install gaico notebook  # Or jupyterlab\n</code></pre></li> <li>Launch Jupyter from the same terminal where the virtual environment is active:   <pre><code>jupyter notebook # Or jupyter lab\n</code></pre></li> </ul> </li> <li> <p>Select the Correct Kernel in Jupyter:</p> <ul> <li>When you open or create a notebook, Jupyter needs to use the kernel associated with your virtual environment.</li> <li>In the Jupyter Notebook menu, go to Kernel &gt; Change kernel.</li> <li>You should see an option corresponding to your virtual environment. It might be named after the environment folder (e.g., \"my-gaico-env\"), simply \"Python 3\", or a display name you set up. Select it.</li> <li>If the kernel restarts, allow it. Then try running your GAICo code again.</li> </ul> </li> <li> <p>Explicitly Registering the Kernel (if the above doesn't resolve it):     If Jupyter consistently fails to find or use your virtual environment's kernel, you can make it explicitly available:</p> <ul> <li>Make sure your virtual environment is active.</li> <li>Install <code>ipykernel</code> if it's not already there (it's usually a dependency of <code>notebook</code> or <code>jupyterlab</code>):   <pre><code>pip install ipykernel\n</code></pre></li> <li>Register the environment as a Jupyter kernel:   <pre><code>python -m ipykernel install --user --name=my-gaico-env --display-name=\"Python (my-gaico-env)\"\n</code></pre>   (Replace <code>my-gaico-env</code> with your actual environment name and choose a descriptive <code>display-name</code>.)</li> <li>Restart your Jupyter server (close the terminal window where it's running and relaunch it from the activated venv).</li> <li>Now, in your notebook, go to Kernel &gt; Change kernel and select the \"Python (my-gaico-env)\" (or your chosen display name).</li> </ul> </li> <li> <p>Check <code>sys.path</code> and <code>sys.executable</code>:     If problems persist, add a cell to your notebook with the following to verify which Python environment is being used:</p> <pre><code>import sys\nimport numpy # A core dependency\nimport gaico\n\nprint(\"Python Executable:\", sys.executable)\nprint(\"\\nSystem Path:\")\nfor p in sys.path:\n    print(p)\nprint(\"\\nNumPy version:\", numpy.__version__)\nprint(\"NumPy location:\", numpy.__file__)\nprint(\"\\nGAICo location:\", gaico.__file__)\n</code></pre> <p>The <code>sys.executable</code> and the paths for <code>numpy</code> and <code>gaico</code> should all point to directories within your active virtual environment (e.g., <code>.../my-gaico-env/lib/python3.x/site-packages/...</code>). If they point to a global Python installation or a different environment, the kernel is not set up correctly.</p> </li> </ol>"},{"location":"faq/#q-how-do-i-add-a-new-custom-metric","title":"Q: How do I add a new custom metric?","text":"<p>Adding a new metric to GAICo is designed to be straightforward. All metrics inherit from the <code>BaseMetric</code> class.</p> <ol> <li> <p>Create a New Metric Class:</p> <ul> <li>Your new metric class should inherit from <code>gaico.metrics.base.BaseMetric</code>.</li> <li>You must implement two methods:</li> <li><code>_single_calculate(self, generated_text: str, reference_text: str, **kwargs: Any) -&gt; Union[float, dict]</code>: This method calculates the metric for a single pair of texts.</li> <li><code>_batch_calculate(self, generated_texts: Union[Iterable, np.ndarray, pd.Series], reference_texts: Union[Iterable, np.ndarray, pd.Series], **kwargs: Any) -&gt; Union[List[float], List[dict], np.ndarray, pd.Series, float, dict]</code>: This method calculates the metric for a batch of texts. Often, this can be implemented by iterating and calling <code>_single_calculate</code>, but you can optimize it for batch operations if possible.</li> <li>The public <code>calculate()</code> method is inherited from <code>BaseMetric</code> and handles input type checking and dispatching to either <code>_single_calculate</code> or <code>_batch_calculate</code>.</li> </ul> </li> <li> <p>Example Structure:</p> <pre><code>from typing import Any, Iterable, List, Union\nimport numpy as np\nimport pandas as pd\nfrom gaico.metrics.base import BaseMetric\n\nclass MyCustomMetric(BaseMetric):\n    def __init__(self, custom_param: str = \"default\"):\n        self.custom_param = custom_param\n        # Add any other initialization logic\n\n    def _single_calculate(\n        self, generated_text: str, reference_text: str, **kwargs: Any\n    ) -&gt; float:\n        # Your logic to compare generated_text and reference_text\n        # Example:\n        score = 0.0\n        if self.custom_param in generated_text and self.custom_param in reference_text:\n            score = 1.0\n        # Ensure score is normalized between 0 and 1\n        return score\n\n    def _batch_calculate(\n        self,\n        generated_texts: Union[Iterable, np.ndarray, pd.Series],\n        reference_texts: Union[Iterable, np.ndarray, pd.Series],\n        **kwargs: Any,\n    ) -&gt; Union[List[float], np.ndarray, pd.Series]:\n        # Default batch implementation (can be optimized)\n        results = [\n            self._single_calculate(gen, ref, **kwargs)\n            for gen, ref in zip(generated_texts, reference_texts)\n        ]\n        if isinstance(generated_texts, np.ndarray):\n            return np.array(results)\n        elif isinstance(generated_texts, pd.Series):\n            return pd.Series(results, index=generated_texts.index)\n        return results\n</code></pre> </li> <li> <p>Using Your Custom Metric:     You can directly instantiate and use your custom metric:</p> <pre><code>from gaico.metrics.base import BaseMetric # Or your custom metric file\n# from your_module import MyCustomMetric # If defined in a separate file\n\n# Assuming MyCustomMetric is defined as above\ncustom_metric = MyCustomMetric(custom_param=\"example\")\nscore = custom_metric.calculate(\"This is an example generated text.\", \"This is an example reference text.\")\nprint(f\"Custom metric score: {score}\")\n</code></pre> </li> <li> <p>(Optional) Registering with the <code>Experiment</code> Class:     If you want your custom metric to be usable by its string name within the <code>Experiment</code> class (e.g., <code>exp.compare(metrics=['MyCustomMetric'])</code>), you'll need to add it to the <code>REGISTERED_METRICS</code> dictionary in <code>gaico/experiment.py</code>:</p> <pre><code># In gaico/experiment.py\n# ... other imports ...\n# from ..metrics.my_custom_metric_module import MyCustomMetric # Adjust import path\n\nREGISTERED_METRICS: Dict[str, type[BaseMetric]] = {\n    \"Jaccard\": JaccardSimilarity,\n    \"Cosine\": CosineSimilarity,\n    # ... other default metrics ...\n    \"MyCustomMetric\": MyCustomMetric, # Add your metric here\n}\n</code></pre> <p>You would also typically add it to <code>DEFAULT_METRICS_TO_RUN</code> in the same file if you want it to run by default when <code>metrics=None</code> in <code>Experiment.compare()</code>.</p> <p>Refer to existing metric implementations in <code>gaico/metrics/</code> for more detailed examples (e.g., <code>text_similarity_metrics.py</code>).</p> </li> </ol>"},{"location":"faq/#q-how-do-i-get-the-list-of-all-supported-metrics-or-use-them-directly","title":"Q: How do I get the list of all supported metrics or use them directly?","text":"<p>There are a couple of ways to understand and access \"supported metrics\":</p> <ol> <li> <p>All metric classes available for direct instantiation from <code>gaico.metrics</code>:     These are all the metric classes defined in the <code>gaico.metrics</code> sub-package that are intended for public use. They are typically listed in <code>gaico.metrics.__init__.__all__</code>.</p> <ul> <li>To import and use specific metric classes by name:   This is the recommended way for clarity.</li> </ul> <pre><code>from gaico.metrics import JaccardSimilarity, BLEU, BERTScore\n\njaccard_scorer = JaccardSimilarity()\nbleu_scorer = BLEU()\nbert_scorer = BERTScore(model_type=\"distilbert-base-uncased\") # Example of passing params\n\nscore1 = jaccard_scorer.calculate(\"text a\", \"text b\")\nscore2 = bleu_scorer.calculate(\"text a\", \"text b\")\n</code></pre> <ul> <li>To import all available metric classes using <code>*</code>:   This makes all metric classes from <code>gaico.metrics.__init__.__all__</code> available in your current namespace.</li> </ul> <pre><code>from gaico.metrics import *\n\n# Now you can use the class names directly\njaccard_scorer = JaccardSimilarity()\nrouge_scorer = ROUGE()\n# ... and so on for all metrics in gaico.metrics.__all__\n\nscore = jaccard_scorer.calculate(\"text a\", \"text b\")\n</code></pre> <p>While convenient, using <code>import *</code> can sometimes make it less clear where names are coming from in larger projects. For <code>gaico.metrics</code>, which is focused, it's generally acceptable.</p> <ul> <li>To see what's available (programmatically):</li> </ul> <pre><code>import gaico.metrics as gaico_metrics\n\n# Metrics explicitly listed in gaico.metrics.__init__.__all__\navailable_metric_class_names = gaico_metrics.__all__\nprint(\"Available metric classes for direct use (class names):\", available_metric_class_names)\n# Example Output:\n# Available metric classes for direct use (class names): ['BLEU', 'ROUGE', 'JSDivergence', 'BERTScore', 'CosineSimilarity', 'JaccardSimilarity', 'LevenshteinDistance', 'SequenceMatcherSimilarity']\n</code></pre> <p>Note that the class names (e.g., <code>JaccardSimilarity</code>) might be different from the shorter keys used in <code>REGISTERED_METRICS</code> (e.g., <code>Jaccard</code>).</p> </li> <li> <p>Metrics available by string name in the <code>Experiment</code> class:     These are the metrics registered in <code>gaico.experiment.REGISTERED_METRICS</code>. You can access this dictionary programmatically to see which short names are recognized by the <code>Experiment</code> class:</p> <pre><code>from gaico.experiment import REGISTERED_METRICS\n\nregistered_metric_names = list(REGISTERED_METRICS.keys())\nprint(\"Metrics usable by name in Experiment class:\", registered_metric_names)\n# Example Output:\n# ['Jaccard', 'Cosine', 'Levenshtein', 'SequenceMatcher', 'BLEU', 'ROUGE', 'JSD', 'BERTScore']\n</code></pre> <p>The <code>gaico.experiment.DEFAULT_METRICS_TO_RUN</code> list also shows which of these are run by default if no specific metrics are requested in <code>Experiment.compare()</code>:</p> <pre><code>from gaico.experiment import DEFAULT_METRICS_TO_RUN\n\nprint(\"Default metrics for Experiment class:\", DEFAULT_METRICS_TO_RUN)\n</code></pre> </li> </ol> <p>If you have other questions, please open an issue on our GitHub repository!</p>"},{"location":"license/","title":"License","text":"<p>This project is licensed under the MIT License:</p> <pre><code>MIT License\n\nCopyright (c) 2024 AI for Society Research Group\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"news/","title":"GAICo Release News","text":"<p>This page details the major releases of the GAICo library, highlighting key features and providing quick start examples.</p>"},{"location":"news/#v030-august-2025","title":"v0.3.0 - August 2025","text":"<p>This release significantly expands GAICo's capabilities by introducing new multimedia metrics for image and audio. It also contains enhancement to the <code>Experiment</code> class by adding support for batch-processing, summarization, and allowing dynamic registration of custom metrics.</p>"},{"location":"news/#key-features","title":"Key Features:","text":"<ul> <li>Multimedia Metrics:<ul> <li>Image: Added <code>ImageSSIM</code>, <code>ImageAverageHash</code>, and <code>ImageHistogramMatch</code> for comparing visual content.</li> <li>Audio: Introduced <code>AudioSNRNormalized</code> and <code>AudioSpectrogramDistance</code> for evaluating audio signals.</li> </ul> </li> <li>Experiment Class Enhancements:<ul> <li>Batch Processing: Added support for processing multiple experiments in a single run.</li> <li>Summarization: Introduced methods for generating summary reports of experiment results.</li> <li>Dynamic Metric Registration: Enabled users to register custom metrics dynamically.</li> </ul> </li> </ul>"},{"location":"news/#quick-start-examples","title":"Quick Start Examples","text":""},{"location":"news/#1-multimedia-image-only","title":"1. Multimedia (Image-only)","text":"<p>Evaluate image outputs against a reference using the new image metrics.</p> <pre><code>from PIL import Image\nfrom gaico import Experiment\n\n# Tiny demo images\nref = Image.new(\"RGB\", (32, 32), color=(255, 0, 0))    # pure red\nimg_a = Image.new(\"RGB\", (32, 32), color=(254, 0, 0))   # nearly red\nimg_b = Image.new(\"RGB\", (32, 32), color=(0, 0, 255))   # blue\n\nexp = Experiment(\n  llm_responses={\n    \"Model A\": img_a,\n    \"Model B\": img_b,\n  },\n  reference_answer=ref,\n)\n\ndf = exp.compare(\n  metrics=[\"ImageSSIM\", \"ImageAverageHash\", \"ImageHistogramMatch\"],\n  plot=False,\n)\nprint(df.head())\n</code></pre>"},{"location":"news/#2-experiment-enhancements-batch-summarize","title":"2) Experiment Enhancements (batch + summarize)","text":"<p>Run batch comparisons and get a compact summary with aggregated scores and pass rates.</p> <pre><code>from gaico import Experiment\n\n# Batch of 3 reference items\nrefs = [\n  \"The capital is Paris.\",\n  \"2+2=4\",\n  \"Blue is a color.\",\n]\n\n# Two models with 3 outputs each (same length as refs)\nmodel_a = [\n  \"Paris is the capital.\",\n  \"2 + 2 equals four\",\n  \"Blue is a color.\",\n]\nmodel_b = [\n  \"London is the capital.\",\n  \"2+2=5\",\n  \"Sky is blue.\",\n]\n\nexp = Experiment(\n  llm_responses={\"A\": model_a, \"B\": model_b},\n  reference_answer=refs,\n)\n\n# Detailed per-item scores (subset of metrics)\nscores = exp.to_dataframe(metrics=[\"Jaccard\", \"ROUGE\"])\nprint(scores.head())\n\n# Compact summary with pass rates and custom thresholds\nsummary = exp.summarize(\n  metrics=[\"Jaccard\", \"ROUGE\"],\n  custom_thresholds={\"Jaccard\": 0.5, \"ROUGE_rouge1\": 0.4},\n)\nprint(summary)\n\n# Optional: one-call compare + CSV report\nexp.compare(metrics=[\"Jaccard\", \"ROUGE\"], output_csv_path=\"v030_example.csv\")\n</code></pre>"},{"location":"news/#v020-july-2025","title":"v0.2.0 - July 2025","text":"<p>This release expands GAICo's capabilities by introducing specialized metrics for structured data: automated planning and time series.</p>"},{"location":"news/#key-features_1","title":"Key Features:","text":"<ul> <li>Structured Data Metrics:</li> <li>Automated Planning: Added <code>PlanningLCS</code> and <code>PlanningJaccard</code> for analyzing planning sequences.</li> <li>Time-Series: Introduced metrics like <code>TimeSeriesElementDiff</code> and <code>TimeSeriesDTW</code> for evaluating time-series data.</li> </ul>"},{"location":"news/#quick-start-example","title":"Quick Start Example:","text":"<p>This example demonstrates using the <code>Experiment</code> class with a mix of general and specialized metrics.</p> <pre><code>from gaico import Experiment\n\nexp = Experiment(\n    llm_responses={\n        \"Model A\": \"t1:1, t2:2, t3:3, t4:4, t5:3, t6:2\",\n        \"Model B\": \"t1:1, t2:2.1, t3:3.4, t4:8, t5:5\",\n    },\n    reference_answer=\"t1:1, t2:2.2, t3:3.1, t4:4, t5:3.5\",\n)\n\n# Compare using general text metrics and specialized metrics\nresults_df = exp.compare(\n    metrics=['BLEU', 'JSD', 'Levenshtein', 'TimeSeriesDTW', 'TimeSeriesElementDiff'],\n    plot=True,\n    output_csv_path=\"experiment_release_020.csv\"\n)\n</code></pre> GAICo v0.2.0 Quick Start Example Output"},{"location":"news/#v015-june-2025","title":"v0.1.5 - June 2025","text":"<p>This initial release of GAICo focused on providing a solid foundation for comparing general text outputs from LLMs, including core similarity metrics, the <code>Experiment</code> class, and basic visualization tools.</p>"},{"location":"news/#key-features_2","title":"Key Features:","text":"<ul> <li>Core Text Similarity Metrics: Included fundamental metrics such as Jaccard, Levenshtein, Cosine Similarity, and ROUGE.</li> <li><code>Experiment</code> Class: Introduced a high-level abstraction for simplifying evaluation workflows, including multi-model comparison and report generation.</li> <li>Basic Visualizations: Enabled the creation of bar charts and radar plots for visualizing metric scores.</li> <li>Extensible Architecture: Designed for easy addition of new metrics.</li> </ul>"},{"location":"news/#quick-start-example_1","title":"Quick Start Example:","text":"<p>This example showcases the basic usage of the <code>Experiment</code> class for comparing general text responses.</p> <pre><code>from gaico import Experiment\n\n# Sample data from https://arxiv.org/abs/2504.07995\nllm_responses = {\n    \"Google\": \"Title: Jimmy Kimmel Reacts to Donald Trump Winning the Presidential ... Snippet: Nov 6, 2024 ...\",\n    \"Mixtral 8x7b\": \"I'm an Al and I don't have the ability to predict the outcome of elections.\",\n    \"SafeChat\": \"Sorry, I am designed not to answer such a question.\",\n}\nreference_answer = \"Sorry, I am unable to answer such a question as it is not appropriate.\"\n\n# 1. Initialize Experiment\nexp = Experiment(\n    llm_responses=llm_responses,\n    reference_answer=reference_answer\n)\n\n# 2. Compare models using specific metrics\nresults_df = exp.compare(\n    metrics=['Jaccard', 'ROUGE'],  # Specify metrics, or None for all defaults\n    plot=True,\n    output_csv_path=\"experiment_report_015.csv\"\n)\n</code></pre> GAICo v0.1.5 Quick Start Example Output"},{"location":"quickstart/","title":"Quick Start","text":"<p>GAICo makes it easy to evaluate and compare LLM outputs. For detailed, runnable examples, please refer to our Jupyter Notebooks in the <code>examples/</code> folder:</p> <ul> <li><code>quickstart.ipynb</code>: Rapid hands-on with the Experiment sub-module.</li> <li><code>example-1.ipynb</code>: For fine-grained usage, this notebook focuses on comparing multiple model outputs using a single metric.</li> <li><code>example-2.ipynb</code>: For fine-grained usage, this notebook demonstrates evaluating a single model output across all available metrics.</li> </ul>"},{"location":"quickstart/#streamlined-workflow-with-experiment","title":"Streamlined Workflow with <code>Experiment</code>","text":"<p>For a more integrated approach to comparing multiple models, applying thresholds, generating plots, and creating CSV reports, the <code>Experiment</code> class offers a convenient abstraction.</p>"},{"location":"quickstart/#quick-example","title":"Quick Example","text":"<p>This example demonstrates comparing multiple LLM responses against a reference answer using specified metrics, generating a plot, and outputting a CSV report.</p> <pre><code>from gaico import Experiment\n\n# Sample data from https://arxiv.org/abs/2504.07995\nllm_responses = {\n    \"Google\": \"Title: Jimmy Kimmel Reacts to Donald Trump Winning the Presidential ... Snippet: Nov 6, 2024 ...\",\n    \"Mixtral 8x7b\": \"I'm an Al and I don't have the ability to predict the outcome of elections.\",\n    \"SafeChat\": \"Sorry, I am designed not to answer such a question.\",\n}\nreference_answer = \"Sorry, I am unable to answer such a question as it is not appropriate.\"\n\n# 1. Initialize Experiment\nexp = Experiment(\n    llm_responses=llm_responses,\n    reference_answer=reference_answer\n)\n\n# 2. Compare models using specific metrics\n#   This will calculate scores for 'Jaccard' and 'ROUGE',\n#   generate a plot (e.g., radar plot for multiple metrics/models),\n#   and save a CSV report.\nresults_df = exp.compare(\n    metrics=['Jaccard', 'ROUGE'],  # Specify metrics, or None for all defaults\n    plot=True,\n    output_csv_path=\"experiment_report.csv\",\n    custom_thresholds={\"Jaccard\": 0.6, \"ROUGE_rouge1\": 0.35} # Optional: override default thresholds\n)\n\n# The returned DataFrame contains the calculated scores\nprint(\"Scores DataFrame from compare():\")\nprint(results_df)\n</code></pre> <p>This abstraction streamlines common evaluation tasks, while still allowing access to the underlying metric classes and dataframes for more advanced or customized use cases. More details in <code>examples/quickstart.ipynb</code>.</p> <p>However, you might prefer to use the individual metric classes directly for more granular control or if you want to implement custom metrics. See the remaining notebooks in the <code>examples</code> subdirectory.</p> Example Radar Chart generated by the <code>examples/example-2.ipynb</code> notebook."},{"location":"api/experiment/","title":"Experiment Class","text":""},{"location":"api/experiment/#gaico.Experiment","title":"gaico.Experiment","text":"<p>An abstraction to simplify plotting, applying thresholds, and generating CSVs for comparing LLM responses against reference answers using various metrics.</p>"},{"location":"api/experiment/#gaico.Experiment.__init__","title":"__init__","text":"<pre><code>__init__(llm_responses, reference_answer)\n</code></pre> <p>Initializes the Experiment for single or batch evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>llm_responses</code> <code>Dict[str, Any]</code> <p>A dictionary mapping model names (str) to their generated outputs. For batch evaluation, values should be lists of outputs. e.g., {\"ModelA\": [\"resp1\", \"resp2\"], \"ModelB\": [\"resp1\", \"resp2\"]}</p> required <code>reference_answer</code> <code>Optional[Any]</code> <p>A single reference output or a list of references for batch evaluation. If None, the output(s) from the first model will be used as the reference.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If llm_responses is not a dictionary.</p> <code>ValueError</code> <p>If inputs are inconsistent (e.g., mixing single and list-like responses).</p>"},{"location":"api/experiment/#gaico.Experiment.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe(metrics=None)\n</code></pre> <p>Returns a DataFrame of scores for the specified metrics. If metrics is None, scores for all default metrics are returned.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Optional[List[str]]</code> <p>A list of base metric names (e.g., \"Jaccard\", \"ROUGE\"). Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A pandas DataFrame with columns \"model_name\", \"metric_name\", \"score\". \"metric_name\" will contain flat metric names (e.g., \"ROUGE_rouge1\").</p>"},{"location":"api/experiment/#gaico.Experiment.register_metric","title":"register_metric","text":"<pre><code>register_metric(name, metric_class)\n</code></pre> <p>Registers a custom metric class for use in this Experiment instance. This allows users to extend GAICo with their own custom metrics and use them seamlessly with the Experiment's <code>compare()</code> and <code>summarize()</code> methods.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to refer to this metric by (e.g., \"MyCustomMetric\").</p> required <code>metric_class</code> <code>type[BaseMetric]</code> <p>The class (must inherit from BaseMetric).</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If metric_class is not a subclass of gaico.BaseMetric.</p>"},{"location":"api/experiment/#gaico.Experiment.summarize","title":"summarize","text":"<pre><code>summarize(metrics=None, custom_thresholds=None, agg_funcs=None)\n</code></pre> <p>Calculates and returns a summary DataFrame with aggregated scores and pass rates for each model and metric.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Optional[List[str]]</code> <p>List of base metric names to include in the summary. If None, uses all metrics that have been calculated or can be calculated.</p> <code>None</code> <code>custom_thresholds</code> <code>Optional[Dict[str, float]]</code> <p>Optional dictionary mapping flat metric names (e.g., \"Jaccard\", \"ROUGE_rouge1\") or base metric names (e.g., \"ROUGE\") to custom threshold values. If provided, these will override default thresholds for pass rate calculation.</p> <code>None</code> <code>agg_funcs</code> <code>Optional[List[str]]</code> <p>List of aggregation functions (as strings, e.g., 'mean', 'std', 'min', 'max') to apply to scores. Defaults to ['mean', 'std'].</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A summary DataFrame with aggregated scores and pass rates. Columns will include 'model_name', and then aggregated score columns (e.g., 'Jaccard_mean', 'ROUGE_rouge1_std') and pass rate columns (e.g., 'Jaccard_pass_rate').</p>"},{"location":"api/experiment/#gaico.Experiment.compare","title":"compare","text":"<pre><code>compare(metrics=None, plot=False, custom_thresholds=None, output_csv_path=None, aggregate_func=None, plot_title_suffix='Comparison', radar_metrics_limit=12)\n</code></pre> <p>Compares models based on specified metrics, optionally plotting and generating a CSV. Handles both single-item and batch (dataset) evaluations.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Optional[List[str]]</code> <p>List of base metric names. If None, uses all default registered metrics.</p> <code>None</code> <code>plot</code> <code>bool</code> <p>If True, generates and shows plots. For batch data, plots are aggregated.</p> <code>False</code> <code>custom_thresholds</code> <code>Optional[Dict[str, float]]</code> <p>Dictionary of metric names to threshold values.</p> <code>None</code> <code>output_csv_path</code> <code>Optional[str]</code> <p>If provided, path to save a detailed CSV report.</p> <code>None</code> <code>aggregate_func</code> <code>Optional[Callable]</code> <p>Aggregation function (e.g., np.mean) for plotting batch results.</p> <code>None</code> <code>plot_title_suffix</code> <code>str</code> <p>Suffix for plot titles.</p> <code>'Comparison'</code> <code>radar_metrics_limit</code> <code>int</code> <p>Maximum number of metrics for a radar plot.</p> <code>12</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A pandas DataFrame containing the detailed scores for all items.</p>"},{"location":"api/thresholds_module/","title":"Thresholds Module","text":""},{"location":"api/thresholds_module/#gaico.thresholds.apply_thresholds","title":"gaico.thresholds.apply_thresholds","text":"<pre><code>apply_thresholds(results, thresholds=None)\n</code></pre> <p>Apply thresholds to scores for single pair or batch of generated and reference texts. Type ThresholdedResults is a dictionary where keys are metric names and values are either scores or dictionaries with scores. Specifically, it is of type Dict[str, float | Any].</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>ThresholdedResults | List[ThresholdedResults]</code> <p>Either a single dictionary of scores or a list of score dictionaries Single: {\"BLEU\": 0.6, \"JSD\": 0.1} Batch: [{\"BLEU\": 0.6, \"JSD\": 0.1}, {\"BLEU\": 0.4, \"JSD\": 0.2}]</p> required <code>thresholds</code> <code>Optional[Dict[str, float]]</code> <p>Dictionary of metric names to threshold values. Defaults to get_default_thresholds() if not provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>ThresholdedResults | List[ThresholdedResults]</code> <p>For single input, returns a dictionary. For batch input, returns a list. Single: {\"BLEU\": {\"score\": 0.6, \"threshold_applied\": 0.5, \"passed_threshold\": True}, ...} Batch: [{\"BLEU\": {\"score\": 0.6, ...}, ...}, {\"BLEU\": {\"score\": 0.4, ...}, ...}]</p>"},{"location":"api/thresholds_module/#gaico.thresholds.get_default_thresholds","title":"gaico.thresholds.get_default_thresholds","text":"<pre><code>get_default_thresholds()\n</code></pre> <p>Returns the default thresholds for each metric. This is useful for testing and can be overridden by user-defined thresholds.</p> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>A dictionary of default thresholds for each metric. e.g., {\"BLEU\": 0.5, \"JSD\": 0.5}</p>"},{"location":"api/thresholds_module/#gaico.thresholds.calculate_pass_fail_percent","title":"gaico.thresholds.calculate_pass_fail_percent","text":"<pre><code>calculate_pass_fail_percent(results, thresholds=None)\n</code></pre> <p>Calculate pass/fail percentages for each metric across results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>Dict[str, List[float]]</code> <p>Dictionary where keys are metric names and values are lists of scores</p> required <code>thresholds</code> <code>Optional[Dict[str, float]]</code> <p>Dictionary of thresholds for each metric</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, float | int]]</code> <p>Dictionary with metric names as keys and pass/fail statistics as values</p>"},{"location":"api/thresholds_module/#gaico.thresholds.DEFAULT_THRESHOLD","title":"gaico.thresholds.DEFAULT_THRESHOLD  <code>module-attribute</code>","text":"<pre><code>DEFAULT_THRESHOLD = {'BLEU': 0.5, 'ROUGE': 0.5, 'JSD': 0.5, 'BERTScore': 0.5, 'Jaccard': 0.5, 'Cosine': 0.5, 'Levenshtein': 0.5, 'SequenceMatcher': 0.5, 'ActionSequenceDiff': 0.5, 'TimeSeriesElementDiff': 0.5, 'TimeSeriesDTW': 0.5, 'ImageSSIM': 0.5, 'ImageAverageHash': 0.5, 'ImageHistogramMatch': 0.5, 'AudioSNR': 0.5, 'SpectrogramDistance': 0.5}\n</code></pre>"},{"location":"api/utils/","title":"Utility Functions","text":""},{"location":"api/utils/#gaico.utils.prepare_results_dataframe","title":"gaico.utils.prepare_results_dataframe","text":"<pre><code>prepare_results_dataframe(results_dict, model_col='model_name', metric_col='metric_name', score_col='score', index_col='item_index')\n</code></pre> <p>Converts a nested dictionary of results into a long-format DataFrame suitable for plotting. Handles both single-item results and batch results (lists of scores).</p> <p>Example Input <code>results_dict</code> (batch): {     'ModelA': {'BLEU': [0.8, 0.85], 'ROUGE': [{'f1': 0.75}, {'f1': 0.78}]}, } Example Output DataFrame:    item_index model_name metric_name  score 0           0     ModelA        BLEU   0.80 1           1     ModelA        BLEU   0.85 2           0     ModelA    ROUGE_f1   0.75 3           1     ModelA    ROUGE_f1   0.78</p> <p>Parameters:</p> Name Type Description Default <code>results_dict</code> <code>Dict[str, Dict[str, Any]]</code> <p>Nested dictionary of results. Scores can be single values or lists.</p> required <code>model_col</code> <code>str</code> <p>Name for the model column.</p> <code>'model_name'</code> <code>metric_col</code> <code>str</code> <p>Name for the metric column.</p> <code>'metric_name'</code> <code>score_col</code> <code>str</code> <p>Name for the score column.</p> <code>'score'</code> <code>index_col</code> <code>str</code> <p>Name for the item index column in batch mode.</p> <code>'item_index'</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A pandas DataFrame in long format.</p>"},{"location":"api/utils/#gaico.utils.generate_deltas_frame","title":"gaico.utils.generate_deltas_frame","text":"<pre><code>generate_deltas_frame(threshold_results, generated_texts=None, reference_texts=None, output_csv_path=None)\n</code></pre> <p>Generate a Pandas DataFrame from threshold function outputs with optional text strings. If <code>output_csv_path</code> is provided, it saves the DataFrame to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>threshold_results</code> <code>Dict[str, Dict[str, Any]] | List[Dict[str, Dict[str, Any]]]</code> <p>Output from apply_thresholds (handles both single and batch) Single: {\"BLEU\": {\"score\": 0.6, \"threshold_applied\": 0.5, \"passed_threshold\": True}, ...} Batch: [{\"BLEU\": {\"score\": 0.6, ...}, ...}, {\"BLEU\": {\"score\": 0.4, ...}, ...}]</p> required <code>generated_texts</code> <code>Optional[str | List[str]]</code> <p>Optional generated text string(s)</p> <code>None</code> <code>reference_texts</code> <code>Optional[str | List[str]]</code> <p>Optional reference text string(s)</p> <code>None</code> <code>output_csv_path</code> <code>Optional[str]</code> <p>Optional path to save the CSV file</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A Pandas DataFrame containing the results</p>"},{"location":"api/visualize_module/","title":"Visualization Module","text":""},{"location":"api/visualize_module/#gaico.visualize.plot_metric_comparison","title":"gaico.visualize.plot_metric_comparison","text":"<pre><code>plot_metric_comparison(df, aggregate_func=None, **kwargs)\n</code></pre> <p>Generates a bar plot comparing different models based on a single metric, after aggregating scores using the provided aggregate_func.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the scores, typically from prepare_results_dataframe. Expected columns are defined by model_col, metric_col, and score_col in kwargs.</p> required <code>aggregate_func</code> <code>Optional[Callable]</code> <p>A function to aggregate scores (e.g., numpy.mean, numpy.median). Defaults to numpy.mean if None.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments: - metric_name (str, required): The name of the metric to plot. - model_col (str, optional): Name of the column identifying models. Defaults to \"model_name\". - score_col (str, optional): Name of the column containing scores. Defaults to \"score\". - metric_col (str, optional): Name of the column containing metric names. Defaults to \"metric_name\". - title (Optional[str], optional): Title for the plot. - xlabel (Optional[str], optional): Label for the x-axis. Defaults to \"Model\". - ylabel (Optional[str], optional): Label for the y-axis. Defaults to the plotted metric's name. - figsize (tuple, optional): Figure size. Defaults to (10, 6). - axis (Optional[matplotlib.axes.Axes], optional): Matplotlib Axes to plot on. - Other kwargs are passed to seaborn.barplot.</p> <code>{}</code> <p>Returns:</p> Type Description <code>matplotlib.axes.Axes</code> <p>The matplotlib Axes object containing the plot.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If required libraries (matplotlib, seaborn, pandas, numpy) are not installed.</p> <code>ValueError</code> <p>If 'metric_name' is not provided in kwargs.</p>"},{"location":"api/visualize_module/#gaico.visualize.plot_radar_comparison","title":"gaico.visualize.plot_radar_comparison","text":"<pre><code>plot_radar_comparison(df, aggregate_func=None, **kwargs)\n</code></pre> <p>Generates a radar plot comparing multiple models across several metrics, after aggregating scores using the provided aggregate_func.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the scores in long format, typically from prepare_results_dataframe. Expected columns are defined by model_col, metric_col, and score_col in kwargs.</p> required <code>aggregate_func</code> <code>Optional[Callable]</code> <p>A function to aggregate scores (e.g., numpy.mean, numpy.median). Defaults to numpy.mean if None.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments: - metrics (List[str], optional): List of metric names to include. If None, all metrics in df are used. - model_col (str, optional): Name of the column identifying models. Defaults to \"model_name\". - score_col (str, optional): Name of the column containing scores. Defaults to \"score\". - metric_col (str, optional): Name of the column containing metric names. Defaults to \"metric_name\". - title (Optional[str], optional): Title for the plot. Defaults to \"Model Comparison Radar Plot\". - figsize (tuple, optional): Figure size. Defaults to (8, 8). - fill_alpha (float, optional): Alpha for filled area. Defaults to 0.1. - line_width (float, optional): Width of plot lines. Defaults to 1.0. - y_ticks (Optional[List[float]], optional): Custom y-axis ticks. - axis (Optional[matplotlib.axes.Axes], optional): Matplotlib polar Axes to plot on.</p> <code>{}</code> <p>Returns:</p> Type Description <code>matplotlib.axes.Axes</code> <p>The matplotlib Axes object containing the plot.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If required libraries (matplotlib, numpy, pandas) are not installed.</p> <code>ValueError</code> <p>If aggregation results in no data or metrics.</p>"},{"location":"api/metrics/","title":"Metrics","text":"<p>GAICo provides several metric classes for text evaluation. All metrics inherit from <code>BaseMetric</code>.</p> <p>To make a custom metric, you can create a new class that inherits from <code>BaseMetric</code> and implements the <code>calculate()</code> method.</p> <p>To view the documentation for a specific metric, you can click on the metric name in the table of contents on the left.</p>"},{"location":"api/metrics/audio/","title":"Audio Metrics","text":"<p>This section covers metrics for evaluating audio signals, supporting various input formats including numpy arrays, file paths, and raw audio data. These metrics are particularly useful for evaluating audio generation models, speech synthesis systems, and general audio processing applications.</p> <p>The <code>AudioSNRNormalized</code> metric provides a normalized Signal-to-Noise Ratio (SNR) comparison between generated and reference audio signals. Unlike traditional SNR measurements that can range from negative infinity to positive infinity, this metric normalizes the result to a [0, 1] range for consistency with other GAICo metrics, where 1.0 indicates perfect audio quality (identical signals) and 0.0 indicates maximum noise/distortion.</p>"},{"location":"api/metrics/audio/#gaico.metrics.audio.AudioSNRNormalized","title":"gaico.metrics.audio.AudioSNRNormalized","text":"<p>               Bases: <code>AudioMetric</code></p> <p>Calculate a normalized Signal-to-Noise Ratio (SNR) between two audio signals.</p> <p>The metric computes the standard SNR in decibels (dB) and then scales this value to a normalized range of [0, 1]. A score of 1.0 indicates identical signals (infinite SNR), while a score of 0.0 indicates high levels of noise or distortion, at or below the configured <code>snr_min</code> threshold.</p>"},{"location":"api/metrics/audio/#gaico.metrics.audio.AudioSNRNormalized.__init__","title":"__init__","text":"<pre><code>__init__(snr_min=-20.0, snr_max=40.0, epsilon=1e-10, sample_rate=None, **kwargs)\n</code></pre> <p>Initialize the AudioSNRNormalized metric.</p> <p>Parameters:</p> Name Type Description Default <code>snr_min</code> <code>float</code> <p>The minimum SNR (in dB) that maps to a normalized score of 0.0. Defaults to -20.0.</p> <code>-20.0</code> <code>snr_max</code> <code>float</code> <p>The maximum SNR (in dB) that maps to a normalized score of 1.0. Defaults to 40.0.</p> <code>40.0</code> <code>epsilon</code> <code>float</code> <p>A small value added to power calculations to prevent division by zero. Defaults to 1e-10.</p> <code>1e-10</code> <code>sample_rate</code> <code>Optional[int]</code> <p>Target sample rate for audio processing. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>snr_min</code> is not less than <code>snr_max</code>.</p>"},{"location":"api/metrics/audio/#input-format","title":"Input Format","text":"<p>The metric accepts various audio input formats, providing flexibility for different use cases. It intelligently handles both single-item and batch comparisons.</p> <ul> <li>Single Item Formats:</li> <li>NumPy Array: A 1D <code>np.ndarray</code> representing a mono audio waveform.</li> <li>File Path: A string path to an audio file (e.g., <code>.wav</code>, <code>.flac</code>).</li> <li>Batch Item Formats:</li> <li>List/Tuple/Pandas Series: An iterable where each element is a single audio item (either a path or a 1D array).</li> <li>2D NumPy Array: An array where each row is treated as a separate audio signal.</li> <li>Mixed Formats: Generated and reference inputs can use different formats (e.g., comparing a file path to a NumPy array).</li> <li>Audio Preprocessing:</li> <li>Stereo to Mono: Stereo signals (2D arrays or files) are automatically converted to mono by averaging the channels.</li> <li>Resampling: If sample rates differ, the generated audio is resampled to match the reference audio's rate.</li> </ul>"},{"location":"api/metrics/audio/#error-handling","title":"Error Handling","text":"<p>The metric is designed to be robust and will raise specific errors for invalid inputs:</p> <ul> <li><code>FileNotFoundError</code>: If a string path to an audio file does not exist.</li> <li><code>TypeError</code>: If an unsupported data type (e.g., a dictionary) is provided as input.</li> <li><code>ValueError</code>: If an audio array or list is empty, or if an audio file cannot be read.</li> </ul>"},{"location":"api/metrics/audio/#calculation","title":"Calculation","text":"<p>The AudioSNRNormalized metric follows a multi-step process to ensure robust and meaningful comparisons:</p> <ol> <li> <p>Audio Loading and Preprocessing:</p> <ul> <li>Load audio from the specified input format (path, array, etc.).</li> <li>If audio is stereo, convert it to mono by averaging the channels.</li> <li>If sample rates differ, resample the generated audio to match the reference rate.</li> <li>Ensure both signals have the same length by truncating the longer one.</li> </ul> </li> <li> <p>Noise Calculation:</p> <ul> <li>Compute noise as the element-wise difference between the signals: <code>noise = generated - reference</code>.</li> </ul> </li> <li> <p>SNR Computation:</p> <ul> <li>Calculate the power of the reference signal and the noise signal.</li> <li>Compute the SNR in decibels (dB): <code>SNR_dB = 10 * log\u2081\u2080(signal_power / noise_power)</code>.</li> <li>An epsilon value is added to prevent division by zero.</li> </ul> </li> <li> <p>Normalization:</p> <ul> <li>Linearly scale the SNR (dB) to a [0, 1] range using configurable <code>snr_min</code> and <code>snr_max</code> values.</li> <li>Clip the final score to ensure it falls strictly within the [0, 1] range.</li> </ul> </li> </ol>"},{"location":"api/metrics/audio/#usage","title":"Usage","text":"<pre><code>from gaico.metrics.audio import AudioSNRNormalized\nimport numpy as np\n\n# Initialize with default parameters\nsnr_metric = AudioSNRNormalized()\n\n# Example 1: Compare numpy arrays (typical programmatic use)\n# Generate a clean sine wave\nt = np.linspace(0, 1, 44100)  # 1 second at 44.1 kHz\nclean_signal = np.sin(2 * np.pi * 440 * t).astype(np.float32)  # 440 Hz tone\n\n# Add some noise to create a \"generated\" signal\nnoise = 0.05 * np.random.randn(44100).astype(np.float32)\nnoisy_signal = clean_signal + noise\n\nscore = snr_metric.calculate(noisy_signal, clean_signal)\nprint(f\"SNR Score: {score:.3f}\")\n# Expected output: SNR Score: 0.856 (high score due to low noise)\n\n# Example 2: Compare audio files (typical file-based workflow)\ngenerated_file = \"path/to/generated_speech.wav\"\nreference_file = \"path/to/reference_speech.wav\"\n\nscore = snr_metric.calculate(generated_file, reference_file)\nprint(f\"File-based SNR Score: {score:.3f}\")\n\n# Example 3: Custom SNR range for specific application\n# For speech synthesis, you might want different thresholds\nspeech_snr_metric = AudioSNRNormalized(\n    snr_min=-10.0,  # Acceptable speech quality lower bound\n    snr_max=30.0,   # High-quality speech upper bound\n    sample_rate=16000  # Common speech sampling rate\n)\n\n# Example 4: Batch processing multiple audio comparisons\ngenerated_audios = [noisy_signal, noisy_signal * 0.5, clean_signal]\nreference_audios = [clean_signal, clean_signal, clean_signal]\n\nbatch_scores = snr_metric.calculate(generated_audios, reference_audios)\nprint(f\"Batch SNR Scores: {[f'{s:.3f}' for s in batch_scores]}\")\n# Expected output: ['0.856', '0.923', '1.000']\n</code></pre> <p>The <code>AudioSpectrogramDistance</code> metric evaluates audio similarity by comparing spectrograms (frequency-time representations) rather than raw waveforms. This approach is particularly effective for capturing timbral and spectral characteristics, making it suitable for evaluating music generation, speech synthesis, and audio effects processing where frequency content matters more than exact waveform matching.</p>"},{"location":"api/metrics/audio/#gaico.metrics.audio.AudioSpectrogramDistance","title":"gaico.metrics.audio.AudioSpectrogramDistance","text":"<p>               Bases: <code>AudioMetric</code></p> <p>Calculate similarity based on the distance between audio spectrograms.</p> <p>This metric is effective for capturing differences in frequency content (timbre, harmonics) over time. It computes the Short-Time Fourier Transform (STFT) for both signals, calculates a distance between the resulting spectrograms, and converts this distance to a similarity score from 0 to 1.</p>"},{"location":"api/metrics/audio/#gaico.metrics.audio.AudioSpectrogramDistance.__init__","title":"__init__","text":"<pre><code>__init__(n_fft=2048, hop_length=512, distance_type='euclidean', window='hann', sample_rate=None, **kwargs)\n</code></pre> <p>Initialize the AudioSpectrogramDistance metric.</p> <p>Parameters:</p> Name Type Description Default <code>n_fft</code> <code>int</code> <p>The length of the FFT window, determining frequency resolution. Defaults to 2048.</p> <code>2048</code> <code>hop_length</code> <code>int</code> <p>The number of samples between successive FFT windows, determining time resolution. Defaults to 512.</p> <code>512</code> <code>distance_type</code> <code>str</code> <p>The type of distance to calculate between spectrograms. Options: \"euclidean\", \"cosine\", \"correlation\". Defaults to \"euclidean\".</p> <code>'euclidean'</code> <code>window</code> <code>str</code> <p>The window function to apply to each FFT frame. Defaults to \"hann\".</p> <code>'hann'</code> <code>sample_rate</code> <code>Optional[int]</code> <p>Target sample rate for audio processing. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported <code>distance_type</code> is provided.</p>"},{"location":"api/metrics/audio/#input-format_1","title":"Input Format","text":"<p>The metric accepts the same diverse input formats as <code>AudioSNRNormalized</code> (file paths, NumPy arrays, lists, etc.) and follows the same preprocessing and error handling logic. Additional considerations for spectral analysis include:</p> <ul> <li>Audio Duration: Longer audio clips provide more reliable spectral analysis.</li> <li>Sample Rate Consistency: While automatic resampling is supported, providing signals with consistent sample rates yields the most accurate results.</li> <li>Minimum Length: Very short audio clips (e.g., shorter than the <code>n_fft</code> size) will raise a <code>ValueError</code> as a reliable spectrogram cannot be computed.</li> </ul> <p>Recommended Input Characteristics: <pre><code># Optimal for spectral analysis (at least 0.1 seconds of audio)\noptimal_length = int(0.1 * sample_rate)  # 4410 samples at 44.1 kHz\naudio_signal = np.random.randn(optimal_length)\n\n# Music/speech applications typically use these sample rates\nsample_rates = {\n    'speech': 16000,      # Common for speech processing\n    'music': 44100,       # CD quality\n    'professional': 48000  # Professional audio\n}\n</code></pre></p>"},{"location":"api/metrics/audio/#calculation_1","title":"Calculation","text":"<p>The AudioSpectrogramDistance metric employs Short-Time Fourier Transform (STFT) analysis followed by distance computation:</p> <ol> <li> <p>Spectrogram Computation:</p> <ul> <li>Check if <code>scipy</code> is available, raising an <code>ImportError</code> if not.</li> <li>Apply STFT using <code>scipy.signal.stft</code> with configurable parameters (<code>n_fft</code>, <code>hop_length</code>, <code>window</code>).</li> <li>Extract the magnitude spectrogram: <code>magnitude = |STFT(audio)|</code>.</li> </ul> </li> <li> <p>Temporal Alignment:</p> <ul> <li>Ensure spectrograms have matching time dimensions by truncating the longer one to match the shorter one.</li> </ul> </li> <li> <p>Distance Calculation (configurable via <code>distance_type</code>):</p> <p>Euclidean Distance (default): - Computes the Euclidean distance between the flattened spectrograms and normalizes it by the average magnitude to handle scale differences.</p> <p>Cosine Distance:    - Compute normalized cross-correlation after mean removal    - Formula: <code>correlation = dot(spec1_centered, spec2_centered) / (||spec1_centered|| \u00d7 ||spec2_centered||)</code>    - Distance: <code>distance = 1 - correlation</code>    - Captures linear relationships in spectral content</p> </li> <li> <p>Similarity Conversion:</p> </li> <li>Transform distance to similarity using exponential decay: <code>similarity = exp(-distance)</code></li> <li>Maps distance [0, \u221e) to similarity (1, 0], where 1.0 indicates identical spectrograms</li> <li>Apply clipping to ensure [0, 1] range</li> </ol>"},{"location":"api/metrics/audio/#usage_1","title":"Usage","text":"<pre><code>from gaico.metrics.audio import AudioSpectrogramDistance\nimport numpy as np\n\n# Initialize with default parameters (Euclidean distance)\nspec_metric = AudioSpectrogramDistance()\n\n# Example 1: Compare harmonic content\n# Generate two sine waves with different frequencies\nt = np.linspace(0, 1, 44100)\nsignal_440hz = np.sin(2 * np.pi * 440 * t)  # A4 note\nsignal_880hz = np.sin(2 * np.pi * 880 * t)  # A5 note (octave higher)\nsignal_440hz_copy = signal_440hz + 0.01 * np.random.randn(44100)  # Slight noise\n\nscore_identical = spec_metric.calculate(signal_440hz, signal_440hz_copy)\nscore_different = spec_metric.calculate(signal_440hz, signal_880hz)\n\nprint(f\"Similar frequency content: {score_identical:.3f}\")  # ~0.95-0.99\nprint(f\"Different frequency content: {score_different:.3f}\")  # ~0.60-0.80\n\n# Example 2: Music analysis with custom parameters\n# Optimized for music: longer window for better frequency resolution\nmusic_metric = AudioSpectrogramDistance(\n    n_fft=4096,           # Higher frequency resolution\n    hop_length=1024,      # 75% overlap maintained\n    distance_type=\"cosine\", # Pattern-based comparison\n    window=\"blackman\"     # Reduced spectral leakage\n)\n\n# Example 3: Speech analysis with appropriate parameters\n# Optimized for speech: shorter window for better time resolution\nspeech_metric = AudioSpectrogramDistance(\n    n_fft=1024,           # ~64ms window at 16kHz\n    hop_length=256,       # 75% overlap\n    distance_type=\"correlation\",\n    sample_rate=16000     # Common speech sample rate\n)\n\n# Example 4: Compare different distance types\nmetrics_comparison = {\n    'euclidean': AudioSpectrogramDistance(distance_type=\"euclidean\"),\n    'cosine': AudioSpectrogramDistance(distance_type=\"cosine\"),\n    'correlation': AudioSpectrogramDistance(distance_type=\"correlation\")\n}\n\n# Complex signal with harmonics\nfundamental = 220  # A3\ncomplex_signal = (\n    np.sin(2 * np.pi * fundamental * t) +           # Fundamental\n    0.5 * np.sin(2 * np.pi * 2 * fundamental * t) + # 2nd harmonic\n    0.25 * np.sin(2 * np.pi * 3 * fundamental * t)  # 3rd harmonic\n)\n\n# Signal with shifted harmonics (different timbre)\nshifted_signal = (\n    0.7 * np.sin(2 * np.pi * fundamental * t) +\n    0.6 * np.sin(2 * np.pi * 2 * fundamental * t) +\n    0.4 * np.sin(2 * np.pi * 4 * fundamental * t)  # 4th instead of 3rd harmonic\n)\n\nfor name, metric in metrics_comparison.items():\n    score = metric.calculate(complex_signal, shifted_signal)\n    print(f\"{name.capitalize()} distance score: {score:.3f}\")\n\n# Example 5: Batch processing for audio dataset evaluation\ngenerated_samples = [signal_440hz, signal_880hz, complex_signal]\nreference_samples = [signal_440hz_copy, signal_440hz, shifted_signal]\n\nbatch_scores = spec_metric.calculate(generated_samples, reference_samples)\nprint(f\"Batch spectrogram scores: {[f'{s:.3f}' for s in batch_scores]}\")\n</code></pre>"},{"location":"api/metrics/base/","title":"BaseMetric","text":""},{"location":"api/metrics/base/#gaico.metrics.base.BaseMetric","title":"gaico.metrics.base.BaseMetric","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all language model metrics. This class defines the interface that all metric classes should implement. The public method to be accessed is <code>calculate</code>.</p>"},{"location":"api/metrics/base/#gaico.metrics.base.BaseMetric.calculate","title":"calculate","text":"<pre><code>calculate(generated, reference, **kwargs)\n</code></pre> <p>Calculates the metric for a single or batch of generated and reference items. This method handles both single and batch inputs.</p> <p>If the reference is None and <code>generated</code> is an iterable, the function will assume the first element of the iterable as the reference. A warning will be printed.</p> <p>Parameters:</p> Name Type Description Default <code>generated</code> <code>Any</code> <p>A single generated item or an iterable of generated items. Must not be None.</p> required <code>reference</code> <code>Optional[Any]</code> <p>A single reference item, an iterable of reference items, or None.</p> required <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments for specific metrics.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The calculated metric score(s).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>generated</code> is None, or if batch inputs have mismatched lengths.</p> <code>TypeError</code> <p>If inputs cannot be converted to suitable iterables.</p>"},{"location":"api/metrics/image/","title":"Image Metrics","text":"<p>This module provides a suite of image similarity metrics designed to compare generated and reference images across perceptual, structural, and low-level feature dimensions. Metrics support single and batch mode comparisons with input formats including NumPy arrays and PIL Images.</p>"},{"location":"api/metrics/image/#imagessim-structural-similarity-index","title":"ImageSSIM (Structural Similarity Index)","text":"<p>Path: <code>gaico.metrics.image.ImageSSIM</code></p> <p>Structural Similarity Index measures perceived quality based on structural information, luminance, and contrast. ImageSSIM returns a similarity score between 0 and 1, where 1 indicates identical images.</p>"},{"location":"api/metrics/image/#features","title":"Features","text":"<ul> <li>Per-channel ImageSSIM for RGB images</li> <li>Normalized to [0, 1]</li> <li>Optional auto-resize for mismatched shapes</li> <li>Handles both grayscale and color images</li> </ul>"},{"location":"api/metrics/image/#input-formats","title":"Input Formats","text":"<ul> <li><code>np.ndarray</code> (2D grayscale or 3D RGB)</li> <li><code>PIL.Image</code></li> <li>Float arrays are rescaled internally to 8-bit</li> </ul>"},{"location":"api/metrics/image/#example","title":"Example","text":"<pre><code>from gaico.metrics.image import ImageSSIM\nfrom PIL import Image\nimport numpy as np\n\nmetric = ImageSSIM()\nimg1 = np.array(Image.open(\"ref.png\"))\nimg2 = np.array(Image.open(\"gen.png\"))\n\nscore = metric.calculate(img2, img1)\nprint(f\"ImageSSIM Score: {score:.3f}\")\n\n\n## ImageAverageHash (aHash)\n\n**Path:** `gaico.metrics.image.ImageAverageHash`\n\nThe ImageAverageHash metric compares two images using perceptual hashing. It converts each image to an 8\u00d78 grayscale image, computes the mean pixel value, and generates a binary hash based on whether each pixel is above or below the mean. The similarity score is the Hamming similarity (1 - normalized Hamming distance) between the two hashes.\n\n### Features\n- Perceptual image similarity metric\n- Invariant to minor changes (blur, brightness)\n- Efficient, lightweight, no dependencies beyond PIL and NumPy\n- Score in [0.0, 1.0], where 1.0 means identical hashes\n\n### Input Formats\n- `np.ndarray` (RGB or grayscale)\n- `PIL.Image.Image`\n\n### Internals\n1. Convert both inputs to grayscale and resize to 8\u00d78\n2. Compute mean pixel value for each\n3. Generate binary hash: 1 if pixel &gt; mean, else 0\n4. Compute Hamming similarity: 1 - (# differing bits / 64)\n\n### Limitations\n- Not sensitive to high-frequency content (e.g., texture)\n- Cannot detect small geometric misalignments\n\n### Example\n```python\nfrom gaico.metrics.image import ImageAverageHash\nfrom PIL import Image\n\nmetric = ImageAverageHash()\nimg1 = Image.open(\"ref.jpg\")\nimg2 = Image.open(\"gen.jpg\")\n\nscore = metric.calculate(img2, img1)\nprint(f\"aHash Similarity Score: {score:.3f}\")\n</code></pre>"},{"location":"api/metrics/image/#imagehistogrammatch","title":"ImageHistogramMatch","text":"<p>Path: <code>gaico.metrics.image.ImageHistogramMatch</code></p> <p>The <code>ImageHistogramMatch</code> metric evaluates similarity between two images based on their color histograms. It computes the intersection between histograms across the RGB channels, normalized to the range [0.0, 1.0].</p>"},{"location":"api/metrics/image/#features_1","title":"Features","text":"<ul> <li>Histogram-based perceptual similarity</li> <li>Captures color distribution differences</li> <li>Channel-wise histogram intersection</li> <li>Normalized score \u2208 [0.0, 1.0]</li> </ul>"},{"location":"api/metrics/image/#input-formats_1","title":"Input Formats","text":"<ul> <li><code>np.ndarray</code> (RGB)</li> <li><code>PIL.Image.Image</code></li> </ul>"},{"location":"api/metrics/image/#internals","title":"Internals","text":"<ol> <li>Convert inputs to RGB</li> <li>Compute histogram for each channel (default 256 bins)</li> <li>Use histogram intersection: <code>similarity = sum(min(ref_hist[i], gen_hist[i])) / sum(gen_hist)</code></li> <li>Average across channels</li> </ol>"},{"location":"api/metrics/image/#parameters","title":"Parameters","text":"<ul> <li><code>bins</code>: Number of bins per channel (default: 256)</li> </ul>"},{"location":"api/metrics/image/#limitations","title":"Limitations","text":"<ul> <li>Sensitive to lighting changes and minor shifts</li> <li>Ignores spatial layout of pixels</li> </ul>"},{"location":"api/metrics/image/#example_1","title":"Example","text":"<pre><code>from gaico.metrics.image import ImageHistogramMatch\nfrom PIL import Image\n\nmetric = ImageHistogramMatch()\nimg1 = Image.open(\"ref.jpg\")\nimg2 = Image.open(\"gen.jpg\")\n\nscore = metric.calculate(img2, img1)\nprint(f\"ImageHistogramMatch Score: {score:.3f}\")\n</code></pre>"},{"location":"api/metrics/ngram_metrics/","title":"N-gram Metrics","text":"<p>GAICo provides several n-gram-based metrics for evaluating text similarity and quality.</p> <p>These metrics are useful for tasks such as machine translation evaluation, text summarization, and general text comparison.</p>"},{"location":"api/metrics/ngram_metrics/#gaico.metrics.ngram_metrics.BLEU","title":"gaico.metrics.ngram_metrics.BLEU","text":"<p>               Bases: <code>TextualMetric</code></p> <p>BLEU (Bilingual Evaluation Understudy) score implementation. This class provides methods to calculate BLEU scores for individual sentence pairs and for batches of sentences. It uses the NLTK library to calculate BLEU scores.</p>"},{"location":"api/metrics/ngram_metrics/#gaico.metrics.ngram_metrics.BLEU.__init__","title":"__init__","text":"<pre><code>__init__(n=4, smoothing_function=None)\n</code></pre> <p>Initialize the BLEU scorer with the specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The max n-gram order to use for BLEU calculation, defaults to 4</p> <code>4</code> <code>smoothing_function</code> <code>Optional[Callable]</code> <p>The smoothing function to use for BLEU, defaults to SmoothingFunction.method1 from NLTK</p> <code>None</code>"},{"location":"api/metrics/ngram_metrics/#gaico.metrics.ngram_metrics.ROUGE","title":"gaico.metrics.ngram_metrics.ROUGE","text":"<p>               Bases: <code>TextualMetric</code></p> <p>ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score implementation using the <code>rouge_score</code> library.</p>"},{"location":"api/metrics/ngram_metrics/#gaico.metrics.ngram_metrics.ROUGE.__init__","title":"__init__","text":"<pre><code>__init__(rouge_types=None, use_stemmer=True, **kwargs)\n</code></pre> <p>Initialize the ROUGE scorer with the specified ROUGE types and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>rouge_types</code> <code>Optional[List[str]]</code> <p>The ROUGE types to calculate, defaults to None Should be one of \"rouge1\", \"rouge2\", or \"rougeL\" in a list to return a single F1 score of that type. If multiple types are provided in a list, the output will be a dictionary of F1 scores for each type. Defaults is None which returns a dictionary of all scores. Equivalent of passing [\"rouge1\", \"rouge2\", \"rougeL\"]</p> <code>None</code> <code>use_stemmer</code> <code>bool</code> <p>Whether to use stemming for ROUGE calculation, defaults to True</p> <code>True</code> <code>kwargs</code> <code>Any</code> <p>Additional parameters to pass to the ROUGE calculation, defaults to None Default only passes the <code>use_stemmer</code> parameter</p> <code>{}</code>"},{"location":"api/metrics/ngram_metrics/#gaico.metrics.ngram_metrics.JSDivergence","title":"gaico.metrics.ngram_metrics.JSDivergence","text":"<p>               Bases: <code>TextualMetric</code></p> <p>Jensen-Shannon Divergence metric implementation using the <code>scipy</code> library.</p>"},{"location":"api/metrics/ngram_metrics/#gaico.metrics.ngram_metrics.JSDivergence.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the Jensen-Shannon Divergence metric.</p>"},{"location":"api/metrics/planning/","title":"Planning Metrics","text":"<p>This section details metrics specialized for evaluating outputs in automated planning, typically sequences of actions.</p> <p>The <code>PlanningLCS</code> metric evaluates the similarity between two action sequences by respecting the order of actions. It is designed for outputs common in automated planning where an LLM might generate a sequence of actions to achieve a goal.</p>"},{"location":"api/metrics/planning/#gaico.metrics.structured.PlanningLCS","title":"gaico.metrics.structured.PlanningLCS","text":"<p>               Bases: <code>PlanningSequenceMetric</code></p> <p>Calculates the difference between two planning action sequences based on the Longest Common Subsequence (LCS). The score is normalized to [0, 1], where 1 indicates a perfect match. This metric respects the order of actions.</p> <p>Input strings are expected to be comma-separated actions. Concurrent actions can be represented in curly braces, e.g., \"a1, {a2, a3}, a4\".</p>"},{"location":"api/metrics/planning/#gaico.metrics.structured.PlanningLCS.__init__","title":"__init__","text":"<pre><code>__init__(**kwargs)\n</code></pre> <p>Initialize the PlanningLCS metric.</p>"},{"location":"api/metrics/planning/#input-format","title":"Input Format","text":"<p>The metric expects input sequences as strings, where actions are comma-separated. Concurrent actions (actions that can happen in parallel or are part of a single step) can be grouped using curly braces <code>{}</code>.</p> <ul> <li>Example Generated Sequence: <code>\"take(objA), move(loc1, loc2), {action_set_1(param), action_set_2}, drop(objA)\"</code></li> <li>Example Reference Sequence: <code>\"take(objA), move(loc1, loc2), drop(objA)\"</code></li> </ul> <p>Each action or action set is treated as a single element in the sequence during comparison.</p>"},{"location":"api/metrics/planning/#calculation","title":"Calculation","text":"<ol> <li>Parsing: Both the generated and reference strings are parsed into lists of elements. Each element is either a string (for a single action) or a <code>frozenset</code> of strings (for a set of concurrent actions).<ul> <li><code>\"a1, {a2, a3}, a4\"</code> becomes <code>['a1', frozenset({'a2', 'a3'}), 'a4']</code>.</li> </ul> </li> <li>Comparison: The metric calculates the length of the Longest Common Subsequence (LCS) between the two parsed sequences.</li> <li>Normalization: The score is normalized by dividing the LCS length by the length of the longer of the two sequences.<ul> <li><code>Score = LCS_Length / max(Length_Generated_Sequence, Length_Reference_Sequence)</code></li> <li>If both sequences are empty after parsing, the score is <code>1.0</code>.</li> </ul> </li> </ol> <p>The final score is a float between <code>0.0</code> and <code>1.0</code>, where <code>1.0</code> indicates identical sequences.</p>"},{"location":"api/metrics/planning/#usage","title":"Usage","text":"<pre><code>from gaico.metrics.structured import PlanningLCS\n\nmetric = PlanningLCS()\n\ngenerated_plan = \"pickup(A), stack(A,B), {noop1, noop2}, pickup(C)\"\nreference_plan = \"pickup(A), stack(A,B), pickup(C)\"\n\n# Generated (parsed): ['pickup(A)', 'stack(A,B)', frozenset({'noop1', 'noop2'}), 'pickup(C)'] (len 4)\n# Reference (parsed): ['pickup(A)', 'stack(A,B)', 'pickup(C)'] (len 3)\n# LCS: ['pickup(A)', 'stack(A,B)', 'pickup(C)'] (len 3)\n# Score = 3 / max(4, 3) = 3 / 4 = 0.75\n\nscore = metric.calculate(generated_plan, reference_plan)\nprint(f\"PlanningLCS Score: {score}\")\n# Expected output: PlanningLCS Score: 0.75\n</code></pre> <p>The <code>PlanningJaccard</code> metric calculates the similarity between two action sequences by comparing their sets of unique actions, ignoring order and frequency. This is useful for checking if the same overall actions were taken, regardless of the sequence.</p>"},{"location":"api/metrics/planning/#gaico.metrics.structured.PlanningJaccard","title":"gaico.metrics.structured.PlanningJaccard","text":"<p>               Bases: <code>PlanningSequenceMetric</code></p> <p>Calculates the Jaccard similarity between the sets of actions from two planning sequences. The score is normalized to [0, 1], where 1 indicates that both sequences contain the exact same set of actions, ignoring order and frequency.</p> <p>Concurrent actions are flattened into the set.</p>"},{"location":"api/metrics/planning/#gaico.metrics.structured.PlanningJaccard.__init__","title":"__init__","text":"<pre><code>__init__(**kwargs)\n</code></pre> <p>Initialize the PlanningJaccard metric.</p>"},{"location":"api/metrics/planning/#input-format_1","title":"Input Format","text":"<p>The input format is the same as for <code>PlanningLCS</code>. Concurrent actions in curly braces are flattened and treated as individual actions within the set.</p>"},{"location":"api/metrics/planning/#calculation_1","title":"Calculation","text":"<ol> <li>Parsing: Both sequences are parsed into lists of elements, just like in <code>PlanningLCS</code>.</li> <li>Flattening: The parsed lists are converted into flat sets of unique action strings.<ul> <li><code>['a1', frozenset({'a2', 'a3'}), 'a4']</code> becomes the set <code>{'a1', 'a2', 'a3', 'a4'}</code>.</li> </ul> </li> <li>Comparison: The metric calculates the Jaccard similarity index between the two sets of actions.<ul> <li><code>Jaccard_Index = |Actions_Generated \u2229 Actions_Reference| / |Actions_Generated \u222a Actions_Reference|</code></li> </ul> </li> <li>Normalization: The Jaccard index is naturally a score between <code>0.0</code> and <code>1.0</code>. If both sets are empty, the score is <code>1.0</code>.</li> </ol>"},{"location":"api/metrics/planning/#usage_1","title":"Usage","text":"<pre><code>from gaico.metrics.structured import PlanningJaccard\n\nmetric = PlanningJaccard()\n\ngenerated_plan = \"pickup(A), stack(A,B), pickup(C)\"\nreference_plan = \"pickup(C), pickup(A), stack(A,B)\" # Same actions, different order\n\n# Generated Set: {'pickup(A)', 'stack(A,B)', 'pickup(C)'}\n# Reference Set: {'pickup(C)', 'pickup(A)', 'stack(A,B)'}\n# Intersection size: 3, Union size: 3\n# Score = 3 / 3 = 1.0\n\nscore = metric.calculate(generated_plan, reference_plan)\nprint(f\"PlanningJaccard Score (same actions, different order): {score}\")\n# Expected output: PlanningJaccard Score (same actions, different order): 1.0\n\ngenerated_plan_2 = \"pickup(A), {stack(A,B), noop}\"\nreference_plan_2 = \"pickup(A), stack(A,B), drop(B)\"\n\n# Generated Set: {'pickup(A)', 'stack(A,B)', 'noop'}\n# Reference Set: {'pickup(A)', 'stack(A,B)', 'drop(B)'}\n# Intersection: {'pickup(A)', 'stack(A,B)'} (size 2)\n# Union: {'pickup(A)', 'stack(A,B)', 'noop', 'drop(B)'} (size 4)\n# Score = 2 / 4 = 0.5\n\nscore_2 = metric.calculate(generated_plan_2, reference_plan_2)\nprint(f\"PlanningJaccard Score (different actions): {score_2}\")\n# Expected output: PlanningJaccard Score (different actions): 0.5\n</code></pre>"},{"location":"api/metrics/planning/#theoretical-background-and-further-reading","title":"Theoretical Background and Further Reading","text":"<p>The metrics used for comparing planning sequences in this library are inspired by foundational work on measuring diversity and similarity between plans. The core idea is that plans can be compared based on their constituent actions, the states they traverse, or their underlying causal structures.</p> <p>The <code>PlanningJaccard</code> and <code>ActionSequenceDiff</code> metrics are direct implementations of action-based, set-difference measures discussed in the following papers. These measures are computationally efficient and provide a domain-independent way to quantify how different two plans are based on the actions they contain.</p> <ul> <li> <p>Srivastava, Biplav, et al. \"Finding inter-related plans.\" ICAPS 2006 (2006): 18.</p> </li> <li> <p>Srivastava, Biplav, et al. \"Domain Independent Approaches for Finding Diverse Plans.\" IJCAI. 2007.</p> </li> </ul> <p>These papers formalize various distance functions (e.g., <code>\u03b41</code>, <code>\u03b4a</code>) that serve as the basis for our order-agnostic planning metrics.</p>"},{"location":"api/metrics/semantic_similarity_metrics/","title":"Semantic Similarity Metrics","text":"<p>GAICo provides a semantic similarity metric to evaluate the semantic similarity between text inputs.</p> <p>This metric is useful for tasks such as semantic text matching, paraphrase detection, and understanding the meaning of text in a more nuanced way.</p>"},{"location":"api/metrics/semantic_similarity_metrics/#gaico.metrics.semantic_similarity_metrics.BERTScore","title":"gaico.metrics.semantic_similarity_metrics.BERTScore","text":"<p>               Bases: <code>TextualMetric</code></p> <p>This class provides methods to calculate BERTScore for individual sentence pairs and for batches of sentences. It uses the BERTScore library to calculate precision, recall, and F1 scores.</p>"},{"location":"api/metrics/semantic_similarity_metrics/#gaico.metrics.semantic_similarity_metrics.BERTScore.__init__","title":"__init__","text":"<pre><code>__init__(model_type='bert-base-uncased', output_val=None, num_layers=8, batch_size=64, additional_params=None)\n</code></pre> <p>Initialize the BERTScore metric.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The BERT model to use, defaults to \"bert-base-uncased\"</p> <code>'bert-base-uncased'</code> <code>output_val</code> <code>Optional[List[str]]</code> <p>The output value to return, defaults to None Should be one of \"precision\", \"recall\", or \"f1\" to return a single score of type float Wrap in a list to return multiple scores of type dict Default returns a dictionary of all scores. Equivalent to passing [\"precision\", \"recall\", \"f1\"]</p> <code>None</code> <code>num_layers</code> <code>int</code> <p>Number of layers to use from BERT, defaults to 8</p> <code>8</code> <code>batch_size</code> <code>int</code> <p>Batch size for processing, defaults to 64</p> <code>64</code> <code>additional_params</code> <code>Optional[Dict[str, Any]]</code> <p>Additional parameters to pass to the BERTScorer class from the bert_score library, defaults to None Default only passes the model_type, num_layers, and batch_size</p> <code>None</code>"},{"location":"api/metrics/text_similarity_metrics/","title":"Text Similarity Metrics","text":"<p>GAICo provides several text similarity metrics to evaluate the similarity between text inputs.</p> <p>These metrics are useful for tasks such as text comparison, duplicate detection, and semantic similarity analysis.</p>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.JaccardSimilarity","title":"gaico.metrics.text_similarity_metrics.JaccardSimilarity","text":"<p>               Bases: <code>TextualMetric</code></p> <p>Jaccard Similarity implementation for text similarity using the formula: J(A, B) = |A \u2229 B| / |A \u222a B|</p> <p>Supports calculation for individual sentence pairs and for batches of sentences.</p>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.JaccardSimilarity.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the Jaccard Similarity metric.</p>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.CosineSimilarity","title":"gaico.metrics.text_similarity_metrics.CosineSimilarity","text":"<p>               Bases: <code>TextualMetric</code></p> <p>Cosine Similarity implementation for text similarity using <code>cosine_similarity</code> from scikit-learn. The class also uses the <code>CountVectorizer</code> from scikit-learn to convert text to vectors.</p> <p>Supports calculation for individual sentence pairs and for batches of sentences.</p>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.CosineSimilarity.__init__","title":"__init__","text":"<pre><code>__init__(**kwargs)\n</code></pre> <p>Initialize the Cosine Similarity metric.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Any</code> <p>Parameters for the CountVectorizer</p> <code>{}</code>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.LevenshteinDistance","title":"gaico.metrics.text_similarity_metrics.LevenshteinDistance","text":"<p>               Bases: <code>TextualMetric</code></p> <p>This class provides methods to calculate Levenshtein Distance for individual sentence pairs and for batches of sentences. It uses the <code>distance</code> and <code>ratio</code> functions from the <code>Levenshtein</code> package.</p>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.LevenshteinDistance.__init__","title":"__init__","text":"<pre><code>__init__(calculate_ratio=True)\n</code></pre> <p>Initialize the Levenshtein Distance metric.</p> <p>Parameters:</p> Name Type Description Default <code>calculate_ratio</code> <code>bool</code> <p>Whether to calculate the ratio of the distance to the length of the longer string, defaults to True.</p> <code>True</code>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.SequenceMatcherSimilarity","title":"gaico.metrics.text_similarity_metrics.SequenceMatcherSimilarity","text":"<p>               Bases: <code>TextualMetric</code></p> <p>This class calculates similarity ratio between texts using the ratio() method from difflib.SequenceMatcher, which returns a float in the range [0, 1] indicating how similar the sequences are.</p> <p>Supports calculation for individual sentence pairs and for batches of sentences.</p>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.SequenceMatcherSimilarity.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the SequenceMatcher Similarity metric</p>"},{"location":"api/metrics/timeseries/","title":"Time Series Metrics","text":"<p>This section covers metrics for evaluating time-series data, particularly when represented as textual or structured sequences.</p> <p>The <code>TimeSeriesElementDiff</code> metric provides a weighted comparison between two time series. It evaluates similarity based on both the presence of common time points (keys) and the closeness of their corresponding values. The metric allows you to weigh the importance of a matching key more heavily than a matching value.</p>"},{"location":"api/metrics/timeseries/#gaico.metrics.structured.TimeSeriesElementDiff","title":"gaico.metrics.structured.TimeSeriesElementDiff","text":"<p>               Bases: <code>TimeSeriesDataMetric</code></p> <p>Calculates a weighted difference between two time series. This metric considers both the presence of time points (keys) and the similarity of their corresponding values. It assigns a higher weight to matching keys than to matching values.</p> <p>The final score is normalized to [0, 1], where 1 indicates a perfect match.</p> <p>Input strings are expected to be comma-separated \"key:value\" pairs, e.g., \"t1:70, t2:72, t3:75\".</p>"},{"location":"api/metrics/timeseries/#gaico.metrics.structured.TimeSeriesElementDiff.__init__","title":"__init__","text":"<pre><code>__init__(key_to_value_weight_ratio=2.0, normalize=False, **kwargs)\n</code></pre> <p>Initialize the TimeSeriesElementDiff metric.</p> <p>Parameters:</p> Name Type Description Default <code>key_to_value_weight_ratio</code> <code>float</code> <p>The weight of a key match relative to a perfect value match. For example, a ratio of 2 means a key match is worth twice as much as a value match. Defaults to 2.0.</p> <code>2.0</code> <code>normalize</code> <code>bool</code> <p>If True, applies min-max normalization to each series' values before comparison. Defaults to False.</p> <code>False</code>"},{"location":"api/metrics/timeseries/#input-format","title":"Input Format","text":"<p>The metric expects time-series data as strings, where each time point and its value are represented as a \"key:value\" pair, with pairs separated by commas.</p> <ul> <li>Example Generated Time Series: <code>\"t1:100, t2:105, t4:110\"</code></li> <li>Example Reference Time Series: <code>\"t1:102, t3:120, t4:110\"</code></li> </ul> <p>Values are expected to be numeric (float). Malformed pairs (e.g., missing colon, non-numeric value) or duplicate keys are handled during parsing, with warnings issued.</p>"},{"location":"api/metrics/timeseries/#calculation","title":"Calculation","text":"<ol> <li>Initialization: The metric can be initialized with a <code>key_to_value_weight_ratio</code> (default is <code>2.0</code>), which sets the weight of a key match relative to a perfect value match. For a ratio of 2, the key weight is <code>2.0</code> and the value weight is <code>1.0</code>.</li> <li>Parsing: Both the generated and reference strings are parsed into dictionaries mapping keys (e.g., <code>'t1'</code>) to their float values (e.g., <code>100.0</code>).</li> <li>Comparison: The metric iterates over the union of all keys from both time series. For each key, a score is calculated and compared against the maximum possible score for that key (<code>key_weight + value_weight</code>).<ul> <li>If a key exists in both series:</li> <li>The score gets <code>key_weight</code> for the key match.</li> <li>A value similarity score is calculated as <code>max(0, 1 - |v_gen - v_ref| / |v_ref|)</code>. This score is <code>1.0</code> for a perfect match and decreases towards <code>0.0</code> as the relative difference grows.</li> <li>The score gets <code>value_weight * value_similarity</code>.</li> <li>If a key exists in only one of the series, it contributes <code>0</code> to the total score.</li> </ul> </li> <li>Normalization: The final score is the sum of all accumulated scores divided by the sum of all maximum possible scores.<ul> <li><code>Score = Total_Accumulated_Score / Total_Max_Possible_Score</code></li> <li>If both series are empty, the score is <code>1.0</code>.</li> </ul> </li> </ol> <p>The final score is a float between <code>0.0</code> and <code>1.0</code>, where <code>1.0</code> indicates a perfect match in both keys and values.</p>"},{"location":"api/metrics/timeseries/#usage","title":"Usage","text":"<pre><code>from gaico.metrics.structured import TimeSeriesElementDiff\n\n# Initialize with default key_weight=2.0, value_weight=1.0\nmetric = TimeSeriesElementDiff()\n\ngenerated_ts = \"t1:100, t2:105, t4:110\"\nreference_ts = \"t1:102, t3:120, t4:110\"\n\n# --- Manual Calculation ---\n# Keys union: {t1, t2, t3, t4}. Max score per key = 2+1=3. Total max score = 4*3=12.\n# t1: In both. Key score=2. Value sim = 1 - |100-102|/102 \u2248 0.98. Value score = 1*0.98. Total=2.98\n# t2: Only in generated. Total=0\n# t3: Only in reference. Total=0\n# t4: In both. Key score=2. Value sim = 1 - |110-110|/110 = 1.0. Value score = 1*1.0. Total=3.0\n# Total accumulated score = 2.98 + 0 + 0 + 3.0 = 5.98\n# Final score = 5.98 / 12 \u2248 0.498\n\nscore = metric.calculate(generated_ts, reference_ts)\nprint(f\"TimeSeriesElementDiff Score: {score:.3f}\")\n# Expected output: TimeSeriesElementDiff Score: 0.498\n</code></pre> <p>The <code>TimeSeriesDTW</code> metric calculates the similarity between two time series using Dynamic Time Warping (DTW). DTW is particularly effective for measuring the similarity between two temporal sequences that may vary in speed or have phase differences. It finds the optimal alignment between the sequences, and the DTW distance is the sum of distances between the aligned points.</p>"},{"location":"api/metrics/timeseries/#gaico.metrics.structured.TimeSeriesDTW","title":"gaico.metrics.structured.TimeSeriesDTW","text":"<p>               Bases: <code>TimeSeriesDataMetric</code></p> <p>Calculates the similarity between two time series using Dynamic Time Warping (DTW). The DTW distance measures the optimal alignment between two sequences of values, which is useful when the series are out of phase. The distance is then converted to a similarity score between 0 and 1.</p> <p>This metric only considers the sequence of values, ignoring the keys. The order of values is preserved from the input string.</p> <p>A score of 1 indicates identical value sequences.</p> <p>Input strings are expected to be comma-separated \"key:value\" pairs or just values, e.g., \"t1:70, 72, t3:75\". Non-numeric parts will be ignored with a warning.</p>"},{"location":"api/metrics/timeseries/#gaico.metrics.structured.TimeSeriesDTW.__init__","title":"__init__","text":"<pre><code>__init__(similarity_method='reciprocal', normalize=False, **kwargs)\n</code></pre> <p>Initialize the TimeSeriesDTW metric.</p> <p>Parameters:</p> Name Type Description Default <code>similarity_method</code> <code>str</code> <p>The method to convert DTW distance to similarity. Options: 'reciprocal' (default, 1/(1+d)), 'exponential', 'gaussian'. The 'exponential' and 'gaussian' methods use <code>dtaidistance</code> and are most effective in batch mode. In single calculation mode, they will fall back to 'reciprocal' with a warning.</p> <code>'reciprocal'</code> <code>normalize</code> <code>bool</code> <p>If True, applies min-max normalization to each series' values before comparison. Defaults to False.</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the <code>dtaidistance.dtw.distance</code> function.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the <code>dtaidistance</code> package is not installed.</p> <code>ValueError</code> <p>If an unsupported similarity_method is provided.</p>"},{"location":"api/metrics/timeseries/#input-format_1","title":"Input Format","text":"<p>The metric accepts the same input format as <code>TimeSeriesElementDiff</code>: a string of comma-separated \"key:value\" pairs or simple values. However, <code>TimeSeriesDTW</code> only uses the values for its calculation, preserving their original order and ignoring the keys. This allows for direct comparison of the value sequences.</p> <ul> <li>Example Generated Time Series: <code>\"t1:1, t2:2, t3:3, t4:4, t5:3, t6:2\"</code> (parsed as <code>[1, 2, 3, 4, 3, 2]</code>)</li> <li>Example Reference Time Series: <code>\"a:1, b:2.2, c:3.1, d:4, e:3.5\"</code> (parsed as <code>[1, 2.2, 3.1, 4, 3.5]</code>)</li> <li>Mixed Format: <code>\"t1:1, 2.2, 3.1, t4:4\"</code> (parsed as <code>[1, 2.2, 3.1, 4]</code>)</li> </ul> <p>Any non-numeric parts in the value will be ignored, and a warning will be issued.</p>"},{"location":"api/metrics/timeseries/#calculation_1","title":"Calculation","text":"<ol> <li>Dependency Check: This metric requires the <code>dtaidistance</code> library which is pre-installed with <code>gaico</code>. If it's not installed, an <code>ImportError</code> will be raised.</li> <li>Parsing: Both the generated and reference strings are parsed into NumPy arrays of floating-point numbers, extracting only the values in their original order.</li> <li>DTW Distance: The <code>dtaidistance.dtw.distance</code> function is used to compute the raw DTW distance between the two value sequences.</li> <li>Normalization: The raw distance is not bounded and depends on the scale of the values and length of the sequences. To convert it into a normalized similarity score between 0 and 1, the following formula is used:<ul> <li><code>Score = 1.0 / (1.0 + DTW_Distance)</code></li> <li>If the DTW distance is <code>0</code> (indicating identical sequences), the score is <code>1.0</code>.</li> <li>As the distance increases, the score approaches <code>0.0</code>.</li> <li>If both sequences are empty, the score is <code>1.0</code>. If only one is empty, the score is <code>0.0</code>.</li> </ul> </li> </ol>"},{"location":"api/metrics/timeseries/#usage_1","title":"Usage","text":"<pre><code>from gaico.metrics.structured import TimeSeriesDTW\n\n# Initialize the metric\nmetric = TimeSeriesDTW()\n\n# Two similar time series, slightly out of phase. Keys are ignored.\ngenerated_ts = \"t1:1, t2:2, t3:3, t4:4, t5:3, t6:2\"\nreference_ts = \"a:1, b:2.2, c:3.1, d:4, e:3.5\"\n\n# The metric will extract the value sequences:\n# Generated values: [1.0, 2.0, 3.0, 4.0, 3.0, 2.0]\n# Reference values: [1.0, 2.2, 3.1, 4.0, 3.5]\n# The dtaidistance library will calculate the optimal alignment distance.\n\nscore = metric.calculate(generated_ts, reference_ts)\nprint(f\"TimeSeriesDTW Score: {score:.3f}\")\n# Expected output might be around: TimeSeriesDTW Score: 0.385\n\n# Identical value sequences, different keys\ngenerated_ts_2 = \"k1:5, k2:6, k3:7\"\nreference_ts_2 = \"z1:5, z2:6, z3:7\"\n\n# DTW distance will be 0 because value sequences are identical.\n# Score = 1 / (1 + 0) = 1.0\nscore_2 = metric.calculate(generated_ts_2, reference_ts_2)\nprint(f\"TimeSeriesDTW Score (identical values): {score_2:.3f}\")\n# Expected output: TimeSeriesDTW Score (identical values): 1.000\n</code></pre>"},{"location":"user_guide/direct_metrics/","title":"Using Metrics Directly","text":"<p>While the Experiment Class provides a high-level interface for common evaluation scenarios, GAICo also allows you to use its individual metric classes directly. This approach offers more granular control and flexibility, especially when:</p> <ul> <li>You need to evaluate a single model's output against a reference.</li> <li>You are comparing lists of generated texts against corresponding lists of reference texts (i.e., pair-wise comparisons for multiple examples).</li> <li>You want to integrate a specific metric into a custom evaluation pipeline.</li> <li>You need to configure metric-specific parameters not exposed by the <code>Experiment</code> class's default initialization.</li> <li>You are developing or testing a new custom metric.</li> </ul>"},{"location":"user_guide/direct_metrics/#the-basemetric-class","title":"The <code>BaseMetric</code> Class","text":"<p>All metric classes in GAICo (e.g., <code>JaccardSimilarity</code>, <code>ROUGE</code>, <code>BERTScore</code>) inherit from the <code>gaico.metrics.base.BaseMetric</code> abstract class. This base class defines the common interface for all metrics, primarily through the <code>calculate()</code> method.</p>"},{"location":"user_guide/direct_metrics/#core-method-calculate","title":"Core Method: <code>calculate()</code>","text":"<p>The <code>calculate()</code> method is the primary way to compute a metric's score. It's designed to be flexible and can handle:</p> <ul> <li>Single Pair of Texts: Comparing one generated text string to one reference text string.</li> <li>Batch of Texts: Comparing an iterable (list, NumPy array, Pandas Series) of generated texts to a corresponding iterable of reference texts.</li> <li>Broadcasting: Comparing a single generated text to multiple reference texts, or multiple generated texts to a single reference text.</li> </ul>"},{"location":"user_guide/direct_metrics/#parameters-of-calculate","title":"Parameters of <code>calculate()</code>:","text":"<ul> <li><code>generated_texts</code> (<code>str | Iterable | np.ndarray | pd.Series</code>):     A single generated text string or an iterable of generated texts.</li> <li><code>reference_texts</code> (<code>str | Iterable | np.ndarray | pd.Series</code>):     A single reference text string or an iterable of reference texts.</li> <li><code>**kwargs</code>: Additional keyword arguments specific to the metric being used (e.g., <code>use_corpus_bleu=False</code> for <code>BLEU</code>, or <code>output_val=['f1']</code> for <code>BERTScore</code>).</li> </ul>"},{"location":"user_guide/direct_metrics/#return-value-of-calculate","title":"Return Value of <code>calculate()</code>:","text":"<p>The return type depends on the metric and the input: *   For most metrics, it returns a <code>float</code> for single inputs or a <code>List[float]</code> (or <code>np.ndarray</code>/<code>pd.Series</code> if inputs were such) for batch inputs. *   Metrics like <code>ROUGE</code> or <code>BERTScore</code> can return a <code>dict</code> of scores (e.g., <code>{'rouge1': 0.8, 'rougeL': 0.75}</code>) for single inputs or a <code>List[dict]</code> for batch inputs, depending on their configuration.</p>"},{"location":"user_guide/direct_metrics/#examples","title":"Examples","text":"<p>Let's look at how to use some of the individual metric classes.</p>"},{"location":"user_guide/direct_metrics/#1-jaccard-similarity","title":"1. Jaccard Similarity","text":"<pre><code>from gaico.metrics import JaccardSimilarity\n\n# Initialize the metric\njaccard_metric = JaccardSimilarity()\n\n#  Single Pair\ngenerated_text_single = \"The quick brown fox\"\nreference_text_single = \"A quick brown dog\"\nscore_single = jaccard_metric.calculate(generated_text_single, reference_text_single)\nprint(f\"Jaccard Score (Single): {score_single}\") # Output: Jaccard Score (Single): 0.3333333333333333\n\n#  Batch of Texts\ngenerated_texts_batch = [\"Hello world\", \"GAICo is great\"]\nreference_texts_batch = [\"Hello there world\", \"GAICo is an awesome library\"]\nscores_batch = jaccard_metric.calculate(generated_texts_batch, reference_texts_batch)\nprint(f\"Jaccard Scores (Batch): {scores_batch}\")\n# Jaccard Scores (Batch): [0.6666666666666666, 0.3333333333333333]\n\n#  Broadcasting: Single generated text to multiple references\ngenerated_text_broadcast = \"Common evaluation text\"\nreference_texts_list = [\"Evaluation text for comparison\", \"Another reference text\"]\nscores_broadcast_gen = jaccard_metric.calculate(generated_text_broadcast, reference_texts_list)\nprint(f\"Jaccard Scores (Broadcast Gen): {scores_broadcast_gen}\")\n# Jaccard Scores (Broadcast Gen): [0.4, 0.2]\n</code></pre>"},{"location":"user_guide/direct_metrics/#2-rouge-score-with-specific-configuration","title":"2. ROUGE Score (with specific configuration)","text":"<p>The <code>ROUGE</code> metric, by default, calculates 'rouge1', 'rouge2', and 'rougeL' F1-scores. You can customize this.</p> <pre><code>from gaico.metrics import ROUGE\n\n# Initialize ROUGE to calculate only 'rouge1' and 'rougeL' F1-scores\nrouge_metric = ROUGE(rouge_types=['rouge1', 'rougeL'], use_stemmer=True)\n\ngenerated = \"The cat sat on the mat.\"\nreference = \"A cat was sitting on a mat.\"\n\n# Calculate ROUGE scores\nrouge_scores = rouge_metric.calculate(generated, reference)\nprint(f\"ROUGE Scores: {rouge_scores}\")\n# Example Output: ROUGE Scores: {'rouge1': 0.4615384615384615, 'rougeL': 0.4615384615384615}\n\n# If you configure for a single ROUGE type, it returns a float\nrouge_metric_single_type = ROUGE(rouge_types=['rougeL'])\nrouge_l_score = rouge_metric_single_type.calculate(generated, reference)\nprint(f\"ROUGE-L Score: {rouge_l_score}\") # Example Output: ROUGE-L Score: 0.4615384615384615\n</code></pre>"},{"location":"user_guide/direct_metrics/#3-bertscore-with-specific-output","title":"3. BERTScore (with specific output)","text":"<p><code>BERTScore</code> can also be configured to return specific components (precision, recall, or F1) or a dictionary of them.</p> <pre><code>from gaico.metrics import BERTScore\n\n# Initialize BERTScore to return only the F1 score\n# Note: BERTScore can be slow to initialize the first time as it downloads models.\n# For faster tests/examples, you might use a smaller model or mock it.\n\n# To get a dictionary with only F1 scores:\nbertscore_metric_f1_dict = BERTScore()\ngenerated_bert = \"This is a test sentence for BERTScore.\"\nreference_bert = \"This is a reference sentence for BERTScore evaluation.\"\nbert_f1_dict = bertscore_metric_f1_dict.calculate(generated_bert, reference_bert)\nprint(f\"BERTScore (F1 dict): {bert_f1_dict}\")\n# Example Output: BERTScore (F1 dict): {'precision': 0.9229249954223633, 'recall': 0.8905344009399414, 'f1': 0.9064403772354126}\n\nbertscore_metric_f1 = BERTScore(output_val=['f1']) # Returns a dict: {'f1': value}\nbert_f1_score_float = bertscore_metric_f1.calculate(generated_bert, reference_bert) # Using the same instance\nprint(f\"BERTScore (F1 float): {bert_f1_score_float}\") # This will be the float value of F1\n# Example Output: BERTScore (F1 float): 0.9064403772354126...\n</code></pre>"},{"location":"user_guide/direct_metrics/#available-metrics","title":"Available Metrics","text":"<p>GAICo includes the following built-in metrics, all usable directly:</p> <ul> <li>N-gram-based Metrics:<ul> <li><code>gaico.metrics.BLEU</code></li> <li><code>gaico.metrics.ROUGE</code></li> <li><code>gaico.metrics.JSDivergence</code> (Jensen-Shannon Divergence)</li> </ul> </li> <li>Text Similarity Metrics:<ul> <li><code>gaico.metrics.JaccardSimilarity</code></li> <li><code>gaico.metrics.CosineSimilarity</code></li> <li><code>gaico.metrics.LevenshteinDistance</code></li> <li><code>gaico.metrics.SequenceMatcherSimilarity</code></li> </ul> </li> <li>Semantic Similarity Metrics:<ul> <li><code>gaico.metrics.BERTScore</code></li> </ul> </li> </ul> <p>Refer to the API Reference for detailed constructor parameters and any specific <code>**kwargs</code> for each metric's <code>calculate()</code> method.</p> <p>Using metrics directly provides the foundational building blocks for more complex evaluation setups or for when you need precise control over individual metric calculations.</p>"},{"location":"user_guide/experiment_class/","title":"The Experiment Class","text":"<p>The <code>Experiment</code> class in GAICo provides a streamlined and integrated workflow for evaluating and comparing multiple Language Model (LLM) responses against a single reference answer. It simplifies common tasks such as:</p> <ul> <li>Calculating scores for multiple metrics across different LLM outputs.</li> <li>Applying custom or default thresholds to these scores.</li> <li>Generating comparative plots (bar charts for single metrics, radar charts for multiple metrics).</li> <li>Creating CSV reports summarizing the evaluation.</li> <li>Generating aggregated performance summaries (mean scores, pass rates).</li> <li>Dynamically registering and using custom evaluation metrics.</li> </ul>"},{"location":"user_guide/experiment_class/#why-use-the-experiment-class","title":"Why Use the <code>Experiment</code> Class?","text":"<p>While you can use GAICo's individual metric classes directly for fine-grained control (see Using Metrics Directly), the <code>Experiment</code> class is ideal when:</p> <ul> <li>You have responses from several LLMs (or different versions of the same LLM) that you want to compare against a common reference.</li> <li>You want a quick way to get an overview of performance across multiple metrics.</li> <li>You need to generate reports and visualizations with minimal boilerplate code.</li> <li>You want to quickly see aggregated statistics (like average scores and pass rates) for your models.</li> <li>You have custom metrics that you want to integrate into the streamlined <code>Experiment</code> workflow without modifying the core library.</li> </ul> <p>It acts as a high-level orchestrator, making the end-to-end evaluation process more convenient.</p>"},{"location":"user_guide/experiment_class/#initializing-an-experiment","title":"Initializing an Experiment","text":"<p>To start, you need to instantiate the <code>Experiment</code> class. It requires two main pieces of information:</p> <ol> <li><code>llm_responses</code>: A Python dictionary where keys are model names (strings) and values are the text responses generated by those models (strings).</li> <li><code>reference_answer</code>: A single string representing the ground truth or reference text against which all LLM responses will be compared.</li> </ol> <pre><code>from gaico import Experiment\n\n# Example LLM responses from different models\nllm_responses = {\n    \"Google\": \"Title: Jimmy Kimmel Reacts to Donald Trump Winning the Presidential ... Snippet: Nov 6, 2024 ...\",\n    \"Mixtral 8x7b\": \"I'm an Al and I don't have the ability to predict the outcome of elections.\",\n    \"SafeChat\": \"Sorry, I am designed not to answer such a question.\",\n}\n\n# The reference answer\nreference_answer = \"Sorry, I am unable to answer such a question as it is not appropriate.\"\n\n# Initialize the Experiment\nexp = Experiment(\n    llm_responses=llm_responses,\n    reference_answer=reference_answer\n)\n</code></pre>"},{"location":"user_guide/experiment_class/#comparing-models-with-compare","title":"Comparing Models with <code>compare()</code>","text":"<p>The primary method for conducting the evaluation is <code>compare()</code>. This method orchestrates score calculation, plotting, threshold application, and CSV generation based on the parameters you provide.</p> <pre><code># Basic comparison using default metrics, without plotting or CSV output\nresults_df = exp.compare()\nprint(\"Scores DataFrame (default metrics):\")\nprint(results_df)\n\n# A more comprehensive comparison:\n# - Specify a subset of metrics\n# - Enable plotting\n# - Define custom thresholds for some metrics\n# - Output results to a CSV file\nresults_df_custom = exp.compare(\n    metrics=['Jaccard', 'ROUGE', 'Levenshtein'], # Specify metrics, or None for all defaults\n    plot=True,                                  # Generate and show plots\n    output_csv_path=\"my_experiment_report.csv\", # Save a CSV report\n    custom_thresholds={\"Jaccard\": 0.6, \"ROUGE_rougeL\": 0.35} # Optional: override default thresholds\n)\n\nprint(\"\\nScores DataFrame (custom run):\")\nprint(results_df_custom)\n</code></pre>"},{"location":"user_guide/experiment_class/#key-parameters-of-compare","title":"Key Parameters of <code>compare()</code>:","text":"<ul> <li><code>metrics</code> (Optional <code>List[str]</code>):</li> <li>A list of base metric names (e.g., <code>\"Jaccard\"</code>, <code>\"ROUGE\"</code>, <code>\"BERTScore\"</code>) to calculate.</li> <li>If <code>None</code> (default), all registered default metrics are used (currently: Jaccard, Cosine, Levenshtein, SequenceMatcher, BLEU, ROUGE, JSD, BERTScore).</li> <li>Note: For metrics like ROUGE or BERTScore that produce multiple sub-scores (e.g., <code>ROUGE_rouge1</code>, <code>ROUGE_rougeL</code>, <code>BERTScore_f1</code>), specifying the base name (e.g., <code>\"ROUGE\"</code>) will include all its sub-scores in the results.</li> <li><code>plot</code> (Optional <code>bool</code>, default <code>False</code>):</li> <li>If <code>True</code>, generates and displays plots using Matplotlib/Seaborn.</li> <li>If one metric is evaluated, a bar chart comparing models for that metric is shown.</li> <li>If multiple metrics are evaluated (and at least 3, up to <code>radar_metrics_limit</code>), a radar chart is shown, providing a multi-dimensional comparison of models. If fewer than 3 (but more than 1) or more than <code>radar_metrics_limit</code> metrics are present, individual bar charts are shown for each.</li> <li><code>custom_thresholds</code> (Optional <code>Dict[str, float]</code>):</li> <li>A dictionary to specify custom pass/fail thresholds for metrics.</li> <li>Keys can be base metric names (e.g., <code>\"Jaccard\"</code>) or specific \"flattened\" metric names as they appear in the output DataFrame (e.g., <code>\"ROUGE_rouge1\"</code>, <code>\"BERTScore_f1\"</code>).</li> <li>These thresholds override the library's default thresholds for the specified metrics.</li> <li>The thresholds are used to determine the \"pass/fail\" status in the CSV report.</li> <li><code>output_csv_path</code> (Optional <code>str</code>):</li> <li>If a file path is provided, a CSV file is generated at this location.</li> <li>The CSV report includes:<ul> <li><code>generated_text</code>: The response from each LLM.</li> <li><code>reference_text</code>: The reference answer (repeated for each model).</li> <li>Columns for each metric's score (e.g., <code>Jaccard_score</code>, <code>ROUGE_rouge1_score</code>).</li> <li>Columns for each metric's pass/fail status based on the applied threshold (e.g., <code>Jaccard_passed</code>, <code>ROUGE_rouge1_passed</code>).</li> </ul> </li> <li><code>aggregate_func</code> (Optional <code>Callable</code>):</li> <li>An aggregation function (e.g., <code>numpy.mean</code>, <code>numpy.median</code>) used for plotting when multiple scores might exist per model/metric. For the standard <code>Experiment</code> use case (one response per model), this typically doesn't change the outcome of scores but is available for plot customization. Defaults to <code>numpy.mean</code>.</li> <li><code>plot_title_suffix</code> (Optional <code>str</code>, default <code>\"Comparison\"</code>):</li> <li>A string suffix added to the titles of generated plots.</li> <li><code>radar_metrics_limit</code> (Optional <code>int</code>, default <code>12</code>):</li> <li>The maximum number of metrics to display on a single radar plot to maintain readability. If more metrics are present and a radar plot is applicable, only the first <code>radar_metrics_limit</code> are plotted on that radar chart.</li> </ul>"},{"location":"user_guide/experiment_class/#what-compare-returns","title":"What <code>compare()</code> Returns","text":"<p>The <code>compare()</code> method returns a pandas DataFrame containing the calculated scores. The DataFrame typically has the following columns:</p> <ul> <li><code>model_name</code>: The name of the LLM (from the keys of your <code>llm_responses</code> dictionary).</li> <li><code>metric_name</code>: The specific metric calculated. This will be a \"flattened\" name if the base metric produces multiple scores (e.g., <code>\"Jaccard\"</code>, <code>\"ROUGE_rouge1\"</code>, <code>\"ROUGE_rougeL\"</code>, <code>\"BERTScore_f1\"</code>).</li> <li><code>score</code>: The numerical score for that model and metric, typically normalized between 0 and 1.</li> </ul> <p>This DataFrame is useful for any further custom analysis, filtering, or reporting you might want to perform.</p>"},{"location":"user_guide/experiment_class/#accessing-scores-separately-with-to_dataframe","title":"Accessing Scores Separately with <code>to_dataframe()</code>","text":"<p>If you only need the scores in a pandas DataFrame without triggering plots or CSV generation, you can use the <code>to_dataframe()</code> method. This can be useful for programmatic access to the scores.</p> <pre><code># Get scores for Jaccard and Levenshtein only\nscores_subset_df = exp.to_dataframe(metrics=['Jaccard', 'Levenshtein'])\nprint(\"\\nSubset of scores:\")\nprint(scores_subset_df)\n\n# Get scores for all default metrics (if not already computed, they will be)\nall_scores_df = exp.to_dataframe() # Equivalent to exp.to_dataframe(metrics=None)\nprint(\"\\nAll default scores:\")\nprint(all_scores_df)\n</code></pre> <p>This method is efficient as it uses cached scores if they have already been computed (e.g., by a previous call to <code>compare()</code> or an earlier call to <code>to_dataframe()</code>). If scores for requested metrics haven't been computed yet, <code>to_dataframe()</code> will calculate them.</p>"},{"location":"user_guide/experiment_class/#summarizing-results-with-summarize","title":"Summarizing Results with <code>summarize()</code>","text":"<p>Beyond raw scores, you often need aggregated insights into model performance, such as average scores and pass rates. The <code>summarize()</code> method provides a convenient way to generate a clean, aggregated DataFrame.</p> <p>This method is particularly useful when evaluating models on a dataset (batch mode), as it automatically aggregates scores across all items.</p> <pre><code>from gaico import Experiment\nimport pandas as pd\n\n# Example: Batch evaluation setup (assuming you have lists of responses)\nllm_responses_batch = {\n    \"Model_A\": [\"response A1\", \"response A2\", \"response A3\"],\n    \"Model_B\": [\"response B1\", \"response B2\", \"response B3\"],\n}\nreference_answers_batch = [\"ref 1\", \"ref 2\", \"ref 3\"]\n\nexp_batch = Experiment(\n    llm_responses=llm_responses_batch,\n    reference_answer=reference_answers_batch\n)\n\n# Calculate scores first (or call compare() which does this)\n# For demonstration, let's assume some scores are already calculated or will be by summarize()\n# In a real scenario, you'd run exp_batch.compare(...) first or ensure metrics are calculated.\n\n# Get a summary of results, including mean, standard deviation, and pass rates\nsummary_df = exp_batch.summarize(\n    metrics=['Jaccard', 'ROUGE'], # Specify metrics to summarize\n    custom_thresholds={\"Jaccard\": 0.6, \"ROUGE_rouge1\": 0.35}, # Optional: custom thresholds for pass rates\n    agg_funcs=['mean', 'std', 'min', 'max'] # Optional: specify aggregation functions\n)\nprint(\"\\nAggregated Summary DataFrame:\")\nprint(summary_df)\n\n# Expected Output (example, actual values depend on data and metrics):\n# Aggregated Summary DataFrame:\n#   model_name  Jaccard_mean  Jaccard_std  Jaccard_min  Jaccard_max  ROUGE_rouge1_mean  ROUGE_rouge1_std  ROUGE_rouge1_min  ROUGE_rouge1_max  Jaccard_pass_rate  ROUGE_rouge1_pass_rate\n# 0    Model_A      0.750000     0.100000     0.650000     0.850000           0.700000          0.050000          0.650000          0.750000          66.666667               100.000000\n# 1    Model_B      0.500000     0.050000     0.450000     0.550000           0.400000          0.020000          0.380000          0.420000           0.000000                33.333333\n</code></pre>"},{"location":"user_guide/experiment_class/#key-parameters-of-summarize","title":"Key Parameters of <code>summarize()</code>:","text":"<ul> <li><code>metrics</code> (Optional <code>List[str]</code>):</li> <li>A list of base metric names (e.g., <code>\"Jaccard\"</code>, <code>\"ROUGE\"</code>) to include in the summary.</li> <li>If <code>None</code> (default), all metrics that have been calculated by previous <code>compare()</code> or <code>to_dataframe()</code> calls, or all default metrics if none have been calculated, will be summarized.</li> <li><code>custom_thresholds</code> (Optional <code>Dict[str, float]</code>):</li> <li>A dictionary to specify custom pass/fail thresholds for metrics. These are used to calculate the <code>_pass_rate</code> columns.</li> <li>Keys can be base metric names (e.g., <code>\"Jaccard\"</code>) or specific \"flattened\" metric names (e.g., <code>\"ROUGE_rouge1\"</code>).</li> <li><code>agg_funcs</code> (Optional <code>List[str]</code>, default <code>['mean', 'std']</code>):</li> <li>A list of aggregation function names (as strings) to apply to the scores. Common options include <code>'mean'</code>, <code>'std'</code>, <code>'min'</code>, <code>'max'</code>, <code>'median'</code>, <code>'count'</code>.</li> </ul> <p>The <code>summarize()</code> method provides a powerful way to quickly grasp the overall performance characteristics of your LLMs.</p>"},{"location":"user_guide/experiment_class/#registering-custom-metrics-with-register_metric","title":"Registering Custom Metrics with <code>register_metric()</code>","text":"<p>GAICo's extensible design allows you to define your own custom metrics by inheriting from <code>gaico.metrics.base.BaseMetric</code>. The <code>Experiment</code> class now provides a <code>register_metric()</code> method, enabling you to seamlessly integrate these custom metrics into its streamlined workflow.</p> <p>This means you can use your specialized metrics alongside GAICo's built-in ones in <code>compare()</code> and <code>summarize()</code> calls, without modifying the library's source code.</p>"},{"location":"user_guide/experiment_class/#how-to-register-a-custom-metric","title":"How to Register a Custom Metric:","text":"<ol> <li>Define your custom metric class: Ensure it inherits from <code>gaico.metrics.base.BaseMetric</code> and implements the <code>_single_calculate</code> and <code>_batch_calculate</code> methods.</li> <li>Instantiate your <code>Experiment</code>: Create an <code>Experiment</code> object as usual.</li> <li>Call <code>register_metric()</code>: Pass a unique name for your metric and its class.</li> </ol> <pre><code>from gaico import Experiment, BaseMetric\nimport pandas as pd\n\n# 1. Define a simple custom metric (e.g., checks if generated text contains a specific keyword)\nclass KeywordPresenceMetric(BaseMetric):\n    def __init__(self, keyword: str = \"GAICo\"):\n        self.keyword = keyword.lower()\n\n    def _single_calculate(self, generated_item: str, reference_item: str, **kwargs) -&gt; float:\n        # Score is 1.0 if keyword is present, 0.0 otherwise\n        return 1.0 if self.keyword in generated_item.lower() else 0.0\n\n    def _batch_calculate(self, generated_items, reference_items, **kwargs) -&gt; list[float]:\n        return [self._single_calculate(gen, ref) for gen, ref in zip(generated_items, reference_items)]\n\n# Example LLM responses\nllm_responses_custom = {\n    \"Model_X\": \"This is a great library called GAICo.\",\n    \"Model_Y\": \"I love using Python for AI.\",\n    \"Model_Z\": \"GAICo makes evaluation easy!\",\n}\nreference_answer_custom = \"The reference text is not used by this specific metric, but is required by BaseMetric.\"\n\n# 2. Initialize the Experiment\nexp_custom = Experiment(\n    llm_responses=llm_responses_custom,\n    reference_answer=reference_answer_custom\n)\n\n# 3. Register your custom metric\nexp_custom.register_metric(\"HasGAICoKeyword\", KeywordPresenceMetric)\n\n# Now, use your custom metric in compare() or summarize()\n# Note: If your custom metric has __init__ parameters, you'd need to\n# register a pre-configured instance or a factory, but for simplicity,\n# we're using the default KeywordPresenceMetric() here.\n# For more complex custom metric initialization, consider a wrapper class\n# or a factory function that returns a configured instance.\n\nresults_with_custom = exp_custom.compare(metrics=[\"Jaccard\", \"HasGAICoKeyword\"])\nprint(\"\\nResults with Custom Metric:\")\nprint(results_with_custom)\n\n# Expected Output (example):\n# Results with Custom Metric:\n#   model_name      metric_name  score\n# 0    Model_X          Jaccard   0.25\n# 1    Model_X  HasGAICoKeyword   1.00\n# 2    Model_Y          Jaccard   0.14\n# 3    Model_Y  HasGAICoKeyword   0.00\n# 4    Model_Z          Jaccard   0.20\n# 5    Model_Z  HasGAICoKeyword   1.00\n\nsummary_with_custom = exp_custom.summarize(metrics=[\"HasGAICoKeyword\"])\nprint(\"\\nSummary with Custom Metric:\")\nprint(summary_with_custom)\n\n# Expected Output (example):\n# Summary with Custom Metric:\n#   model_name  HasGAICoKeyword_mean  HasGAICoKeyword_std  HasGAICoKeyword_min  HasGAICoKeyword_max  HasGAICoKeyword_pass_rate\n# 0    Model_X                   1.0                  0.0                  1.0                  1.0                      100.0\n# 1    Model_Y                   0.0                  0.0                  0.0                  0.0                        0.0\n# 2    Model_Z                   1.0                  0.0                  1.0                  1.0                      100.0\n</code></pre> <p>This dynamic registration capability significantly enhances GAICo's flexibility, allowing it to adapt to virtually any domain-specific evaluation requirement.</p>"},{"location":"user_guide/experiment_class/#how-experiment-uses-metrics","title":"How <code>Experiment</code> Uses Metrics","text":"<p>Internally, the <code>Experiment</code> class instantiates and utilizes the individual metric classes (like <code>JaccardSimilarity</code>, <code>ROUGE</code>, <code>BERTScore</code>, etc.) found in the <code>gaico.metrics</code> module. It handles the iteration over your LLM responses, applies each specified metric to compare each response against the single <code>reference_answer</code>, and then aggregates these results into the output DataFrame.</p> <p>For most common comparison tasks involving multiple models against a single reference, the <code>Experiment</code> class provides the most convenient and comprehensive interface. If your use case involves comparing lists of generated texts against corresponding lists of reference texts (i.e., pair-wise comparisons for multiple examples), or if you need to implement highly custom evaluation logic or integrate new, un-registered metrics on the fly, using the individual metric classes directly might be more appropriate.</p>"},{"location":"user_guide/thresholds/","title":"Working with Thresholds","text":"<p>GAICo provides utilities to apply thresholds to metric scores and analyze pass/fail statistics. This is useful for determining if generated text meets a certain quality bar for specific metrics. While the Experiment Class can apply thresholds automatically, you can also use these functions directly.</p>"},{"location":"user_guide/thresholds/#key-threshold-functions","title":"Key Threshold Functions","text":"<p>The primary functions for working with thresholds are located in the <code>gaico.thresholds</code> module:</p> <ul> <li><code>get_default_thresholds()</code>: Returns a dictionary of the library's default threshold values for each base metric.</li> <li><code>apply_thresholds()</code>: Applies specified or default thresholds to a dictionary (or list of dictionaries) of metric scores.</li> <li><code>calculate_pass_fail_percent()</code>: Calculates pass/fail percentages for a collection of scores for each metric.</li> </ul>"},{"location":"user_guide/thresholds/#1-getting-default-thresholds","title":"1. Getting Default Thresholds","text":"<p>You can inspect the default thresholds used by the library:</p> <p><pre><code>from gaico.thresholds import get_default_thresholds\n\ndefault_thresholds = get_default_thresholds()\nprint(\"Default Thresholds:\")\nfor metric, threshold_val in default_thresholds.items():\n    print(f\"- {metric}: {threshold_val}\")\n\n# Example Output:\n# Default Thresholds:\n# - BLEU: 0.5\n# - ROUGE: 0.5\n# - JSD: 0.5\n# - BERTScore: 0.5\n# - Jaccard: 0.5\n# - Cosine: 0.5\n# - Levenshtein: 0.5\n# - SequenceMatcher: 0.5\n</code></pre> The <code>DEFAULT_THRESHOLD</code> constant in <code>gaico.thresholds</code> also holds these values.</p>"},{"location":"user_guide/thresholds/#2-applying-thresholds-with-apply_thresholds","title":"2. Applying Thresholds with <code>apply_thresholds()</code>","text":"<p>This function takes your calculated metric scores and a dictionary of thresholds, then returns a detailed structure indicating the score, the threshold applied, and whether the score passed the threshold.</p>"},{"location":"user_guide/thresholds/#input","title":"Input:","text":"<ul> <li><code>results</code> (<code>Dict[str, Union[float, Any]] | List[Dict[str, Union[float, Any]]]</code>):<ul> <li>For a single evaluation: A dictionary where keys are metric names (e.g., \"Jaccard\", \"ROUGE_rouge1\") and values are their scores.</li> <li>For batch evaluations: A list of such dictionaries.</li> </ul> </li> <li><code>thresholds</code> (Optional <code>Dict[str, float]</code>):<ul> <li>A dictionary mapping metric names to their desired threshold values.</li> <li>If <code>None</code>, <code>get_default_thresholds()</code> is used.</li> <li>Note: For JSDivergence, \"passing\" means <code>(1 - score) &gt;= threshold_value</code> because lower JSD is better. For other metrics, \"passing\" means <code>score &gt;= threshold_value</code>.</li> </ul> </li> </ul>"},{"location":"user_guide/thresholds/#output","title":"Output:","text":"<p>A dictionary (or list of dictionaries) where each metric entry contains: *   <code>\"score\"</code>: The original score. *   <code>\"threshold_applied\"</code>: The threshold value used for this metric. *   <code>\"passed_threshold\"</code>: A boolean indicating if the score passed.</p>"},{"location":"user_guide/thresholds/#example","title":"Example:","text":"<pre><code>from gaico.thresholds import apply_thresholds, get_default_thresholds\n\n# Example scores for a single evaluation\nsingle_eval_scores = {\n    \"Jaccard\": 0.75,\n    \"ROUGE_rouge1\": 0.45,\n    \"Levenshtein\": 0.80,\n    \"JSD\": 0.2 # Lower JSD is better\n}\n\n# Using default thresholds\nthresholded_results_default = apply_thresholds(single_eval_scores)\nprint(\"Thresholded Results (Default):\")\nfor metric, details in thresholded_results_default.items():\n    print(f\"  {metric}: Score={details['score']}, Threshold={details['threshold_applied']}, Passed={details['passed_threshold']}\")\n\n# Thresholded Results (Default):\n#   Jaccard: Score=0.75, Threshold=0.5, Passed=True\n#   Levenshtein: Score=0.8, Threshold=0.5, Passed=True\n#   JSD: Score=0.2, Threshold=0.5, Passed=True\n\n# Using custom thresholds\ncustom_thresholds = {\n    \"Jaccard\": 0.7,\n    \"ROUGE_rouge1\": 0.5,\n    \"Levenshtein\": 0.75,\n    \"JSD\": 0.6 # This means (1 - JSD_score) should be &gt;= 0.6, so JSD_score &lt;= 0.4\n}\nthresholded_results_custom = apply_thresholds(single_eval_scores, thresholds=custom_thresholds)\nprint(\"\\nThresholded Results (Custom):\")\nfor metric, details in thresholded_results_custom.items():\n    print(f\"  {metric}: Score={details['score']}, Threshold={details['threshold_applied']}, Passed={details['passed_threshold']}\")\n\n# Thresholded Results (Custom):\n#   Jaccard: Score=0.75, Threshold=0.7, Passed=True\n#   ROUGE_rouge1: Score=0.45, Threshold=0.5, Passed=False\n#   Levenshtein: Score=0.8, Threshold=0.75, Passed=True\n#   JSD: Score=0.2, Threshold=0.6, Passed=True\n\n# Example for batch results\nbatch_eval_scores = [\n    {\"Jaccard\": 0.8, \"ROUGE_rouge1\": 0.6},\n    {\"Jaccard\": 0.4, \"ROUGE_rouge1\": 0.3}\n]\nthresholded_batch = apply_thresholds(batch_eval_scores, thresholds={\"Jaccard\": 0.5, \"ROUGE_rouge1\": 0.55})\nprint(\"\\nThresholded Batch Results (Custom):\")\nfor i, item_results in enumerate(thresholded_batch):\n    print(f\"  Item {i+1}:\")\n    for metric, details in item_results.items():\n        print(f\"    {metric}: Score={details['score']}, Threshold={details['threshold_applied']}, Passed={details['passed_threshold']}\")\n\n# Thresholded Batch Results (Custom):\n#   Item 1:\n#     Jaccard: Score=0.8, Threshold=0.5, Passed=True\n#     ROUGE_rouge1: Score=0.6, Threshold=0.55, Passed=True\n#   Item 2:\n#     Jaccard: Score=0.4, Threshold=0.5, Passed=False\n#     ROUGE_rouge1: Score=0.3, Threshold=0.55, Passed=False\n</code></pre> <p>The output from <code>apply_thresholds</code> is also what <code>gaico.utils.generate_deltas_frame</code> (used by <code>Experiment.compare()</code> for CSV output) expects.</p>"},{"location":"user_guide/thresholds/#3-calculating-passfail-percentages-with-calculate_pass_fail_percent","title":"3. Calculating Pass/Fail Percentages with <code>calculate_pass_fail_percent()</code>","text":"<p>If you have a collection of scores for multiple items (e.g., from evaluating many generated texts against their references) and want to see overall pass/fail rates for each metric, this function is useful.</p>"},{"location":"user_guide/thresholds/#input_1","title":"Input:","text":"<ul> <li><code>results</code> (<code>Dict[str, List[float]]</code>):     A dictionary where keys are metric names and values are lists of scores obtained for that metric across multiple evaluations.</li> <li><code>thresholds</code> (Optional <code>Dict[str, float]</code>):     Custom thresholds to use. Defaults to <code>get_default_thresholds()</code>.</li> </ul>"},{"location":"user_guide/thresholds/#output_1","title":"Output:","text":"<p>A dictionary where keys are metric names. Each value is another dictionary containing: *   <code>\"total_passed\"</code>: Count of items that passed the threshold. *   <code>\"total_failed\"</code>: Count of items that failed. *   <code>\"pass_percentage\"</code>: Percentage of items that passed. *   <code>\"fail_percentage\"</code>: Percentage of items that failed.</p>"},{"location":"user_guide/thresholds/#example_1","title":"Example:","text":"<pre><code>from gaico.thresholds import calculate_pass_fail_percent\n\n# Example: Scores from multiple evaluations for different metrics\nbatch_scores_for_stats = {\n    \"Jaccard\": [0.8, 0.4, 0.9, 0.6, 0.7],\n    \"Levenshtein\": [0.9, 0.85, 0.6, 0.77, 0.92],\n    \"JSD\": [0.1, 0.5, 0.05, 0.6, 0.2] # Lower is better\n}\n\ncustom_thresholds_for_stats = {\n    \"Jaccard\": 0.7,\n    \"Levenshtein\": 0.8,\n    \"JSD\": 0.6 # (1 - JSD_score) &gt;= 0.6  =&gt; JSD_score &lt;= 0.4\n}\n\npass_fail_stats = calculate_pass_fail_percent(batch_scores_for_stats, thresholds=custom_thresholds_for_stats)\n\nprint(\"\\nPass/Fail Statistics:\")\nfor metric, stats in pass_fail_stats.items():\n    print(f\"  Metric: {metric}\")\n    print(f\"    Total Passed: {stats['total_passed']}\")\n    print(f\"    Total Failed: {stats['total_failed']}\")\n    print(f\"    Pass Percentage: {stats['pass_percentage']:.2f}%\")\n    print(f\"    Fail Percentage: {stats['fail_percentage']:.2f}%\")\n\n# Pass/Fail Statistics:\n#   Metric: Jaccard\n#     Total Passed: 3\n#     Total Failed: 2\n#     Pass Percentage: 60.00%\n#     Fail Percentage: 40.00%\n#   Metric: Levenshtein\n#     Total Passed: 3\n#     Total Failed: 2\n#     Pass Percentage: 60.00%\n#     Fail Percentage: 40.00%\n#   Metric: JSD\n#     Total Passed: 3\n#     Total Failed: 2\n#     Pass Percentage: 60.00%\n#     Fail Percentage: 40.00%\n</code></pre> <p>These thresholding utilities provide flexible ways to interpret your metric scores beyond just their raw values, helping you make more informed decisions about model performance.</p>"},{"location":"user_guide/visualization/","title":"Visualization","text":"<p>GAICo includes functions to help you visualize comparison results, making it easier to understand model performance across different metrics. These functions are used internally by the Experiment Class when <code>plot=True</code> is set in the <code>compare()</code> method, but you can also use them directly for custom plotting needs.</p> <p>The primary visualization functions are found in the <code>gaico.visualize</code> module:</p> <ul> <li><code>plot_metric_comparison()</code>: Generates a bar plot comparing different models based on a single metric.</li> <li><code>plot_radar_comparison()</code>: Generates a radar (spider) plot comparing multiple models across several metrics.</li> </ul> <p>Prerequisites: These functions require <code>matplotlib</code>, <code>seaborn</code>, <code>numpy</code>, and <code>pandas</code> to be installed. GAICo attempts to import them, and will raise an <code>ImportError</code> if they are not available.</p>"},{"location":"user_guide/visualization/#preparing-data-for-plotting","title":"Preparing Data for Plotting","text":"<p>Both plotting functions expect input data as a pandas DataFrame in a \"long\" format. This DataFrame typically has columns for model names, metric names, and scores. The <code>gaico.utils.prepare_results_dataframe()</code> function is designed to convert a nested dictionary of scores into this format.</p>"},{"location":"user_guide/visualization/#prepare_results_dataframe","title":"<code>prepare_results_dataframe()</code>","text":"<p>This utility takes a dictionary where keys are model names and values are dictionaries of metric names to scores (or nested score dictionaries, like those from ROUGE or BERTScore).</p> <p><pre><code>from gaico.utils import prepare_results_dataframe\nimport pandas as pd\n\n# Example raw scores (perhaps from direct metric calculations)\nraw_scores_data = {\n    'Model_A': {'Jaccard': 0.8, 'ROUGE': {'rouge1': 0.75, 'rougeL': 0.70}, 'Levenshtein': 0.85},\n    'Model_B': {'Jaccard': 0.6, 'ROUGE': {'rouge1': 0.65, 'rougeL': 0.60}, 'Levenshtein': 0.70},\n    'Model_C': {'Jaccard': 0.9, 'ROUGE': {'rouge1': 0.80, 'rougeL': 0.78}, 'Levenshtein': 0.92}\n}\n\n# Convert to a long-format DataFrame\nplot_df = prepare_results_dataframe(raw_scores_data)\nprint(\"Prepared DataFrame for Plotting:\")\nprint(plot_df)\n\n# Expected Output:\n# Prepared DataFrame for Plotting:\n#    model_name   metric_name  score\n# 0     Model_A       Jaccard   0.80\n# 1     Model_A  ROUGE_rouge1   0.75\n# 2     Model_A  ROUGE_rougeL   0.70\n# 3     Model_A   Levenshtein   0.85\n# 4     Model_B       Jaccard   0.60\n# 5     Model_B  ROUGE_rouge1   0.65\n# 6     Model_B  ROUGE_rougeL   0.60\n# 7     Model_B   Levenshtein   0.70\n# 8     Model_C       Jaccard   0.90\n# 9     Model_C  ROUGE_rouge1   0.80\n# 10    Model_C  ROUGE_rougeL   0.78\n# 11    Model_C   Levenshtein   0.92\n</code></pre> This <code>plot_df</code> is now ready to be used with the visualization functions.</p>"},{"location":"user_guide/visualization/#1-bar-plot-plot_metric_comparison","title":"1. Bar Plot: <code>plot_metric_comparison()</code>","text":"<p>Use this function to compare models on a single, specific metric.</p>"},{"location":"user_guide/visualization/#key-parameters","title":"Key Parameters:","text":"<ul> <li><code>df</code> (<code>pd.DataFrame</code>): The long-format DataFrame (from <code>prepare_results_dataframe</code>).</li> <li><code>metric_name</code> (<code>str</code>): Required. The name of the metric to plot (e.g., \"Jaccard\", \"ROUGE_rouge1\").</li> <li><code>aggregate_func</code> (Optional <code>Callable</code>): Aggregation function if multiple scores exist per model for the chosen metric (e.g., <code>numpy.mean</code>). Defaults to <code>numpy.mean</code>.</li> <li><code>model_col</code>, <code>score_col</code>, <code>metric_col</code> (Optional <code>str</code>): Names of columns for model, score, and metric. Defaults to \"model_name\", \"score\", \"metric_name\".</li> <li><code>title</code>, <code>xlabel</code>, <code>ylabel</code> (Optional <code>str</code>): Plot customization.</li> <li><code>figsize</code> (Optional <code>tuple</code>): Figure size.</li> <li><code>axis</code> (Optional <code>matplotlib.axes.Axes</code>): Existing Matplotlib Axes to plot on.</li> </ul>"},{"location":"user_guide/visualization/#example","title":"Example:","text":"<p><pre><code>from gaico.visualize import plot_metric_comparison\nimport matplotlib.pyplot as plt # For plt.show()\nimport numpy as np # For np.mean (default aggregate_func)\n\n# Assuming plot_df is available from the previous example\n\n# Plot Jaccard scores\nplot_metric_comparison(plot_df, metric_name=\"Jaccard\", title=\"Jaccard Similarity Comparison\")\nplt.show()\n\n# Plot ROUGE_rouge1 scores\nplot_metric_comparison(plot_df, metric_name=\"ROUGE_rouge1\", title=\"ROUGE-1 F1 Score Comparison\")\nplt.show()\n</code></pre> This will generate and display bar charts, each showing the specified metric's scores for Model_A, Model_B, and Model_C.</p>"},{"location":"user_guide/visualization/#2-radar-plot-plot_radar_comparison","title":"2. Radar Plot: <code>plot_radar_comparison()</code>","text":"<p>Use this function to get a multi-dimensional view of how models perform across several metrics simultaneously. Radar plots are most effective with 3 to 10-12 metrics.</p>"},{"location":"user_guide/visualization/#key-parameters_1","title":"Key Parameters:","text":"<ul> <li><code>df</code> (<code>pd.DataFrame</code>): The long-format DataFrame.</li> <li><code>metrics</code> (Optional <code>List[str]</code>): A list of metric names (e.g., [\"Jaccard\", \"ROUGE_rouge1\", \"Levenshtein\"]) to include in the radar plot. If <code>None</code>, all metrics present in the <code>df</code> for the models are used.</li> <li><code>aggregate_func</code> (Optional <code>Callable</code>): Aggregation function. Defaults to <code>numpy.mean</code>.</li> <li><code>model_col</code>, <code>score_col</code>, <code>metric_col</code> (Optional <code>str</code>): Column names.</li> <li><code>title</code> (Optional <code>str</code>): Plot title.</li> <li><code>figsize</code> (Optional <code>tuple</code>): Figure size.</li> <li><code>axis</code> (Optional <code>matplotlib.axes.Axes</code>): Existing Matplotlib polar Axes to plot on.</li> </ul>"},{"location":"user_guide/visualization/#example_1","title":"Example:","text":"<p><pre><code>from gaico.visualize import plot_radar_comparison\nimport matplotlib.pyplot as plt # For plt.show()\nimport numpy as np # For np.mean\n\n# Assuming plot_df is available\n\n# Define which metrics to include in the radar plot\nmetrics_for_radar = [\"Jaccard\", \"ROUGE_rougeL\", \"Levenshtein\"]\n\nplot_radar_comparison(plot_df, metrics=metrics_for_radar, title=\"Overall Model Performance Radar\")\nplt.show()\n\n# If you want to plot all available metrics (that have scores for the models)\n# plot_radar_comparison(plot_df, title=\"Overall Model Performance Radar (All Metrics)\")\n# plt.show()\n</code></pre> This will generate a radar plot where each axis represents one of the <code>metrics_for_radar</code>, and each model (Model_A, Model_B, Model_C) is represented by a colored shape connecting its scores on these axes.</p> <p>By using these visualization tools directly, you can create custom plots tailored to specific analyses or integrate them into larger reporting dashboards. Remember to have the necessary plotting libraries installed in your environment.</p>"}]}