{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to GAICo","text":"<p>The library provides a set of metrics for evaluating 2 text strings as inputs. Outputs are on a scale of 0 to 1 (normalized), where 1 indicates a perfect match between the two texts.</p> <p>Class Structure: All metrics are implemented as classes, and they can be easily extended to add new metrics. The metrics start with the <code>BaseMetric</code> class under the <code>gaico/base.py</code> file.</p> <p>Each metric class inherits from this base class and is implemented with just one required method: <code>calculate()</code>.</p> <p>This <code>calculate()</code> method takes two parameters:</p> <ul> <li><code>generated_texts</code>: Either a string or a Iterables of strings representing the texts generated by an LLM.</li> <li><code>reference_texts</code>: Either a string or a Iterables of strings representing the expected or reference texts.</li> </ul> <p>If the inputs are Iterables (lists, Numpy arrays, etc.), then the method assumes that there exists a one-to-one mapping between the generated texts and reference texts, meaning that the first generated text corresponds to the first reference text, and so on.</p> <p>Note: While the library can be used to compare strings, it's main purpose is to aid with comparing results from various LLMs.</p> <p>Inspiration for the library and evaluation metrics was taken from Microsoft's article on evaluating LLM-generated content. In the article, Microsoft describes 3 categories of evaluation metrics: (1) Reference-based metrics, (2) Reference-free metrics, and (3) LLM-based metrics. The library currently supports reference-based metrics.</p> Overview of the workflow supported by the GAICo library"},{"location":"#code-style","title":"Code Style","text":"<p>We use <code>pre-commit</code> hooks to maintain code quality and consistency. The configuration for these hooks is in the <code>.pre-commit-config.yaml</code> file. These hooks run automatically on <code>git commit</code>, but you can also run them manually:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"#running-tests","title":"Running Tests","text":"<p>Navigate to the project root in your terminal and run:</p> <pre><code>uv run pytest\n</code></pre> <p>Or, for more verbose output:</p> <pre><code>uv run pytest -v\n</code></pre> <p>To skip the slow BERTScore tests:</p> <pre><code>uv run pytest -m \"not bertscore\"\n</code></pre> <p>To run only the slow BERTScore tests:</p> <pre><code>uv run pytest -m bertscore\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<p>If you find this project useful, please consider citing it in your work:</p> <pre><code>@software{AI4Society_GAICo_GenAI_Results,\n  author = {{AI4Society}},\n  license = {MIT},\n  title = {{GAICo: GenAI Results Comparator}},\n  url = {https://github.com/ai4society/GenAIResultsComparator}\n}\n</code></pre>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<ul> <li>The library is developed by Nitin Gupta, Pallav Koppisetti, and Biplav Srivastava. Members of AI4Society contributed to this tool as part of ongoing discussions. Major contributors are credited.</li> <li>This library uses several open-source packages including NLTK, scikit-learn, and others. Special thanks to the creators and maintainers of the implemented metrics.</li> </ul>"},{"location":"#contact","title":"Contact","text":"<p>If you have any questions, feel free to reach out to us at ai4societyteam@gmail.com.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome! Please feel free to submit a Pull Request.</p> <ol> <li>Fork the repository</li> <li>Create your feature branch (<code>git checkout -b feature/FeatureName</code>)</li> <li>Commit your changes (<code>git commit -m 'Add some FeatureName'</code>)</li> <li>Push to the branch (<code>git push origin feature/FeatureName</code>)</li> <li>Open a Pull Request</li> </ol> <p>Please ensure that your code passes all tests and adheres to our code style guidelines (enforced by pre-commit hooks) before submitting a pull request.</p>"},{"location":"examples/","title":"Examples","text":"<p>Detailed, runnable examples are available as Jupyter Notebooks in the <code>examples/</code> folder:</p> <ul> <li> <p><code>quickstart.ipynb</code>:</p> <p>Using GAICo's <code>Experiment</code> module to provide a simple, quickstart workflow.</p> </li> <li> <p><code>example-1.ipynb</code>:</p> <p>Evaluating multiple models (LLMs, Google, and Custom) using a single metric.</p> </li> <li> <p><code>example-2.ipynb</code>:</p> <p>Evaluating a single model on multiple metrics.</p> </li> <li> <p><code>DeepSeek-example.ipynb</code></p> <p>The aim for this notebook was to aid with evaluating DeepSeek R1 for AI4Society's Point of View (POV).</p> </li> </ul>"},{"location":"examples/#advanced-examples","title":"Advanced Examples","text":"<p>The <code>advanced-examples</code> directory contains advances notebooks showcasing more complex use cases and metrics. These examples are intended for users who are already familiar with the basics of GAICo. Please refer to the README.md file in that directory for details. A quick description:</p> <ul> <li> <p><code>llm_faq-example.ipynb</code></p> <p>Comparison of various LLM responses (Phi, Mixtral, etc.) on FAQ dataset from USC.</p> </li> <li> <p><code>threshold-example.ipynb</code></p> <p>Exploration of default and custom thresholding techniques for LLM responses.</p> </li> <li> <p><code>viz-example.ipynb</code></p> <p>Hands-on visualizations for LLM results.</p> </li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>Here are answers to some common questions about using GAICo.</p>"},{"location":"faq/#q-how-do-i-add-a-new-custom-metric","title":"Q: How do I add a new custom metric?","text":"<p>Adding a new metric to GAICo is designed to be straightforward. All metrics inherit from the <code>BaseMetric</code> class.</p> <ol> <li> <p>Create a New Metric Class:</p> <ul> <li>Your new metric class should inherit from <code>gaico.metrics.base.BaseMetric</code>.</li> <li>You must implement two methods:<ul> <li><code>_single_calculate(self, generated_text: str, reference_text: str, **kwargs: Any) -&gt; Union[float, dict]</code>: This method calculates the metric for a single pair of texts.</li> <li><code>_batch_calculate(self, generated_texts: Union[Iterable, np.ndarray, pd.Series], reference_texts: Union[Iterable, np.ndarray, pd.Series], **kwargs: Any) -&gt; Union[List[float], List[dict], np.ndarray, pd.Series, float, dict]</code>: This method calculates the metric for a batch of texts. Often, this can be implemented by iterating and calling <code>_single_calculate</code>, but you can optimize it for batch operations if possible.</li> </ul> </li> <li>The public <code>calculate()</code> method is inherited from <code>BaseMetric</code> and handles input type checking and dispatching to either <code>_single_calculate</code> or <code>_batch_calculate</code>.</li> </ul> </li> <li> <p>Example Structure: <pre><code>from typing import Any, Iterable, List, Union\nimport numpy as np\nimport pandas as pd\nfrom gaico.metrics.base import BaseMetric\n\nclass MyCustomMetric(BaseMetric):\n    def __init__(self, custom_param: str = \"default\"):\n        self.custom_param = custom_param\n        # Add any other initialization logic\n\n    def _single_calculate(\n        self, generated_text: str, reference_text: str, **kwargs: Any\n    ) -&gt; float:\n        # Your logic to compare generated_text and reference_text\n        # Example:\n        score = 0.0\n        if self.custom_param in generated_text and self.custom_param in reference_text:\n            score = 1.0\n        # Ensure score is normalized between 0 and 1\n        return score\n\n    def _batch_calculate(\n        self,\n        generated_texts: Union[Iterable, np.ndarray, pd.Series],\n        reference_texts: Union[Iterable, np.ndarray, pd.Series],\n        **kwargs: Any,\n    ) -&gt; Union[List[float], np.ndarray, pd.Series]:\n        # Default batch implementation (can be optimized)\n        results = [\n            self._single_calculate(gen, ref, **kwargs)\n            for gen, ref in zip(generated_texts, reference_texts)\n        ]\n        if isinstance(generated_texts, np.ndarray):\n            return np.array(results)\n        elif isinstance(generated_texts, pd.Series):\n            return pd.Series(results, index=generated_texts.index)\n        return results\n</code></pre></p> </li> <li> <p>Using Your Custom Metric:     You can directly instantiate and use your custom metric:     <pre><code>from gaico.metrics.base import BaseMetric # Or your custom metric file\n# from your_module import MyCustomMetric # If defined in a separate file\n\n# Assuming MyCustomMetric is defined as above\ncustom_metric = MyCustomMetric(custom_param=\"example\")\nscore = custom_metric.calculate(\"This is an example generated text.\", \"This is an example reference text.\")\nprint(f\"Custom metric score: {score}\")\n</code></pre></p> </li> <li> <p>(Optional) Registering with the <code>Experiment</code> Class:     If you want your custom metric to be usable by its string name within the <code>Experiment</code> class (e.g., <code>exp.compare(metrics=['MyCustomMetric'])</code>), you'll need to add it to the <code>REGISTERED_METRICS</code> dictionary in <code>gaico/experiment.py</code>:</p> <p><pre><code># In gaico/experiment.py\n# ... other imports ...\n# from ..metrics.my_custom_metric_module import MyCustomMetric # Adjust import path\n\nREGISTERED_METRICS: Dict[str, type[BaseMetric]] = {\n    \"Jaccard\": JaccardSimilarity,\n    \"Cosine\": CosineSimilarity,\n    # ... other default metrics ...\n    \"MyCustomMetric\": MyCustomMetric, # Add your metric here\n}\n</code></pre> You would also typically add it to <code>DEFAULT_METRICS_TO_RUN</code> in the same file if you want it to run by default when <code>metrics=None</code> in <code>Experiment.compare()</code>.</p> <p>Refer to existing metric implementations in <code>gaico/metrics/</code> for more detailed examples (e.g., <code>text_similarity_metrics.py</code>).</p> </li> </ol>"},{"location":"faq/#q-how-do-i-get-the-list-of-all-supported-metrics-or-use-them-directly","title":"Q: How do I get the list of all supported metrics or use them directly?","text":"<p>There are a couple of ways to understand and access \"supported metrics\":</p> <ol> <li> <p>All metric classes available for direct instantiation from <code>gaico.metrics</code>:     These are all the metric classes defined in the <code>gaico.metrics</code> sub-package that are intended for public use. They are typically listed in <code>gaico.metrics.__init__.__all__</code>.</p> <ul> <li> <p>To import and use specific metric classes by name:     This is the recommended way for clarity.     <pre><code>from gaico.metrics import JaccardSimilarity, BLEU, BERTScore\n\njaccard_scorer = JaccardSimilarity()\nbleu_scorer = BLEU()\nbert_scorer = BERTScore(model_type=\"distilbert-base-uncased\") # Example of passing params\n\nscore1 = jaccard_scorer.calculate(\"text a\", \"text b\")\nscore2 = bleu_scorer.calculate(\"text a\", \"text b\")\n</code></pre></p> </li> <li> <p>To import all available metric classes using <code>*</code>:     This makes all metric classes from <code>gaico.metrics.__init__.__all__</code> available in your current namespace.     <pre><code>from gaico.metrics import *\n\n# Now you can use the class names directly\njaccard_scorer = JaccardSimilarity()\nrouge_scorer = ROUGE()\n# ... and so on for all metrics in gaico.metrics.__all__\n\nscore = jaccard_scorer.calculate(\"text a\", \"text b\")\n</code></pre>     While convenient, using <code>import *</code> can sometimes make it less clear where names are coming from in larger projects. For <code>gaico.metrics</code>, which is focused, it's generally acceptable.</p> </li> <li> <p>To see what's available (programmatically): <pre><code>import gaico.metrics as gaico_metrics\n\n# Metrics explicitly listed in gaico.metrics.__init__.__all__\navailable_metric_class_names = gaico_metrics.__all__\nprint(\"Available metric classes for direct use (class names):\", available_metric_class_names)\n# Example Output:\n# Available metric classes for direct use (class names): ['BLEU', 'ROUGE', 'JSDivergence', 'BERTScore', 'CosineSimilarity', 'JaccardSimilarity', 'LevenshteinDistance', 'SequenceMatcherSimilarity']\n</code></pre></p> </li> </ul> <p>Note that the class names (e.g., <code>JaccardSimilarity</code>) might be different from the shorter keys used in <code>REGISTERED_METRICS</code> (e.g., <code>Jaccard</code>).</p> </li> <li> <p>Metrics available by string name in the <code>Experiment</code> class:     These are the metrics registered in <code>gaico.experiment.REGISTERED_METRICS</code>. You can access this dictionary programmatically to see which short names are recognized by the <code>Experiment</code> class:</p> <p><pre><code>from gaico.experiment import REGISTERED_METRICS\n\nregistered_metric_names = list(REGISTERED_METRICS.keys())\nprint(\"Metrics usable by name in Experiment class:\", registered_metric_names)\n# Example Output:\n# ['Jaccard', 'Cosine', 'Levenshtein', 'SequenceMatcher', 'BLEU', 'ROUGE', 'JSD', 'BERTScore']\n</code></pre> The <code>gaico.experiment.DEFAULT_METRICS_TO_RUN</code> list also shows which of these are run by default if no specific metrics are requested in <code>Experiment.compare()</code>: <pre><code>from gaico.experiment import DEFAULT_METRICS_TO_RUN\n\nprint(\"Default metrics for Experiment class:\", DEFAULT_METRICS_TO_RUN)\n</code></pre></p> </li> </ol> <p>If you have other questions, please open an issue on our GitHub repository!</p>"},{"location":"installation/","title":"Installation","text":"<p>Currently, GAICo is not available on PyPI. To use it, you'll need to clone the repository and set up the environment using UV.</p> <ol> <li> <p>First, make sure you have UV installed. If not, you can install it by following the instructions on the official UV website.</p> </li> <li> <p>Clone the repository:</p> </li> </ol> <pre><code>git clone https://github.com/ai4society/GenAIResultsComparator.git\ncd GenAIResultsComparator\n</code></pre> <ol> <li>Ensure the dependencies are installed by creating a virtual env. (python 3.12 is recommended):</li> </ol> <pre><code>uv venv\nuv sync\n</code></pre> <ol> <li>(Optional) Activate the virtual environment (doing this avoids prepending <code>uv run</code> to any proceeding commands):    <pre><code>source .venv/bin/activate\n</code></pre></li> </ol> <p>If you don't want to use <code>uv</code>, you can install the dependencies with the following commands:</p> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n</code></pre> <p>However note that the <code>requirements.txt</code> is generated automatically with the pre-commit file and might not include all the dependencies (in such case, a manual pip install might be needed).</p> <p>Now you're ready to use GAICo!</p>"},{"location":"license/","title":"License","text":"<p>This project is licensed under the MIT License:</p> <pre><code>MIT License\n\nCopyright (c) 2024 AI for Society Research Group\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"quickstart/","title":"Quick Start","text":"<p>GAICo makes it easy to evaluate and compare LLM outputs. For detailed, runnable examples, please refer to our Jupyter Notebooks in the <code>examples/</code> folder:</p> <ul> <li><code>quickstart.ipynb</code>: Rapid hands-on with the Experiment sub-module.</li> <li><code>example-1.ipynb</code>: For fine-grained usage, this notebook focuses on comparing multiple model outputs using a single metric.</li> <li><code>example-2.ipynb</code>: For fine-grained usage, this notebook demonstrates evaluating a single model output across all available metrics.</li> </ul>"},{"location":"quickstart/#streamlined-workflow-with-experiment","title":"Streamlined Workflow with <code>Experiment</code>","text":"<p>For a more integrated approach to comparing multiple models, applying thresholds, generating plots, and creating CSV reports, the <code>Experiment</code> class offers a convenient abstraction.</p>"},{"location":"quickstart/#quick-example","title":"Quick Example","text":"<p>This example demonstrates comparing multiple LLM responses against a reference answer using specified metrics, generating a plot, and outputting a CSV report.</p> <pre><code>from gaico import Experiment\n\n# Sample data from https://arxiv.org/abs/2504.07995\nllm_responses = {\n    \"Google\": \"Title: Jimmy Kimmel Reacts to Donald Trump Winning the Presidential ... Snippet: Nov 6, 2024 ...\",\n    \"Mixtral 8x7b\": \"I'm an Al and I don't have the ability to predict the outcome of elections.\",\n    \"SafeChat\": \"Sorry, I am designed not to answer such a question.\",\n}\nreference_answer = \"Sorry, I am unable to answer such a question as it is not appropriate.\"\n\n# 1. Initialize Experiment\nexp = Experiment(\n    llm_responses=llm_responses,\n    reference_answer=reference_answer\n)\n\n# 2. Compare models using specific metrics\n#   This will calculate scores for 'Jaccard' and 'ROUGE',\n#   generate a plot (e.g., radar plot for multiple metrics/models),\n#   and save a CSV report.\nresults_df = exp.compare(\n    metrics=['Jaccard', 'ROUGE'],  # Specify metrics, or None for all defaults\n    plot=True,\n    output_csv_path=\"experiment_report.csv\",\n    custom_thresholds={\"Jaccard\": 0.6, \"ROUGE_rouge1\": 0.35} # Optional: override default thresholds\n)\n\n# The returned DataFrame contains the calculated scores\nprint(\"Scores DataFrame from compare():\")\nprint(results_df)\n</code></pre> <p>This abstraction streamlines common evaluation tasks, while still allowing access to the underlying metric classes and dataframes for more advanced or customized use cases. More details in <code>examples/quickstart.ipynb</code>.</p> <p>However, you might prefer to use the individual metric classes directly for more granular control or if you want to implement custom metrics. See the remaining notebooks in the <code>examples</code> subdirectory.</p> Example Radar Chart generated by the <code>examples/example-2.ipynb</code> notebook."},{"location":"api/experiment/","title":"Experiment Class","text":""},{"location":"api/experiment/#gaico.Experiment","title":"gaico.Experiment","text":"<p>An abstraction to simplify plotting, applying thresholds, and generating CSVs for comparing LLM responses against reference answers using various metrics.</p>"},{"location":"api/experiment/#gaico.Experiment.__init__","title":"__init__","text":"<pre><code>__init__(llm_responses, reference_answer)\n</code></pre> <p>Initializes the Experiment.</p> <p>Parameters:</p> Name Type Description Default <code>llm_responses</code> <code>Dict[str, str]</code> <p>A dictionary mapping model names (str) to their generated text responses (str).</p> required <code>reference_answer</code> <code>str</code> <p>A single reference text (str) to compare against.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If llm_responses is not a dictionary or reference_answer is not a string.</p> <code>ValueError</code> <p>If llm_responses does not contain string keys and values.</p>"},{"location":"api/experiment/#gaico.Experiment.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe(metrics=None)\n</code></pre> <p>Returns a DataFrame of scores for the specified metrics. If metrics is None, scores for all default metrics are returned.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Optional[List[str]]</code> <p>A list of base metric names (e.g., \"Jaccard\", \"ROUGE\"). Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A pandas DataFrame with columns \"model_name\", \"metric_name\", \"score\". \"metric_name\" will contain flat metric names (e.g., \"ROUGE_rouge1\").</p>"},{"location":"api/experiment/#gaico.Experiment.compare","title":"compare","text":"<pre><code>compare(metrics=None, plot=False, custom_thresholds=None, output_csv_path=None, aggregate_func=None, plot_title_suffix='Comparison', radar_metrics_limit=12)\n</code></pre> <p>Compares models based on specified metrics, optionally plotting and generating a CSV.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Optional[List[str]]</code> <p>List of base metric names. If None, uses all default registered metrics.</p> <code>None</code> <code>plot</code> <code>bool</code> <p>If True, generates and shows plots. Defaults to False.</p> <code>False</code> <code>custom_thresholds</code> <code>Optional[Dict[str, float]]</code> <p>Dictionary of metric names (base or flat) to threshold values. Overrides default thresholds.</p> <code>None</code> <code>output_csv_path</code> <code>Optional[str]</code> <p>If provided, path to save a CSV report of thresholded results.</p> <code>None</code> <code>aggregate_func</code> <code>Optional[Callable]</code> <p>Aggregation function (e.g., np.mean, np.median) for plotting when multiple scores exist per model/metric (not typical for default setup).</p> <code>None</code> <code>plot_title_suffix</code> <code>str</code> <p>Suffix for plot titles.</p> <code>'Comparison'</code> <code>radar_metrics_limit</code> <code>int</code> <p>Maximum number of metrics for a radar plot to maintain readability.</p> <code>12</code> <p>Returns:</p> Type Description <code>Optional[pd.DataFrame]</code> <p>A pandas DataFrame containing the scores for the compared metrics, or None if no valid metrics.</p>"},{"location":"api/thresholds_module/","title":"Thresholds Module","text":""},{"location":"api/thresholds_module/#gaico.thresholds.apply_thresholds","title":"gaico.thresholds.apply_thresholds","text":"<pre><code>apply_thresholds(results, thresholds=None)\n</code></pre> <p>Apply thresholds to scores for single pair or batch of generated and reference texts. Type ThresholdedResults is a dictionary where keys are metric names and values are either scores or dictionaries with scores. Specifically, it is of type Dict[str, float | Any].</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>ThresholdedResults | List[ThresholdedResults]</code> <p>Either a single dictionary of scores or a list of score dictionaries Single: {\"BLEU\": 0.6, \"JSD\": 0.1} Batch: [{\"BLEU\": 0.6, \"JSD\": 0.1}, {\"BLEU\": 0.4, \"JSD\": 0.2}]</p> required <code>thresholds</code> <code>Optional[Dict[str, float]]</code> <p>Dictionary of metric names to threshold values. Defaults to get_default_thresholds() if not provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>ThresholdedResults | List[ThresholdedResults]</code> <p>For single input, returns a dictionary. For batch input, returns a list. Single: {\"BLEU\": {\"score\": 0.6, \"threshold_applied\": 0.5, \"passed_threshold\": True}, ...} Batch: [{\"BLEU\": {\"score\": 0.6, ...}, ...}, {\"BLEU\": {\"score\": 0.4, ...}, ...}]</p>"},{"location":"api/thresholds_module/#gaico.thresholds.get_default_thresholds","title":"gaico.thresholds.get_default_thresholds","text":"<pre><code>get_default_thresholds()\n</code></pre> <p>Returns the default thresholds for each metric. This is useful for testing and can be overridden by user-defined thresholds.</p> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>A dictionary of default thresholds for each metric. e.g., {\"BLEU\": 0.5, \"JSD\": 0.5}</p>"},{"location":"api/thresholds_module/#gaico.thresholds.calculate_pass_fail_percent","title":"gaico.thresholds.calculate_pass_fail_percent","text":"<pre><code>calculate_pass_fail_percent(results, thresholds=None)\n</code></pre> <p>Calculate pass/fail percentages for each metric across results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>Dict[str, List[float]]</code> <p>Dictionary where keys are metric names and values are lists of scores</p> required <code>thresholds</code> <code>Optional[Dict[str, float]]</code> <p>Dictionary of thresholds for each metric</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, float | int]]</code> <p>Dictionary with metric names as keys and pass/fail statistics as values</p>"},{"location":"api/thresholds_module/#gaico.thresholds.DEFAULT_THRESHOLD","title":"gaico.thresholds.DEFAULT_THRESHOLD  <code>module-attribute</code>","text":"<pre><code>DEFAULT_THRESHOLD = {'BLEU': 0.5, 'ROUGE': 0.5, 'JSD': 0.5, 'BERTScore': 0.5, 'Jaccard': 0.5, 'Cosine': 0.5, 'Levenshtein': 0.5, 'SequenceMatcher': 0.5}\n</code></pre>"},{"location":"api/utils/","title":"Utility Functions","text":""},{"location":"api/utils/#gaico.utils.prepare_results_dataframe","title":"gaico.utils.prepare_results_dataframe","text":"<pre><code>prepare_results_dataframe(results_dict, model_col='model_name', metric_col='metric_name', score_col='score')\n</code></pre> <p>Converts a nested dictionary of results into a long-format DataFrame suitable for plotting.</p> <p>Example Input <code>results_dict</code>: {     'ModelA': {'BLEU': 0.8, 'ROUGE': {'f1': 0.75}},     'ModelB': {'BLEU': 0.7, 'ROUGE': {'f1': 0.65}} } Example Output DataFrame:    model_name  metric_name  score 0     ModelA    BLEU       0.80 1     ModelA    ROUGE_f1   0.75 2     ModelB    BLEU       0.70 3     ModelB    ROUGE_f1   0.65</p> <p>Parameters:</p> Name Type Description Default <code>results_dict</code> <code>Dict[str, Dict[str, Any]]</code> <p>Nested dictionary where keys are model names and values are dictionaries of metric names to scores (or nested score dicts).</p> required <code>model_col</code> <code>str</code> <p>Name for the column containing model names in the output DataFrame.</p> <code>'model_name'</code> <code>metric_col</code> <code>str</code> <p>Name for the column containing metric names in the output DataFrame.</p> <code>'metric_name'</code> <code>score_col</code> <code>str</code> <p>Name for the column containing scores in the output DataFrame.</p> <code>'score'</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A pandas DataFrame in long format.</p>"},{"location":"api/utils/#gaico.utils.generate_deltas_frame","title":"gaico.utils.generate_deltas_frame","text":"<pre><code>generate_deltas_frame(threshold_results, generated_texts=None, reference_texts=None, output_csv_path=None)\n</code></pre> <p>Generate a Pandas DataFrame from threshold function outputs with optional text strings. If <code>output_csv_path</code> is provided, it saves the DataFrame to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>threshold_results</code> <code>Dict[str, Dict[str, Any]] | List[Dict[str, Dict[str, Any]]]</code> <p>Output from apply_thresholds (handles both single and batch) Single: {\"BLEU\": {\"score\": 0.6, \"threshold_applied\": 0.5, \"passed_threshold\": True}, ...} Batch: [{\"BLEU\": {\"score\": 0.6, ...}, ...}, {\"BLEU\": {\"score\": 0.4, ...}, ...}]</p> required <code>generated_texts</code> <code>Optional[str | List[str]]</code> <p>Optional generated text string(s)</p> <code>None</code> <code>reference_texts</code> <code>Optional[str | List[str]]</code> <p>Optional reference text string(s)</p> <code>None</code> <code>output_csv_path</code> <code>Optional[str]</code> <p>Optional path to save the CSV file</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A Pandas DataFrame containing the results</p>"},{"location":"api/visualize_module/","title":"Visualization Module","text":""},{"location":"api/visualize_module/#gaico.visualize.plot_metric_comparison","title":"gaico.visualize.plot_metric_comparison","text":"<pre><code>plot_metric_comparison(df, aggregate_func=None, **kwargs)\n</code></pre> <p>Generates a bar plot comparing different models based on a single metric, after aggregating scores using the provided aggregate_func.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the scores, typically from prepare_results_dataframe. Expected columns are defined by model_col, metric_col, and score_col in kwargs.</p> required <code>aggregate_func</code> <code>Optional[Callable]</code> <p>A function to aggregate scores (e.g., numpy.mean, numpy.median). Defaults to numpy.mean if None.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments: - metric_name (str, required): The name of the metric to plot. - model_col (str, optional): Name of the column identifying models. Defaults to \"model_name\". - score_col (str, optional): Name of the column containing scores. Defaults to \"score\". - metric_col (str, optional): Name of the column containing metric names. Defaults to \"metric_name\". - title (Optional[str], optional): Title for the plot. - xlabel (Optional[str], optional): Label for the x-axis. Defaults to \"Model\". - ylabel (Optional[str], optional): Label for the y-axis. Defaults to the plotted metric's name. - figsize (tuple, optional): Figure size. Defaults to (10, 6). - axis (Optional[matplotlib.axes.Axes], optional): Matplotlib Axes to plot on. - Other kwargs are passed to seaborn.barplot.</p> <code>{}</code> <p>Returns:</p> Type Description <code>matplotlib.axes.Axes</code> <p>The matplotlib Axes object containing the plot.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If required libraries (matplotlib, seaborn, pandas, numpy) are not installed.</p> <code>ValueError</code> <p>If 'metric_name' is not provided in kwargs.</p>"},{"location":"api/visualize_module/#gaico.visualize.plot_radar_comparison","title":"gaico.visualize.plot_radar_comparison","text":"<pre><code>plot_radar_comparison(df, aggregate_func=None, **kwargs)\n</code></pre> <p>Generates a radar plot comparing multiple models across several metrics, after aggregating scores using the provided aggregate_func.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the scores in long format, typically from prepare_results_dataframe. Expected columns are defined by model_col, metric_col, and score_col in kwargs.</p> required <code>aggregate_func</code> <code>Optional[Callable]</code> <p>A function to aggregate scores (e.g., numpy.mean, numpy.median). Defaults to numpy.mean if None.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments: - metrics (List[str], optional): List of metric names to include. If None, all metrics in df are used. - model_col (str, optional): Name of the column identifying models. Defaults to \"model_name\". - score_col (str, optional): Name of the column containing scores. Defaults to \"score\". - metric_col (str, optional): Name of the column containing metric names. Defaults to \"metric_name\". - title (Optional[str], optional): Title for the plot. Defaults to \"Model Comparison Radar Plot\". - figsize (tuple, optional): Figure size. Defaults to (8, 8). - fill_alpha (float, optional): Alpha for filled area. Defaults to 0.1. - line_width (float, optional): Width of plot lines. Defaults to 1.0. - y_ticks (Optional[List[float]], optional): Custom y-axis ticks. - axis (Optional[matplotlib.axes.Axes], optional): Matplotlib polar Axes to plot on.</p> <code>{}</code> <p>Returns:</p> Type Description <code>matplotlib.axes.Axes</code> <p>The matplotlib Axes object containing the plot.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If required libraries (matplotlib, numpy, pandas) are not installed.</p> <code>ValueError</code> <p>If aggregation results in no data or metrics.</p>"},{"location":"api/metrics/","title":"Metrics","text":"<p>GAICo provides several metric classes for text evaluation. All metrics inherit from <code>BaseMetric</code>.</p> <p>To make a custom metric, you can create a new class that inherits from <code>BaseMetric</code> and implements the <code>calculate()</code> method.</p> <p>To view the documentation for a specific metric, you can click on the metric name in the table of contents on the left.</p>"},{"location":"api/metrics/base/","title":"BaseMetric","text":""},{"location":"api/metrics/base/#gaico.metrics.base.BaseMetric","title":"gaico.metrics.base.BaseMetric","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all language model metrics. This class defines the interface that all metric classes should implement. The public method to be accessed is <code>calculate</code>.</p>"},{"location":"api/metrics/base/#gaico.metrics.base.BaseMetric.calculate","title":"calculate","text":"<pre><code>calculate(generated_texts, reference_texts, **kwargs)\n</code></pre> <p>Calculates the metric for a single or batch of generated and reference texts. This method handles both single and batch inputs for generated and reference texts.</p> <ol> <li>If both inputs are single strings, it will call <code>single_calculate</code>.</li> <li>If one is a string and the other an iterable, it broadcasts the string and calls <code>batch_calculate</code>.</li> <li>If both inputs are iterables, they must have the same length, and it will call <code>batch_calculate</code>.</li> <li>If inputs are None , it raises a ValueError.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>generated_texts</code> <code>str | Iterable | ndarray | Series</code> <p>A single generated text or an iterable of generated texts</p> required <code>reference_texts</code> <code>str | Iterable | ndarray | Series</code> <p>A single reference text or an iterable of reference texts</p> required <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments for specific metrics (additional_params, metric-specific flags etc).</p> <code>{}</code> <p>Returns:</p> Type Description <code>float | List[float] | dict | List[dict] | np.ndarray | pd.Series | None</code> <p>The calculated metric score(s) or None if inputs are invalid.</p>"},{"location":"api/metrics/ngram_metrics/","title":"N-gram Metrics","text":"<p>GAICo provides several n-gram-based metrics for evaluating text similarity and quality.</p> <p>These metrics are useful for tasks such as machine translation evaluation, text summarization, and general text comparison.</p>"},{"location":"api/metrics/ngram_metrics/#gaico.metrics.ngram_metrics.BLEU","title":"gaico.metrics.ngram_metrics.BLEU","text":"<p>               Bases: <code>BaseMetric</code></p> <p>BLEU (Bilingual Evaluation Understudy) score implementation. This class provides methods to calculate BLEU scores for individual sentence pairs and for batches of sentences. It uses the NLTK library to calculate BLEU scores.</p>"},{"location":"api/metrics/ngram_metrics/#gaico.metrics.ngram_metrics.BLEU.__init__","title":"__init__","text":"<pre><code>__init__(n=4, smoothing_function=SmoothingFunction().method1)\n</code></pre> <p>Initialize the BLEU scorer with the specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The max n-gram order to use for BLEU calculation, defaults to 4</p> <code>4</code> <code>smoothing_function</code> <code>Callable | SmoothingFunction</code> <p>The smoothing function to use for BLEU, defaults to SmoothingFunction.method1 from NLTK</p> <code>method1</code>"},{"location":"api/metrics/ngram_metrics/#gaico.metrics.ngram_metrics.ROUGE","title":"gaico.metrics.ngram_metrics.ROUGE","text":"<p>               Bases: <code>BaseMetric</code></p> <p>ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score implementation using the <code>rouge_score</code> library.</p>"},{"location":"api/metrics/ngram_metrics/#gaico.metrics.ngram_metrics.ROUGE.__init__","title":"__init__","text":"<pre><code>__init__(rouge_types=None, use_stemmer=True, **kwargs)\n</code></pre> <p>Initialize the ROUGE scorer with the specified ROUGE types and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>rouge_types</code> <code>Optional[List[str]]</code> <p>The ROUGE types to calculate, defaults to None Should be one of \"rouge1\", \"rouge2\", or \"rougeL\" in a list to return a single F1 score of that type. If multiple types are provided in a list, the output will be a dictionary of F1 scores for each type. Defaults is None which returns a dictionary of all scores. Equivalent of passing [\"rouge1\", \"rouge2\", \"rougeL\"]</p> <code>None</code> <code>use_stemmer</code> <code>bool</code> <p>Whether to use stemming for ROUGE calculation, defaults to True</p> <code>True</code> <code>kwargs</code> <code>Any</code> <p>Additional parameters to pass to the ROUGE calculation, defaults to None Default only passes the <code>use_stemmer</code> parameter</p> <code>{}</code>"},{"location":"api/metrics/ngram_metrics/#gaico.metrics.ngram_metrics.JSDivergence","title":"gaico.metrics.ngram_metrics.JSDivergence","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Jensen-Shannon Divergence metric implementation using the <code>scipy</code> library.</p>"},{"location":"api/metrics/ngram_metrics/#gaico.metrics.ngram_metrics.JSDivergence.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the Jensen-Shannon Divergence metric.</p>"},{"location":"api/metrics/semantic_similarity_metrics/","title":"Semantic Similarity Metrics","text":"<p>GAICo provides a semantic similarity metric to evaluate the semantic similarity between text inputs.</p> <p>This metric is useful for tasks such as semantic text matching, paraphrase detection, and understanding the meaning of text in a more nuanced way.</p>"},{"location":"api/metrics/semantic_similarity_metrics/#gaico.metrics.semantic_similarity_metrics.BERTScore","title":"gaico.metrics.semantic_similarity_metrics.BERTScore","text":"<p>               Bases: <code>BaseMetric</code></p> <p>This class provides methods to calculate BERTScore for individual sentence pairs and for batches of sentences. It uses the BERTScore library to calculate precision, recall, and F1 scores.</p>"},{"location":"api/metrics/semantic_similarity_metrics/#gaico.metrics.semantic_similarity_metrics.BERTScore.__init__","title":"__init__","text":"<pre><code>__init__(model_type='bert-base-uncased', output_val=None, num_layers=8, batch_size=64, additional_params=None)\n</code></pre> <p>Initialize the BERTScore metric.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The BERT model to use, defaults to \"bert-base-uncased\"</p> <code>'bert-base-uncased'</code> <code>output_val</code> <code>Optional[List[str]]</code> <p>The output value to return, defaults to None Should be one of \"precision\", \"recall\", or \"f1\" to return a single score of type float Wrap in a list to return multiple scores of type dict Default returns a dictionary of all scores. Equivalent to passing [\"precision\", \"recall\", \"f1\"]</p> <code>None</code> <code>num_layers</code> <code>int</code> <p>Number of layers to use from BERT, defaults to 8</p> <code>8</code> <code>batch_size</code> <code>int</code> <p>Batch size for processing, defaults to 64</p> <code>64</code> <code>additional_params</code> <code>Optional[Dict[str, Any]]</code> <p>Additional parameters to pass to the BERTScorer class from the bert_score library, defaults to None Default only passes the model_type, num_layers, and batch_size</p> <code>None</code>"},{"location":"api/metrics/text_similarity_metrics/","title":"Text Similarity Metrics","text":"<p>GAICo provides several text similarity metrics to evaluate the similarity between text inputs.</p> <p>These metrics are useful for tasks such as text comparison, duplicate detection, and semantic similarity analysis.</p>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.JaccardSimilarity","title":"gaico.metrics.text_similarity_metrics.JaccardSimilarity","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Jaccard Similarity implementation for text similarity using the formula: J(A, B) = |A \u2229 B| / |A \u222a B|</p> <p>Supports calculation for individual sentence pairs and for batches of sentences.</p>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.JaccardSimilarity.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the Jaccard Similarity metric.</p>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.CosineSimilarity","title":"gaico.metrics.text_similarity_metrics.CosineSimilarity","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Cosine Similarity implementation for text similarity using <code>cosine_similarity</code> from scikit-learn. The class also uses the <code>CountVectorizer</code> from scikit-learn to convert text to vectors.</p> <p>Supports calculation for individual sentence pairs and for batches of sentences.</p>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.CosineSimilarity.__init__","title":"__init__","text":"<pre><code>__init__(**kwargs)\n</code></pre> <p>Initialize the Cosine Similarity metric.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Any</code> <p>Parameters for the CountVectorizer</p> <code>{}</code>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.LevenshteinDistance","title":"gaico.metrics.text_similarity_metrics.LevenshteinDistance","text":"<p>               Bases: <code>BaseMetric</code></p> <p>This class provides methods to calculate Levenshtein Distance for individual sentence pairs and for batches of sentences. It uses the <code>distance</code> and <code>ratio</code> functions from the <code>Levenshtein</code> package.</p>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.LevenshteinDistance.__init__","title":"__init__","text":"<pre><code>__init__(calculate_ratio=True)\n</code></pre> <p>Initialize the Levenshtein Distance metric.</p> <p>Parameters:</p> Name Type Description Default <code>calculate_ratio</code> <code>bool</code> <p>Whether to calculate the ratio of the distance to the length of the longer string, defaults to True.</p> <code>True</code>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.SequenceMatcherSimilarity","title":"gaico.metrics.text_similarity_metrics.SequenceMatcherSimilarity","text":"<p>               Bases: <code>BaseMetric</code></p> <p>This class calculates similarity ratio between texts using the ratio() method from difflib.SequenceMatcher, which returns a float in the range [0, 1] indicating how similar the sequences are.</p> <p>Supports calculation for individual sentence pairs and for batches of sentences.</p>"},{"location":"api/metrics/text_similarity_metrics/#gaico.metrics.text_similarity_metrics.SequenceMatcherSimilarity.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the SequenceMatcher Similarity metric</p>"},{"location":"user_guide/direct_metrics/","title":"Using Metrics Directly","text":"<p>While the Experiment Class provides a high-level interface for common evaluation scenarios, GAICo also allows you to use its individual metric classes directly. This approach offers more granular control and flexibility, especially when:</p> <ul> <li>You need to evaluate a single model's output against a reference.</li> <li>You are comparing lists of generated texts against corresponding lists of reference texts (i.e., pair-wise comparisons for multiple examples).</li> <li>You want to integrate a specific metric into a custom evaluation pipeline.</li> <li>You need to configure metric-specific parameters not exposed by the <code>Experiment</code> class's default initialization.</li> <li>You are developing or testing a new custom metric.</li> </ul>"},{"location":"user_guide/direct_metrics/#the-basemetric-class","title":"The <code>BaseMetric</code> Class","text":"<p>All metric classes in GAICo (e.g., <code>JaccardSimilarity</code>, <code>ROUGE</code>, <code>BERTScore</code>) inherit from the <code>gaico.metrics.base.BaseMetric</code> abstract class. This base class defines the common interface for all metrics, primarily through the <code>calculate()</code> method.</p>"},{"location":"user_guide/direct_metrics/#core-method-calculate","title":"Core Method: <code>calculate()</code>","text":"<p>The <code>calculate()</code> method is the primary way to compute a metric's score. It's designed to be flexible and can handle:</p> <ul> <li>Single Pair of Texts: Comparing one generated text string to one reference text string.</li> <li>Batch of Texts: Comparing an iterable (list, NumPy array, Pandas Series) of generated texts to a corresponding iterable of reference texts.</li> <li>Broadcasting: Comparing a single generated text to multiple reference texts, or multiple generated texts to a single reference text.</li> </ul>"},{"location":"user_guide/direct_metrics/#parameters-of-calculate","title":"Parameters of <code>calculate()</code>:","text":"<ul> <li><code>generated_texts</code> (<code>str | Iterable | np.ndarray | pd.Series</code>):     A single generated text string or an iterable of generated texts.</li> <li><code>reference_texts</code> (<code>str | Iterable | np.ndarray | pd.Series</code>):     A single reference text string or an iterable of reference texts.</li> <li><code>**kwargs</code>: Additional keyword arguments specific to the metric being used (e.g., <code>use_corpus_bleu=False</code> for <code>BLEU</code>, or <code>output_val=['f1']</code> for <code>BERTScore</code>).</li> </ul>"},{"location":"user_guide/direct_metrics/#return-value-of-calculate","title":"Return Value of <code>calculate()</code>:","text":"<p>The return type depends on the metric and the input: *   For most metrics, it returns a <code>float</code> for single inputs or a <code>List[float]</code> (or <code>np.ndarray</code>/<code>pd.Series</code> if inputs were such) for batch inputs. *   Metrics like <code>ROUGE</code> or <code>BERTScore</code> can return a <code>dict</code> of scores (e.g., <code>{'rouge1': 0.8, 'rougeL': 0.75}</code>) for single inputs or a <code>List[dict]</code> for batch inputs, depending on their configuration.</p>"},{"location":"user_guide/direct_metrics/#examples","title":"Examples","text":"<p>Let's look at how to use some of the individual metric classes.</p>"},{"location":"user_guide/direct_metrics/#1-jaccard-similarity","title":"1. Jaccard Similarity","text":"<pre><code>from gaico.metrics import JaccardSimilarity\n\n# Initialize the metric\njaccard_metric = JaccardSimilarity()\n\n#  Single Pair\ngenerated_text_single = \"The quick brown fox\"\nreference_text_single = \"A quick brown dog\"\nscore_single = jaccard_metric.calculate(generated_text_single, reference_text_single)\nprint(f\"Jaccard Score (Single): {score_single}\") # Output: Jaccard Score (Single): 0.3333333333333333\n\n#  Batch of Texts\ngenerated_texts_batch = [\"Hello world\", \"GAICo is great\"]\nreference_texts_batch = [\"Hello there world\", \"GAICo is an awesome library\"]\nscores_batch = jaccard_metric.calculate(generated_texts_batch, reference_texts_batch)\nprint(f\"Jaccard Scores (Batch): {scores_batch}\")\n# Jaccard Scores (Batch): [0.6666666666666666, 0.3333333333333333]\n\n#  Broadcasting: Single generated text to multiple references\ngenerated_text_broadcast = \"Common evaluation text\"\nreference_texts_list = [\"Evaluation text for comparison\", \"Another reference text\"]\nscores_broadcast_gen = jaccard_metric.calculate(generated_text_broadcast, reference_texts_list)\nprint(f\"Jaccard Scores (Broadcast Gen): {scores_broadcast_gen}\")\n# Jaccard Scores (Broadcast Gen): [0.4, 0.2]\n</code></pre>"},{"location":"user_guide/direct_metrics/#2-rouge-score-with-specific-configuration","title":"2. ROUGE Score (with specific configuration)","text":"<p>The <code>ROUGE</code> metric, by default, calculates 'rouge1', 'rouge2', and 'rougeL' F1-scores. You can customize this.</p> <pre><code>from gaico.metrics import ROUGE\n\n# Initialize ROUGE to calculate only 'rouge1' and 'rougeL' F1-scores\nrouge_metric = ROUGE(rouge_types=['rouge1', 'rougeL'], use_stemmer=True)\n\ngenerated = \"The cat sat on the mat.\"\nreference = \"A cat was sitting on a mat.\"\n\n# Calculate ROUGE scores\nrouge_scores = rouge_metric.calculate(generated, reference)\nprint(f\"ROUGE Scores: {rouge_scores}\")\n# Example Output: ROUGE Scores: {'rouge1': 0.4615384615384615, 'rougeL': 0.4615384615384615}\n\n# If you configure for a single ROUGE type, it returns a float\nrouge_metric_single_type = ROUGE(rouge_types=['rougeL'])\nrouge_l_score = rouge_metric_single_type.calculate(generated, reference)\nprint(f\"ROUGE-L Score: {rouge_l_score}\") # Example Output: ROUGE-L Score: 0.4615384615384615\n</code></pre>"},{"location":"user_guide/direct_metrics/#3-bertscore-with-specific-output","title":"3. BERTScore (with specific output)","text":"<p><code>BERTScore</code> can also be configured to return specific components (precision, recall, or F1) or a dictionary of them.</p> <pre><code>from gaico.metrics import BERTScore\n\n# Initialize BERTScore to return only the F1 score\n# Note: BERTScore can be slow to initialize the first time as it downloads models.\n# For faster tests/examples, you might use a smaller model or mock it.\n\n# To get a dictionary with only F1 scores:\nbertscore_metric_f1_dict = BERTScore()\ngenerated_bert = \"This is a test sentence for BERTScore.\"\nreference_bert = \"This is a reference sentence for BERTScore evaluation.\"\nbert_f1_dict = bertscore_metric_f1_dict.calculate(generated_bert, reference_bert)\nprint(f\"BERTScore (F1 dict): {bert_f1_dict}\")\n# Example Output: BERTScore (F1 dict): {'precision': 0.9229249954223633, 'recall': 0.8905344009399414, 'f1': 0.9064403772354126}\n\nbertscore_metric_f1 = BERTScore(output_val=['f1']) # Returns a dict: {'f1': value}\nbert_f1_score_float = bertscore_metric_f1.calculate(generated_bert, reference_bert) # Using the same instance\nprint(f\"BERTScore (F1 float): {bert_f1_score_float}\") # This will be the float value of F1\n# Example Output: BERTScore (F1 float): 0.9064403772354126...\n</code></pre>"},{"location":"user_guide/direct_metrics/#available-metrics","title":"Available Metrics","text":"<p>GAICo includes the following built-in metrics, all usable directly:</p> <ul> <li>N-gram-based Metrics:<ul> <li><code>gaico.metrics.BLEU</code></li> <li><code>gaico.metrics.ROUGE</code></li> <li><code>gaico.metrics.JSDivergence</code> (Jensen-Shannon Divergence)</li> </ul> </li> <li>Text Similarity Metrics:<ul> <li><code>gaico.metrics.JaccardSimilarity</code></li> <li><code>gaico.metrics.CosineSimilarity</code></li> <li><code>gaico.metrics.LevenshteinDistance</code></li> <li><code>gaico.metrics.SequenceMatcherSimilarity</code></li> </ul> </li> <li>Semantic Similarity Metrics:<ul> <li><code>gaico.metrics.BERTScore</code></li> </ul> </li> </ul> <p>Refer to the API Reference for detailed constructor parameters and any specific <code>**kwargs</code> for each metric's <code>calculate()</code> method.</p> <p>Using metrics directly provides the foundational building blocks for more complex evaluation setups or for when you need precise control over individual metric calculations.</p>"},{"location":"user_guide/experiment_class/","title":"The Experiment Class","text":"<p>The <code>Experiment</code> class in GAICo provides a streamlined and integrated workflow for evaluating and comparing multiple Language Model (LLM) responses against a single reference answer. It simplifies common tasks such as:</p> <ul> <li>Calculating scores for multiple metrics across different LLM outputs.</li> <li>Applying custom or default thresholds to these scores.</li> <li>Generating comparative plots (bar charts for single metrics, radar charts for multiple metrics).</li> <li>Creating CSV reports summarizing the evaluation.</li> </ul>"},{"location":"user_guide/experiment_class/#why-use-the-experiment-class","title":"Why Use the <code>Experiment</code> Class?","text":"<p>While you can use GAICo's individual metric classes directly for fine-grained control (see Using Metrics Directly), the <code>Experiment</code> class is ideal when:</p> <ul> <li>You have responses from several LLMs (or different versions of the same LLM) that you want to compare against a common reference.</li> <li>You want a quick way to get an overview of performance across multiple metrics.</li> <li>You need to generate reports and visualizations with minimal boilerplate code.</li> </ul> <p>It acts as a high-level orchestrator, making the end-to-end evaluation process more convenient.</p>"},{"location":"user_guide/experiment_class/#initializing-an-experiment","title":"Initializing an Experiment","text":"<p>To start, you need to instantiate the <code>Experiment</code> class. It requires two main pieces of information:</p> <ol> <li><code>llm_responses</code>: A Python dictionary where keys are model names (strings) and values are the text responses generated by those models (strings).</li> <li><code>reference_answer</code>: A single string representing the ground truth or reference text against which all LLM responses will be compared.</li> </ol> <pre><code>from gaico import Experiment\n\n# Example LLM responses from different models\nllm_responses = {\n    \"Google\": \"Title: Jimmy Kimmel Reacts to Donald Trump Winning the Presidential ... Snippet: Nov 6, 2024 ...\",\n    \"Mixtral 8x7b\": \"I'm an Al and I don't have the ability to predict the outcome of elections.\",\n    \"SafeChat\": \"Sorry, I am designed not to answer such a question.\",\n}\n\n# The reference answer\nreference_answer = \"Sorry, I am unable to answer such a question as it is not appropriate.\"\n\n# Initialize the Experiment\nexp = Experiment(\n    llm_responses=llm_responses,\n    reference_answer=reference_answer\n)\n</code></pre>"},{"location":"user_guide/experiment_class/#comparing-models-with-compare","title":"Comparing Models with <code>compare()</code>","text":"<p>The primary method for conducting the evaluation is <code>compare()</code>. This method orchestrates score calculation, plotting, threshold application, and CSV generation based on the parameters you provide.</p> <pre><code># Basic comparison using default metrics, without plotting or CSV output\nresults_df = exp.compare()\nprint(\"Scores DataFrame (default metrics):\")\nprint(results_df)\n\n# A more comprehensive comparison:\n# - Specify a subset of metrics\n# - Enable plotting\n# - Define custom thresholds for some metrics\n# - Output results to a CSV file\nresults_df_custom = exp.compare(\n    metrics=['Jaccard', 'ROUGE', 'Levenshtein'], # Specify metrics, or None for all defaults\n    plot=True,                                  # Generate and show plots\n    output_csv_path=\"my_experiment_report.csv\", # Save a CSV report\n    custom_thresholds={\"Jaccard\": 0.6, \"ROUGE_rougeL\": 0.35} # Optional: override default thresholds\n)\n\nprint(\"\\nScores DataFrame (custom run):\")\nprint(results_df_custom)\n</code></pre>"},{"location":"user_guide/experiment_class/#key-parameters-of-compare","title":"Key Parameters of <code>compare()</code>:","text":"<ul> <li><code>metrics</code> (Optional <code>List[str]</code>):<ul> <li>A list of base metric names (e.g., <code>\"Jaccard\"</code>, <code>\"ROUGE\"</code>, <code>\"BERTScore\"</code>) to calculate.</li> <li>If <code>None</code> (default), all registered default metrics are used (currently: Jaccard, Cosine, Levenshtein, SequenceMatcher, BLEU, ROUGE, JSD, BERTScore).</li> <li>Note: For metrics like ROUGE or BERTScore that produce multiple sub-scores (e.g., <code>ROUGE_rouge1</code>, <code>ROUGE_rougeL</code>, <code>BERTScore_f1</code>), specifying the base name (e.g., <code>\"ROUGE\"</code>) will include all its sub-scores in the results.</li> </ul> </li> <li><code>plot</code> (Optional <code>bool</code>, default <code>False</code>):<ul> <li>If <code>True</code>, generates and displays plots using Matplotlib/Seaborn.</li> <li>If one metric is evaluated, a bar chart comparing models for that metric is shown.</li> <li>If multiple metrics are evaluated (and at least 3, up to <code>radar_metrics_limit</code>), a radar chart is shown, providing a multi-dimensional comparison of models. If fewer than 3 (but more than 1) or more than <code>radar_metrics_limit</code> metrics are present, individual bar charts are shown for each.</li> </ul> </li> <li><code>custom_thresholds</code> (Optional <code>Dict[str, float]</code>):<ul> <li>A dictionary to specify custom pass/fail thresholds for metrics.</li> <li>Keys can be base metric names (e.g., <code>\"Jaccard\"</code>) or specific \"flattened\" metric names as they appear in the output DataFrame (e.g., <code>\"ROUGE_rouge1\"</code>, <code>\"BERTScore_f1\"</code>).</li> <li>These thresholds override the library's default thresholds for the specified metrics.</li> <li>The thresholds are used to determine the \"pass/fail\" status in the CSV report.</li> </ul> </li> <li><code>output_csv_path</code> (Optional <code>str</code>):<ul> <li>If a file path is provided, a CSV file is generated at this location.</li> <li>The CSV report includes:<ul> <li><code>generated_text</code>: The response from each LLM.</li> <li><code>reference_text</code>: The reference answer (repeated for each model).</li> <li>Columns for each metric's score (e.g., <code>Jaccard_score</code>, <code>ROUGE_rouge1_score</code>).</li> <li>Columns for each metric's pass/fail status based on the applied threshold (e.g., <code>Jaccard_passed</code>, <code>ROUGE_rouge1_passed</code>).</li> </ul> </li> </ul> </li> <li><code>aggregate_func</code> (Optional <code>Callable</code>):<ul> <li>An aggregation function (e.g., <code>numpy.mean</code>, <code>numpy.median</code>) used for plotting when multiple scores might exist per model/metric. For the standard <code>Experiment</code> use case (one response per model), this typically doesn't change the outcome of scores but is available for plot customization. Defaults to <code>numpy.mean</code>.</li> </ul> </li> <li><code>plot_title_suffix</code> (Optional <code>str</code>, default <code>\"Comparison\"</code>):<ul> <li>A string suffix added to the titles of generated plots.</li> </ul> </li> <li><code>radar_metrics_limit</code> (Optional <code>int</code>, default <code>12</code>):<ul> <li>The maximum number of metrics to display on a single radar plot to maintain readability. If more metrics are present and a radar plot is applicable, only the first <code>radar_metrics_limit</code> are plotted on that radar chart.</li> </ul> </li> </ul>"},{"location":"user_guide/experiment_class/#what-compare-returns","title":"What <code>compare()</code> Returns","text":"<p>The <code>compare()</code> method returns a pandas DataFrame containing the calculated scores. The DataFrame typically has the following columns:</p> <ul> <li><code>model_name</code>: The name of the LLM (from the keys of your <code>llm_responses</code> dictionary).</li> <li><code>metric_name</code>: The specific metric calculated. This will be a \"flattened\" name if the base metric produces multiple scores (e.g., <code>\"Jaccard\"</code>, <code>\"ROUGE_rouge1\"</code>, <code>\"ROUGE_rougeL\"</code>, <code>\"BERTScore_f1\"</code>).</li> <li><code>score</code>: The numerical score for that model and metric, typically normalized between 0 and 1.</li> </ul> <p>This DataFrame is useful for any further custom analysis, filtering, or reporting you might want to perform.</p>"},{"location":"user_guide/experiment_class/#accessing-scores-separately-with-to_dataframe","title":"Accessing Scores Separately with <code>to_dataframe()</code>","text":"<p>If you only need the scores in a pandas DataFrame without triggering plots or CSV generation, you can use the <code>to_dataframe()</code> method. This can be useful for programmatic access to the scores.</p> <p><pre><code># Get scores for Jaccard and Levenshtein only\nscores_subset_df = exp.to_dataframe(metrics=['Jaccard', 'Levenshtein'])\nprint(\"\\nSubset of scores:\")\nprint(scores_subset_df)\n\n# Get scores for all default metrics (if not already computed, they will be)\nall_scores_df = exp.to_dataframe() # Equivalent to exp.to_dataframe(metrics=None)\nprint(\"\\nAll default scores:\")\nprint(all_scores_df)\n</code></pre> This method is efficient as it uses cached scores if they have already been computed (e.g., by a previous call to <code>compare()</code> or an earlier call to <code>to_dataframe()</code>). If scores for requested metrics haven't been computed yet, <code>to_dataframe()</code> will calculate them.</p>"},{"location":"user_guide/experiment_class/#how-experiment-uses-metrics","title":"How <code>Experiment</code> Uses Metrics","text":"<p>Internally, the <code>Experiment</code> class instantiates and utilizes the individual metric classes (like <code>JaccardSimilarity</code>, <code>ROUGE</code>, <code>BERTScore</code>, etc.) found in the <code>gaico.metrics</code> module. It handles the iteration over your LLM responses, applies each specified metric to compare each response against the single <code>reference_answer</code>, and then aggregates these results into the output DataFrame.</p> <p>For most common comparison tasks involving multiple models against a single reference, the <code>Experiment</code> class provides the most convenient and comprehensive interface. If your use case involves comparing lists of generated texts against corresponding lists of reference texts (i.e., pair-wise comparisons for multiple examples), or if you need to implement highly custom evaluation logic or integrate new, un-registered metrics on the fly, using the individual metric classes directly might be more appropriate.</p>"},{"location":"user_guide/thresholds/","title":"Working with Thresholds","text":"<p>GAICo provides utilities to apply thresholds to metric scores and analyze pass/fail statistics. This is useful for determining if generated text meets a certain quality bar for specific metrics. While the Experiment Class can apply thresholds automatically, you can also use these functions directly.</p>"},{"location":"user_guide/thresholds/#key-threshold-functions","title":"Key Threshold Functions","text":"<p>The primary functions for working with thresholds are located in the <code>gaico.thresholds</code> module:</p> <ul> <li><code>get_default_thresholds()</code>: Returns a dictionary of the library's default threshold values for each base metric.</li> <li><code>apply_thresholds()</code>: Applies specified or default thresholds to a dictionary (or list of dictionaries) of metric scores.</li> <li><code>calculate_pass_fail_percent()</code>: Calculates pass/fail percentages for a collection of scores for each metric.</li> </ul>"},{"location":"user_guide/thresholds/#1-getting-default-thresholds","title":"1. Getting Default Thresholds","text":"<p>You can inspect the default thresholds used by the library:</p> <p><pre><code>from gaico.thresholds import get_default_thresholds\n\ndefault_thresholds = get_default_thresholds()\nprint(\"Default Thresholds:\")\nfor metric, threshold_val in default_thresholds.items():\n    print(f\"- {metric}: {threshold_val}\")\n\n# Example Output:\n# Default Thresholds:\n# - BLEU: 0.5\n# - ROUGE: 0.5\n# - JSD: 0.5\n# - BERTScore: 0.5\n# - Jaccard: 0.5\n# - Cosine: 0.5\n# - Levenshtein: 0.5\n# - SequenceMatcher: 0.5\n</code></pre> The <code>DEFAULT_THRESHOLD</code> constant in <code>gaico.thresholds</code> also holds these values.</p>"},{"location":"user_guide/thresholds/#2-applying-thresholds-with-apply_thresholds","title":"2. Applying Thresholds with <code>apply_thresholds()</code>","text":"<p>This function takes your calculated metric scores and a dictionary of thresholds, then returns a detailed structure indicating the score, the threshold applied, and whether the score passed the threshold.</p>"},{"location":"user_guide/thresholds/#input","title":"Input:","text":"<ul> <li><code>results</code> (<code>Dict[str, Union[float, Any]] | List[Dict[str, Union[float, Any]]]</code>):<ul> <li>For a single evaluation: A dictionary where keys are metric names (e.g., \"Jaccard\", \"ROUGE_rouge1\") and values are their scores.</li> <li>For batch evaluations: A list of such dictionaries.</li> </ul> </li> <li><code>thresholds</code> (Optional <code>Dict[str, float]</code>):<ul> <li>A dictionary mapping metric names to their desired threshold values.</li> <li>If <code>None</code>, <code>get_default_thresholds()</code> is used.</li> <li>Note: For JSDivergence, \"passing\" means <code>(1 - score) &gt;= threshold_value</code> because lower JSD is better. For other metrics, \"passing\" means <code>score &gt;= threshold_value</code>.</li> </ul> </li> </ul>"},{"location":"user_guide/thresholds/#output","title":"Output:","text":"<p>A dictionary (or list of dictionaries) where each metric entry contains: *   <code>\"score\"</code>: The original score. *   <code>\"threshold_applied\"</code>: The threshold value used for this metric. *   <code>\"passed_threshold\"</code>: A boolean indicating if the score passed.</p>"},{"location":"user_guide/thresholds/#example","title":"Example:","text":"<pre><code>from gaico.thresholds import apply_thresholds, get_default_thresholds\n\n# Example scores for a single evaluation\nsingle_eval_scores = {\n    \"Jaccard\": 0.75,\n    \"ROUGE_rouge1\": 0.45,\n    \"Levenshtein\": 0.80,\n    \"JSD\": 0.2 # Lower JSD is better\n}\n\n# Using default thresholds\nthresholded_results_default = apply_thresholds(single_eval_scores)\nprint(\"Thresholded Results (Default):\")\nfor metric, details in thresholded_results_default.items():\n    print(f\"  {metric}: Score={details['score']}, Threshold={details['threshold_applied']}, Passed={details['passed_threshold']}\")\n\n# Thresholded Results (Default):\n#   Jaccard: Score=0.75, Threshold=0.5, Passed=True\n#   Levenshtein: Score=0.8, Threshold=0.5, Passed=True\n#   JSD: Score=0.2, Threshold=0.5, Passed=True\n\n# Using custom thresholds\ncustom_thresholds = {\n    \"Jaccard\": 0.7,\n    \"ROUGE_rouge1\": 0.5,\n    \"Levenshtein\": 0.75,\n    \"JSD\": 0.6 # This means (1 - JSD_score) should be &gt;= 0.6, so JSD_score &lt;= 0.4\n}\nthresholded_results_custom = apply_thresholds(single_eval_scores, thresholds=custom_thresholds)\nprint(\"\\nThresholded Results (Custom):\")\nfor metric, details in thresholded_results_custom.items():\n    print(f\"  {metric}: Score={details['score']}, Threshold={details['threshold_applied']}, Passed={details['passed_threshold']}\")\n\n# Thresholded Results (Custom):\n#   Jaccard: Score=0.75, Threshold=0.7, Passed=True\n#   ROUGE_rouge1: Score=0.45, Threshold=0.5, Passed=False\n#   Levenshtein: Score=0.8, Threshold=0.75, Passed=True\n#   JSD: Score=0.2, Threshold=0.6, Passed=True\n\n# Example for batch results\nbatch_eval_scores = [\n    {\"Jaccard\": 0.8, \"ROUGE_rouge1\": 0.6},\n    {\"Jaccard\": 0.4, \"ROUGE_rouge1\": 0.3}\n]\nthresholded_batch = apply_thresholds(batch_eval_scores, thresholds={\"Jaccard\": 0.5, \"ROUGE_rouge1\": 0.55})\nprint(\"\\nThresholded Batch Results (Custom):\")\nfor i, item_results in enumerate(thresholded_batch):\n    print(f\"  Item {i+1}:\")\n    for metric, details in item_results.items():\n        print(f\"    {metric}: Score={details['score']}, Threshold={details['threshold_applied']}, Passed={details['passed_threshold']}\")\n\n# Thresholded Batch Results (Custom):\n#   Item 1:\n#     Jaccard: Score=0.8, Threshold=0.5, Passed=True\n#     ROUGE_rouge1: Score=0.6, Threshold=0.55, Passed=True\n#   Item 2:\n#     Jaccard: Score=0.4, Threshold=0.5, Passed=False\n#     ROUGE_rouge1: Score=0.3, Threshold=0.55, Passed=False\n</code></pre> <p>The output from <code>apply_thresholds</code> is also what <code>gaico.utils.generate_deltas_frame</code> (used by <code>Experiment.compare()</code> for CSV output) expects.</p>"},{"location":"user_guide/thresholds/#3-calculating-passfail-percentages-with-calculate_pass_fail_percent","title":"3. Calculating Pass/Fail Percentages with <code>calculate_pass_fail_percent()</code>","text":"<p>If you have a collection of scores for multiple items (e.g., from evaluating many generated texts against their references) and want to see overall pass/fail rates for each metric, this function is useful.</p>"},{"location":"user_guide/thresholds/#input_1","title":"Input:","text":"<ul> <li><code>results</code> (<code>Dict[str, List[float]]</code>):     A dictionary where keys are metric names and values are lists of scores obtained for that metric across multiple evaluations.</li> <li><code>thresholds</code> (Optional <code>Dict[str, float]</code>):     Custom thresholds to use. Defaults to <code>get_default_thresholds()</code>.</li> </ul>"},{"location":"user_guide/thresholds/#output_1","title":"Output:","text":"<p>A dictionary where keys are metric names. Each value is another dictionary containing: *   <code>\"total_passed\"</code>: Count of items that passed the threshold. *   <code>\"total_failed\"</code>: Count of items that failed. *   <code>\"pass_percentage\"</code>: Percentage of items that passed. *   <code>\"fail_percentage\"</code>: Percentage of items that failed.</p>"},{"location":"user_guide/thresholds/#example_1","title":"Example:","text":"<pre><code>from gaico.thresholds import calculate_pass_fail_percent\n\n# Example: Scores from multiple evaluations for different metrics\nbatch_scores_for_stats = {\n    \"Jaccard\": [0.8, 0.4, 0.9, 0.6, 0.7],\n    \"Levenshtein\": [0.9, 0.85, 0.6, 0.77, 0.92],\n    \"JSD\": [0.1, 0.5, 0.05, 0.6, 0.2] # Lower is better\n}\n\ncustom_thresholds_for_stats = {\n    \"Jaccard\": 0.7,\n    \"Levenshtein\": 0.8,\n    \"JSD\": 0.6 # (1 - JSD_score) &gt;= 0.6  =&gt; JSD_score &lt;= 0.4\n}\n\npass_fail_stats = calculate_pass_fail_percent(batch_scores_for_stats, thresholds=custom_thresholds_for_stats)\n\nprint(\"\\nPass/Fail Statistics:\")\nfor metric, stats in pass_fail_stats.items():\n    print(f\"  Metric: {metric}\")\n    print(f\"    Total Passed: {stats['total_passed']}\")\n    print(f\"    Total Failed: {stats['total_failed']}\")\n    print(f\"    Pass Percentage: {stats['pass_percentage']:.2f}%\")\n    print(f\"    Fail Percentage: {stats['fail_percentage']:.2f}%\")\n\n# Pass/Fail Statistics:\n#   Metric: Jaccard\n#     Total Passed: 3\n#     Total Failed: 2\n#     Pass Percentage: 60.00%\n#     Fail Percentage: 40.00%\n#   Metric: Levenshtein\n#     Total Passed: 3\n#     Total Failed: 2\n#     Pass Percentage: 60.00%\n#     Fail Percentage: 40.00%\n#   Metric: JSD\n#     Total Passed: 3\n#     Total Failed: 2\n#     Pass Percentage: 60.00%\n#     Fail Percentage: 40.00%\n</code></pre> <p>These thresholding utilities provide flexible ways to interpret your metric scores beyond just their raw values, helping you make more informed decisions about model performance.</p>"},{"location":"user_guide/visualization/","title":"Visualization","text":"<p>GAICo includes functions to help you visualize comparison results, making it easier to understand model performance across different metrics. These functions are used internally by the Experiment Class when <code>plot=True</code> is set in the <code>compare()</code> method, but you can also use them directly for custom plotting needs.</p> <p>The primary visualization functions are found in the <code>gaico.visualize</code> module:</p> <ul> <li><code>plot_metric_comparison()</code>: Generates a bar plot comparing different models based on a single metric.</li> <li><code>plot_radar_comparison()</code>: Generates a radar (spider) plot comparing multiple models across several metrics.</li> </ul> <p>Prerequisites: These functions require <code>matplotlib</code>, <code>seaborn</code>, <code>numpy</code>, and <code>pandas</code> to be installed. GAICo attempts to import them, and will raise an <code>ImportError</code> if they are not available.</p>"},{"location":"user_guide/visualization/#preparing-data-for-plotting","title":"Preparing Data for Plotting","text":"<p>Both plotting functions expect input data as a pandas DataFrame in a \"long\" format. This DataFrame typically has columns for model names, metric names, and scores. The <code>gaico.utils.prepare_results_dataframe()</code> function is designed to convert a nested dictionary of scores into this format.</p>"},{"location":"user_guide/visualization/#prepare_results_dataframe","title":"<code>prepare_results_dataframe()</code>","text":"<p>This utility takes a dictionary where keys are model names and values are dictionaries of metric names to scores (or nested score dictionaries, like those from ROUGE or BERTScore).</p> <p><pre><code>from gaico.utils import prepare_results_dataframe\nimport pandas as pd\n\n# Example raw scores (perhaps from direct metric calculations)\nraw_scores_data = {\n    'Model_A': {'Jaccard': 0.8, 'ROUGE': {'rouge1': 0.75, 'rougeL': 0.70}, 'Levenshtein': 0.85},\n    'Model_B': {'Jaccard': 0.6, 'ROUGE': {'rouge1': 0.65, 'rougeL': 0.60}, 'Levenshtein': 0.70},\n    'Model_C': {'Jaccard': 0.9, 'ROUGE': {'rouge1': 0.80, 'rougeL': 0.78}, 'Levenshtein': 0.92}\n}\n\n# Convert to a long-format DataFrame\nplot_df = prepare_results_dataframe(raw_scores_data)\nprint(\"Prepared DataFrame for Plotting:\")\nprint(plot_df)\n\n# Expected Output:\n# Prepared DataFrame for Plotting:\n#    model_name   metric_name  score\n# 0     Model_A       Jaccard   0.80\n# 1     Model_A  ROUGE_rouge1   0.75\n# 2     Model_A  ROUGE_rougeL   0.70\n# 3     Model_A   Levenshtein   0.85\n# 4     Model_B       Jaccard   0.60\n# 5     Model_B  ROUGE_rouge1   0.65\n# 6     Model_B  ROUGE_rougeL   0.60\n# 7     Model_B   Levenshtein   0.70\n# 8     Model_C       Jaccard   0.90\n# 9     Model_C  ROUGE_rouge1   0.80\n# 10    Model_C  ROUGE_rougeL   0.78\n# 11    Model_C   Levenshtein   0.92\n</code></pre> This <code>plot_df</code> is now ready to be used with the visualization functions.</p>"},{"location":"user_guide/visualization/#1-bar-plot-plot_metric_comparison","title":"1. Bar Plot: <code>plot_metric_comparison()</code>","text":"<p>Use this function to compare models on a single, specific metric.</p>"},{"location":"user_guide/visualization/#key-parameters","title":"Key Parameters:","text":"<ul> <li><code>df</code> (<code>pd.DataFrame</code>): The long-format DataFrame (from <code>prepare_results_dataframe</code>).</li> <li><code>metric_name</code> (<code>str</code>): Required. The name of the metric to plot (e.g., \"Jaccard\", \"ROUGE_rouge1\").</li> <li><code>aggregate_func</code> (Optional <code>Callable</code>): Aggregation function if multiple scores exist per model for the chosen metric (e.g., <code>numpy.mean</code>). Defaults to <code>numpy.mean</code>.</li> <li><code>model_col</code>, <code>score_col</code>, <code>metric_col</code> (Optional <code>str</code>): Names of columns for model, score, and metric. Defaults to \"model_name\", \"score\", \"metric_name\".</li> <li><code>title</code>, <code>xlabel</code>, <code>ylabel</code> (Optional <code>str</code>): Plot customization.</li> <li><code>figsize</code> (Optional <code>tuple</code>): Figure size.</li> <li><code>axis</code> (Optional <code>matplotlib.axes.Axes</code>): Existing Matplotlib Axes to plot on.</li> </ul>"},{"location":"user_guide/visualization/#example","title":"Example:","text":"<p><pre><code>from gaico.visualize import plot_metric_comparison\nimport matplotlib.pyplot as plt # For plt.show()\nimport numpy as np # For np.mean (default aggregate_func)\n\n# Assuming plot_df is available from the previous example\n\n# Plot Jaccard scores\nplot_metric_comparison(plot_df, metric_name=\"Jaccard\", title=\"Jaccard Similarity Comparison\")\nplt.show()\n\n# Plot ROUGE_rouge1 scores\nplot_metric_comparison(plot_df, metric_name=\"ROUGE_rouge1\", title=\"ROUGE-1 F1 Score Comparison\")\nplt.show()\n</code></pre> This will generate and display bar charts, each showing the specified metric's scores for Model_A, Model_B, and Model_C.</p>"},{"location":"user_guide/visualization/#2-radar-plot-plot_radar_comparison","title":"2. Radar Plot: <code>plot_radar_comparison()</code>","text":"<p>Use this function to get a multi-dimensional view of how models perform across several metrics simultaneously. Radar plots are most effective with 3 to 10-12 metrics.</p>"},{"location":"user_guide/visualization/#key-parameters_1","title":"Key Parameters:","text":"<ul> <li><code>df</code> (<code>pd.DataFrame</code>): The long-format DataFrame.</li> <li><code>metrics</code> (Optional <code>List[str]</code>): A list of metric names (e.g., [\"Jaccard\", \"ROUGE_rouge1\", \"Levenshtein\"]) to include in the radar plot. If <code>None</code>, all metrics present in the <code>df</code> for the models are used.</li> <li><code>aggregate_func</code> (Optional <code>Callable</code>): Aggregation function. Defaults to <code>numpy.mean</code>.</li> <li><code>model_col</code>, <code>score_col</code>, <code>metric_col</code> (Optional <code>str</code>): Column names.</li> <li><code>title</code> (Optional <code>str</code>): Plot title.</li> <li><code>figsize</code> (Optional <code>tuple</code>): Figure size.</li> <li><code>axis</code> (Optional <code>matplotlib.axes.Axes</code>): Existing Matplotlib polar Axes to plot on.</li> </ul>"},{"location":"user_guide/visualization/#example_1","title":"Example:","text":"<p><pre><code>from gaico.visualize import plot_radar_comparison\nimport matplotlib.pyplot as plt # For plt.show()\nimport numpy as np # For np.mean\n\n# Assuming plot_df is available\n\n# Define which metrics to include in the radar plot\nmetrics_for_radar = [\"Jaccard\", \"ROUGE_rougeL\", \"Levenshtein\"]\n\nplot_radar_comparison(plot_df, metrics=metrics_for_radar, title=\"Overall Model Performance Radar\")\nplt.show()\n\n# If you want to plot all available metrics (that have scores for the models)\n# plot_radar_comparison(plot_df, title=\"Overall Model Performance Radar (All Metrics)\")\n# plt.show()\n</code></pre> This will generate a radar plot where each axis represents one of the <code>metrics_for_radar</code>, and each model (Model_A, Model_B, Model_C) is represented by a colored shape connecting its scores on these axes.</p> <p>By using these visualization tools directly, you can create custom plots tailored to specific analyses or integrate them into larger reporting dashboards. Remember to have the necessary plotting libraries installed in your environment.</p>"}]}