<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Rating of AI Systems</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body class="container">


    <table style="width:80%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding-left:2.5%;padding-top:3%;width:63%;vertical-align:middle">
                  <p style="text-align:left">
                    <a href='https://ai4society.github.io/'>Home</a>
                  </p>
                </td>
              </tr>
              <h2 style="display:flex;align-items:center;">
                <span style="background-color:hsl(37, 100%, 50%);height:20px;width:20px;margin-right:10px;border:1px solid black;"></span>
                Rating of AI Systems
              </h2>

              <tr style="padding: 0px">
                <td style="padding-left:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align: justify;"><span style="background-color: hsl(37, 100%, 70%);"><b>Rating Sentiment Analysis Systems for Bias through a Causal Lens</b></span><br>
                    <i><b>Collaborators: </b>AIISC, Department of Computer Science and Engineering - University of South Carolina</i>
                    <br>
                    <i><b>Contributors: </b>Kausik Lakkaraju, Biplav Srivastava, Marco Valtorta</i>
                    <br>
                </p>
                <p style="text-align: justify;">
                  Sentiment Analysis Systems (SASs) are data-driven
                  Artificial Intelligence (AI) systems that assign one or more
                  numbers to convey the polarity and emotional intensity of a
                  given piece of text. However, like other automatic machine
                  learning systems, SASs can exhibit model uncertainty, resulting
                  in drastic swings in output with even small changes in input. This
                  issue becomes more problematic when inputs involve protected
                  attributes like gender or race, as it can be perceived as bias or
                  unfairness. To address this, we propose a novel method to assess
                  and rate SASs. We perturb inputs in a controlled causal setting
                  to test if the output sentiment is sensitive to protected attributes
                  while keeping other components of the textual input, such as
                  chosen emotion words, fixed. Based on the results, we assign
                  labels (ratings) at both fine-grained and overall levels to indicate
                  the robustness of the SAS to input changes. These ratings provide
                  a principled basis for comparing SASs and making informed
                  choices based on their behavior. The ratings also benefit all users,
                  especially developers who reuse off-the-shelf SASs to build larger
                  AI systems but do not have access to their code or training data
                  to compare.
                </p>
                
                  <p><i><b>Representative Publications</b></i></p>
                  <ul>
                    <li>Lakkaraju, K., Srivastava, B., & Valtorta, M. (2023). Rating Sentiment Analysis Systems for Bias through a Causal Lens. arXiv preprint arXiv:2302.02038.<br><a href="https://arxiv.org/pdf/2302.02038.pdf" target="_blank"><b>[Paper]</b></a>
                      <a href="https://github.com/ai4society/causal-sas-rating.git" target="_blank" type="text/plain"><b>[GitHub Respository]</b></a>
                      <a href="../rating_page/bib/ieee_tts2023.txt" target="_blank" type="text/plain"><b>[BibTex]</b></a>
                    </li>
                  </ul>
                  <!-- <tr style="padding:0px">
                    <td style="padding-left:2.5%;width:63%;vertical-align:middle">
                        <img src="images/causal_model_2023.png" style="max-width:30%; display: block; margin: 0 auto;">
                    </td>
                </tr> -->
              </td> 
              </tr>
              <br><br>

              <tr style="padding: 0px">
                <td style="padding-left:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align: justify;"><span style="background-color: hsl(37, 100%, 70%);"><b>The Effect of Human v/s Synthetic Test Data and Round-tripping on Assessment of Sentiment Analysis Systems for Bias</b></span><br>
                    <i><b>Collaborators: </b>AIISC, Department of Computer Science and Engineering, Department of Integrated Inforamtion Technology - University of South Carolina, Netaji Subhas University of Technology</i>
                    <br>
                    <i><b>Contributors: </b>Kausik Lakkaraju, Aniket Gupta, Biplav Srivastava, Marco Valtorta, Dezhi Wu</i>
                    <br>
                </p>
                <p style="text-align: justify;">
                  Sentiment Analysis Systems (SASs) are data-driven
                  Artificial Intelligence (AI) systems that output polarity and
                  emotional intensity when given a piece of text as input. Like
                  other AIs, SASs are also known to have unstable behavior when
                  subjected to changes in data which can make them problematic
                  to trust out of concerns like bias when AI works with humans and
                  data has protected attributes like gender, race, and age. Recently,
                  an approach was introduced to assess SASs in a blackbox setting
                  without training data or code, and rating them for bias using
                  synthetic English data. We augment it by introducing two human-generated chatbot datasets and also considering a round-trip
                  setting of translating the data from one language to the same
                  through an intermediate language. We find that these settings
                  show SASs performance in a more realistic light. Specifically,
                  we find that rating SASs on the chatbot data showed more bias
                  compared to the synthetic data, and round-tripping using Spanish
                  and Danish as intermediate languages reduces the bias (up to
                  68% reduction) in human-generated data while, in synthetic data,
                  it takes a surprising turn by increasing the bias! Our findings
                  will help researchers and practitioners refine their SAS testing
                  strategies and foster trust as SASs are considered part of more
                  mission-critical applications for global use.
                </p>
                
                  <p><i><b>Representative Publications</b></i></p>
                  <ul>
                    <li>Kausik Lakkaraju, Aniket Gupta, Biplav Srivastava, Marco Valtorta and Dezhi Wu. The Effect of Human v/s Synthetic Test Data and Round-tripping on Assessment of Sentiment Analysis Systems for Bias. IEEE TPS 2023.<br><a href="https://drive.google.com/file/d/1nd3T-9Sg5ccS2pzTDK7_yndSA9Lq_OAE/view?usp=sharing" target="_blank"><b>[Paper]</b></a>
                      <a href="https://github.com/ai4society/causal-sas-rating.git" target="_blank" type="text/plain"><b>[GitHub Respository]</b></a>
                      <a href="../rating_page/bib/mm_sas.txt" target="_blank" type="text/plain"><b>[BibTex]</b></a>
                    </li>
                  </ul>
                  <!-- <tr style="padding:0px">
                    <td style="padding-left:2.5%;width:63%;vertical-align:middle">
                        <img src="images/causal_model_2023.png" style="max-width:30%; display: block; margin: 0 auto;">
                    </td>
                </tr> -->
              </td> 
              </tr>
              <br><br>

              <tr style="padding: 0px">
                <td style="padding-left:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align: justify;"><span style="background-color: hsl(37, 100%, 70%);"><b>Advances in Automatically Rating the Trustworthiness of Text Processing Services</b></span><br>
                    <i><b>Collaborators: </b>AIISC, Department of Computer Science and Engineering - University of South Carolina, IBM Research</i>
                    <br>
                    <i><b>Contributors: </b>Biplav Srivastava, Kausik Lakkaraju, Mariana Bernagozzi, Marco Valtorta</i>
                    <br>
                </p>
                <p style="text-align: justify;">
                  AI services are known to have unstable behavior when subjected to changes in data, models or users. 
                  Such behaviors, whether triggered by omission or commission, lead to trust issues when AI works with humans. 
                  The current approach of assessing AI services in a black box setting, where the consumer does not have access to the AI's source code or training data, is limited. 
                  The consumer has to rely on the AI developer's documentation and trust that the system has been built as stated. Further, if the AI consumer reuses the service to build other services which they sell to their customers, the consumer is at the risk of the service providers (both data and model providers). 
                  Our approach, in this context, is inspired by the success of nutritional labeling in food industry to promote health and seeks to assess and rate AI services for trust from the perspective of an independent stakeholder. 
                  The ratings become a means to communicate the behavior of AI systems so that the consumer is informed about the risks and can make an informed decision. 
                  In this paper, we will first describe recent progress in developing rating methods for text-based machine translator AI services that have been found promising with user studies. 
                  Then, we will outline challenges and vision for a principled, multi-modal, causality-based rating methodologies and its implication for decision-support in real-world scenarios like health and food recommendation.
                </p>
                
                  <p><i><b>Representative Publications</b></i></p>
                  <ul>
                    <li>Srivastava, B., Lakkaraju, K., Bernagozzi, M., & Valtorta, M. (2023). Advances in Automatically Rating the Trustworthiness of Text Processing Services. arXiv preprint arXiv:2302.09079.<br><a href="https://arxiv.org/abs/2302.09079" target="_blank"><b>[Paper]</b></a>
                      <!-- <a href="" target="_blank" type="text/plain"><b>[GitHub Respository]</b></a> -->
                      <a href="../rating_page/bib/aaai_symp2023.txt" target="_blank" type="text/plain"><b>[BibTex]</b></a>
                    </li>
                  </ul>
                  <tr style="padding:0px">
                    <td style="padding-left:2.5%;width:63%;vertical-align:middle">
                        <img src="images/causal_model_2023.png" style="max-width:30%; display: block; margin: 0 auto;">
                    </td>
                </tr>
              </td> 
              </tr>
              <br><br>

  
              <tr style="padding:0px">
                <td style="padding-left:2.5%;width:63%;vertical-align:middle">   

                  <p style="text-align: justify;"><span style="background-color: hsl(37, 100%, 70%);"><b>Why is my System Biased?: Rating of AI Systems through a Causal Lens</b></span><br>
                  

                  <p style="text-align: justify;"><span style="background-color: hsl(37, 100%, 70%);"><b></b></span><br>
                      <i><b>Collaborators: </b>AIISC - University of South Carolina</i>
                      <br>
                      <i><b>Contributors: </b>Kausik Lakkaraju</i>
                      <br>
                    </p>
                    <p style="text-align: justify;">
                      Artificial Intelligence (AI) systems like facial recognition systems and sentiment analyzers are known to exhibit model uncertainty which can be perceived as algorithmic bias in most cases. 
                      The aim of my Ph.D. is to examine and control the bias present in these AI systems by establishing causal relationships and also assigning a rating to these systems, 
                      which helps the user to make an informed selection when choosing from different systems for their application.
                    </p>
                  
                  <p><i><b>Representative Publications</b></i></p>
                  <ul>
                    <li>Lakkaraju, K. (2022, July). Why is my System Biased?: Rating of AI Systems through a Causal Lens. In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society (pp. 902-902).<br> <a href="https://dl.acm.org/doi/abs/10.1145/3514094.3539556" target="_blank"><b>[Paper]</b></a>
                      <a href="../rating_page/bib/aies_student2022.txt" target="_blank" type="text/plain"><b>[BibTex]</b></a>
                    </li>
                  </ul>
                  <tr style="padding:0px">
                    <!-- <td style="padding-left:2.5%;width:63%;vertical-align:middle">
                        <img src="images/middleware.png" style="max-width:50%; display: block; margin: 0 auto;">
                    </td> -->
                </tr>
              </td> 


            </tr>
            <br><br>
            <tr style="padding: 0px">
              <td style="padding-left:2.5%;width:63%;vertical-align:middle">
                <p style="text-align: justify;"><span style="background-color: hsl(37, 100%, 70%);"><b>ROSE: Tool and Data ResOurces to Explore the Instability of SEntiment Analysis Systems</b></span><br>
                  <i><b>Collaborators: </b>AIISC, Department of Computer Science and Engineering - IIIT Naya Raipur</i>
                  <br>
                  <i><b>Contributors: </b>Gaurav Mundada, Kausik Lakkaraju, & Biplav Srivastava</i>
                  <br>
              </p>
              <p style="text-align: justify;">
                Sentiment Analysis Systems (SASs) are data-driven Artificial Intelligence (AI) systems that assign a score conveying the sentiment and
                emotion intensity when a piece of text is given as input. Like other AI, and especially machine learning (ML) based systems, they have
                also exhibited instability in their values when inputs are perturbed with respect to gender and race, which can be interpreted as biased
                behavior. In this demonstration paper, we present ROSE, a resource for understanding the behavior of SAS systems with respect to
                gender. It consists of data consisting of input text and output sentiment scores and a visualization tool to explore the behavior of SAS.
                We calculated the output sentiment scores using off-the-shelf SASs and our deep-learning-based implementations based on published
                architectures. ROSE, created using the d3.js framework, is publicly available here for easy access.
              </p>
              
                <p><i><b>Representative Publications</b></i></p>
                <ul>
                  <li>MUNDADA, G., LAKKARAJU, K., & SRIVASTAVA, B. ROSE: Tool and Data ResOurces to Explore the Instability of SEntiment Analysis Systems.<br><a href="https://www.researchgate.net/profile/Biplav-Srivastava/publication/358475224_ROSE_Tool_and_Data_ResOurces_to_Explore_the_Instability_of_SEntiment_Analysis_Systems/links/6203f8d0c83d2b75dffd6ecc/ROSE-Tool-and-Data-ResOurces-to-Explore-the-Instability-of-SEntiment-Analysis-Systems.pdf" target="_blank"><b>[Paper]</b></a>
                    <a href="https://github.com/ai4society/sentiment-rating.git" target="_blank" type="text/plain"><b>[GitHub Respository]</b></a>
                    <a href="../rating_page/bib/rose_tool.txt" target="_blank" type="text/plain"><b>[BibTex]</b></a>
                  </li>
                </ul>
                <!-- <tr style="padding:0px">
                  <td style="padding-left:2.5%;width:63%;vertical-align:middle">
                      <img src="images/causal_model_2023.png" style="max-width:30%; display: block; margin: 0 auto;">
                  </td>
              </tr> -->
            </td> 
            </tr>
            <br><br>

            <tr style="padding: 0px">
              <td style="padding-left:2.5%;width:63%;vertical-align:middle">
                <h3>More papers on 'Rating of AI Systems'<br><a href="https://sites.google.com/site/biplavsrivastava/research-1/trustedai?authuser=0#h.useljqp3rw7i" target="_blank"><b>[Click here]</b></a></h3>
              </td> 
            </tr>
            
        </tbody>
    </table>
    <script type="text/javascript" src="../js/script.js"></script>
</body>
</html>